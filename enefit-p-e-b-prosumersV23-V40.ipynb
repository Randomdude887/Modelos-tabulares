{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:12.797510Z","iopub.execute_input":"2025-09-17T20:06:12.797982Z","iopub.status.idle":"2025-09-17T20:06:14.175727Z","shell.execute_reply.started":"2025-09-17T20:06:12.797954Z","shell.execute_reply":"2025-09-17T20:06:14.174409Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/public_timeseries_testing_util.py\n/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/county_id_to_name_map.json\n/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/sample_submission.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CORE DATA MANIPULATION\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import pearsonr, spearmanr\nimport math\n\n# MACHINE LEARNING - SCIKIT-LEARN\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, roc_curve, auc\nfrom sklearn.pipeline import Pipeline\n\n# ADVANCED ML LIBRARIES\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# VISUALIZATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\n\n# CONFIGURATION FOR PLOTS\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n\n# DEEP LEARNING \ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n    print(\"TensorFlow disponible\")\nexcept ImportError:\n    print(\"TensorFlow no disponible en este entorno\")\n\n\n# UTILITIES\nimport os\nimport sys\nimport warnings\nimport itertools\nfrom datetime import datetime, timedelta\nimport time\nfrom collections import Counter\nimport pickle\nimport joblib\n\n# JUPYTER SPECIFIC\nfrom IPython.display import display, HTML\nfrom tqdm.notebook import tqdm\n\n# SUPPRESS WARNINGS\nwarnings.filterwarnings('ignore')\n\n# PANDAS CONFIGURATION\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n# NUMPY CONFIGURATION\nnp.random.seed(42)\n\n# PLOTLY CONFIGURATION\npyo.init_notebook_mode(connected=True)\n\nprint(\"✅ Todas las librerías importadas correctamente!\")\nprint(f\"📊 Pandas version: {pd.__version__}\")\nprint(f\"🔢 NumPy version: {np.__version__}\")\nprint(f\"🤖 Scikit-learn version: {__import__('sklearn').__version__}\")\nprint(f\"📈 Matplotlib version: {__import__('matplotlib').__version__}\")\nprint(f\"🎨 Seaborn version: {sns.__version__}\")\n\n# ============================================================================\n# FUNCIONES ÚTILES ADICIONALES\n# ============================================================================\n\ndef quick_info(df):\n    \"\"\"Información rápida del dataset\"\"\"\n    print(f\"📊 Dataset Shape: {df.shape}\")\n    print(f\"🔢 Columnas numéricas: {df.select_dtypes(include=[np.number]).columns.tolist()}\")\n    print(f\"📝 Columnas categóricas: {df.select_dtypes(include=['object']).columns.tolist()}\")\n    print(f\"❌ Valores nulos por columna:\")\n    print(df.isnull().sum()[df.isnull().sum() > 0])\n\ndef plot_missing_values(df):\n    \"\"\"Visualizar valores faltantes\"\"\"\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    if len(missing) > 0:\n        plt.figure(figsize=(10, 6))\n        missing.plot(kind='bar')\n        plt.title('Valores Faltantes por Columna')\n        plt.ylabel('Cantidad')\n        plt.xticks(rotation=45)\n        plt.show()\n    else:\n        print(\"✅ No hay valores faltantes en el dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:14.177435Z","iopub.execute_input":"2025-09-17T20:06:14.177901Z","iopub.status.idle":"2025-09-17T20:06:34.680626Z","shell.execute_reply.started":"2025-09-17T20:06:14.177873Z","shell.execute_reply":"2025-09-17T20:06:34.679388Z"}},"outputs":[{"name":"stderr","text":"2025-09-17 20:06:24.781159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758139585.080430     227 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758139585.165584     227 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow disponible\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"name":"stdout","text":"✅ Todas las librerías importadas correctamente!\n📊 Pandas version: 2.2.3\n🔢 NumPy version: 1.26.4\n🤖 Scikit-learn version: 1.2.2\n📈 Matplotlib version: 3.7.2\n🎨 Seaborn version: 0.12.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#competencia https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/overview\n\n# Files\n# train.csv\n\n# county - An ID code for the county.\n# is_business - Boolean for whether or not the prosumer is a business.\n# product_type - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n# target - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n# is_consumption - Boolean for whether or not this row's target is consumption or production.\n# datetime - The Estonian time in EET (UTC+2) / EEST (UTC+3). It describes the start of the 1-hour period on which target is given.\n# data_block_id - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n# row_id - A unique identifier for the row.\n# prediction_unit_id - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n# gas_prices.csv\n\n# origin_date - The date when the day-ahead prices became available.\n# forecast_date - The date when the forecast prices should be relevant.\n# [lowest/highest]_price_per_mwh - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n# data_block_id\n# client.csv\n\n# product_type\n# county - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n# eic_count - The aggregated number of consumption points (EICs - European Identifier Code).\n# installed_capacity - Installed photovoltaic solar panel capacity in kilowatts.\n# is_business - Boolean for whether or not the prosumer is a business.\n# date\n# data_block_id\n# electricity_prices.csv\n\n# origin_date\n# forecast_date - Represents the start of the 1-hour period when the price is valid\n# euros_per_mwh - The price of electricity on the day ahead markets in euros per megawatt hour.\n# data_block_id\n# forecast_weather.csv Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n\n# [latitude/longitude] - The coordinates of the weather forecast.\n# origin_datetime - The timestamp of when the forecast was generated.\n# hours_ahead - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n# temperature - The air temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# dewpoint - The dew point temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# cloudcover_[low/mid/high/total] - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total. Estimated for the end of the 1-hour period.\n# 10_metre_[u/v]_wind_component - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second. Estimated for the end of the 1-hour period.\n# data_block_id\n# forecast_datetime - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead. This represents the start of the 1-hour period for which weather data are forecasted.\n# direct_solar_radiation - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the hour, in watt-hours per square meter.\n# surface_solar_radiation_downwards - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, accumulated during the hour, in watt-hours per square meter.\n# snowfall - Snowfall over hour in units of meters of water equivalent.\n# total_precipitation - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the described hour, in units of meters.\n# historical_weather.csv Historic weather data.\n\n# datetime - This represents the start of the 1-hour period for which weather data are measured.\n# temperature - Measured at the end of the 1-hour period.\n# dewpoint - Measured at the end of the 1-hour period.\n# rain - Different from the forecast conventions. The rain from large scale weather systems of the hour in millimeters.\n# snowfall - Different from the forecast conventions. Snowfall over the hour in centimeters.\n# surface_pressure - The air pressure at surface in hectopascals.\n# cloudcover_[low/mid/high/total] - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n# windspeed_10m - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n# winddirection_10m - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n# shortwave_radiation - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n# direct_solar_radiation\n# diffuse_radiation - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n# [latitude/longitude] - The coordinates of the weather station.\n# data_block_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.681730Z","iopub.execute_input":"2025-09-17T20:06:34.682041Z","iopub.status.idle":"2025-09-17T20:06:34.689688Z","shell.execute_reply.started":"2025-09-17T20:06:34.682017Z","shell.execute_reply":"2025-09-17T20:06:34.688390Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#modelos a correr: NN por ser forecasting y algun xgb o lgb\n#son 4 dias a predecir el comportamiento de los prosumers\n#predecir con rolling de a 1 dia\n\n\n#analizar y preparar\n# como siempre primero ver los faltantes y los NaN y en cada caso decidir que hacer\n# atento con valores expresados para el dia actual o la hora actual / la hora anterior / el dia de mañana , ver de no malinterpretar\n# ver la correlacion de precios sea gas y/o electricidad para ver como se comparta con business y no business\n# lo mismo con el clima para business y no business\n# ver si aplica el supuesto de que los business si operan bajo esos valores (costos de electricidad/gas) y los no business no es tan relevante\n# clima historical vs forecast , adaptar columnas que usan distintas proporciones , medir la certeza del forecast\n# analizar por fuera como funcionan los paneles y como miden lo que miden ( para mejor entendimiento del df)\n\n#columnas a agregar\n#Datetime dias lu-1/ma-2/mi-3/ju-4/vi-5/sa-6/do-7 \n#Date time horas y capaz minutos\n#horario laboral ejemplo 7 a 17hs (analizarlo) sea en el dataset y por fuera\n#horario business , para los que son business ver un pseudo horario de apertura y cierre como afecta y si se puede\n#Dia 6hs a 18hs / noche de 18hs a 6hs ver si lo adapto con 4 columnas por las estaciones\n#hora de dormir ejemplo 22hs a 6hs\n#dias festivos y vacaciones , columnas binarias en ambos\n#estaciones del año 4 columnas binarias\n#periodogram para ver los lags y sea mensuales/quincenales/diarios/hora\n#ver si es relevante agregar alguna columna con la poblacion mensual y usar un ratio (info a buscar afuera)\n\n#sugerencias de features\n\n##Energéticas:\n\n# Ratio production/consumption por prediction_unit_id (histórico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n##Temporales:\n\n# Lag de consumo/producción del mismo prediction_unit_id (24h, 48h, 168h)\n# Media móvil de target por prediction_unit_id\n# Cambios día a día (delta vs día anterior)\n\n##Weather engineering:\n\n# Índice de confort térmico (combinando temp + humidity)\n# Potencial solar real (direct + diffuse radiation ajustado por cloudcover)\n# Diferencia forecast vs historical weather (para medir accuracy del forecast)\n\n##Segmentación:\n\n# Clustering de prediction_units por comportamiento similar\n# Ratios county-level (agregaciones por county vs individual)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.692284Z","iopub.execute_input":"2025-09-17T20:06:34.692903Z","iopub.status.idle":"2025-09-17T20:06:34.721398Z","shell.execute_reply.started":"2025-09-17T20:06:34.692873Z","shell.execute_reply":"2025-09-17T20:06:34.720174Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# hacer scatterplot con \n# train3.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) el alpha para ver con mas claridad las zonas mas poblada\n# podria agregar colores para diferencia temperatura o exposicion a rayos solares o nubes y poder encarar distintos el dataset\n# sino funciona probar %matplotlib inline\n# train3.hist(bins=50, figsize=(20,15)) ver tmb los histograma de los datos cuando este todo junto# cambiar los bins a valores mas chicos\n# plt.show() agregar si hace falta\n# corr_matrix = train3.corr()\n# corr_matrix[\"target\"].sort_values(ascending=False) ver si aplica pero creo que no , recordad probar en columnas numericas si hay otra relacion\n# ejemplo crear columna con raiz/potencia/log y ver como apartan esos valores en correlacion\n# o usar scatter_matrix() de pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.722476Z","iopub.execute_input":"2025-09-17T20:06:34.722751Z","iopub.status.idle":"2025-09-17T20:06:34.750908Z","shell.execute_reply.started":"2025-09-17T20:06:34.722730Z","shell.execute_reply":"2025-09-17T20:06:34.749460Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train                 = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\nclient                = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\nelectricity_prices    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\nforecast_weather      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\ngas_prices            = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\nhistorical_weather    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\nweather_station       = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv')\n\ntest                  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv')\nclient_t              = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv')\nelectricity_prices_t  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv')\nforecast_weather_t    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv')\ngas_prices_t          = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv')\nrevealed_targets      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.752316Z","iopub.execute_input":"2025-09-17T20:06:34.753236Z","iopub.status.idle":"2025-09-17T20:07:04.830796Z","shell.execute_reply.started":"2025-09-17T20:06:34.753203Z","shell.execute_reply":"2025-09-17T20:07:04.829720Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# pruebo pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:07:04.831948Z","iopub.execute_input":"2025-09-17T20:07:04.832320Z","iopub.status.idle":"2025-09-17T20:07:04.837343Z","shell.execute_reply.started":"2025-09-17T20:07:04.832284Z","shell.execute_reply":"2025-09-17T20:07:04.836397Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import unicodedata\nfrom sklearn.neighbors import NearestNeighbors\nimport gc\nfrom collections import defaultdict\nfrom pathlib import Path\n\nclass DataPreprocessingPipeline:\n    \"\"\"Pipeline modular para preprocessing de datos de train y test\"\"\"\n    \n    def __init__(self):\n        self.county_id_to_name_map = {\n            0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"JÄRVAMAA\",\n            4: \"JÕGEVAMAA\", 5: \"LÄÄNE-VIRUMAA\", 6: \"LÄÄNEMAA\", 7: \"PÄRNUMAA\",\n            8: \"PÕLVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n            12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"VÕRUMAA\"\n        }\n        self.county_name_to_id_map = {v: k for k, v in self.county_id_to_name_map.items()}\n        \n        # Diccionario de sinónimos para normalización\n        self.synonyms_to_canonical = {\n            \"HARJU\": \"HARJUMAA\", \"HIIU\": \"HIIUMAA\", \"IDA-VIRU\": \"IDA-VIRUMAA\",\n            \"JARVA\": \"JÄRVAMAA\", \"JÄRVA\": \"JÄRVAMAA\", \"JOGEVA\": \"JÕGEVAMAA\", \n            \"JÕGEVA\": \"JÕGEVAMAA\", \"LAANE-VIRU\": \"LÄÄNE-VIRUMAA\", \n            \"LÄÄNE-VIRU\": \"LÄÄNE-VIRUMAA\", \"LAANE\": \"LÄÄNEMAA\", \"LÄÄNE\": \"LÄÄNEMAA\",\n            \"PARNU\": \"PÄRNUMAA\", \"PÄRNU\": \"PÄRNUMAA\", \"POLVA\": \"PÕLVAMAA\", \n            \"PÕLVA\": \"PÕLVAMAA\", \"RAPLA\": \"RAPLAMAA\", \"SAARE\": \"SAAREMAA\",\n            \"TARTU\": \"TARTUMAA\", \"VALGA\": \"VALGAMAA\", \"VILJANDI\": \"VILJANDIMAA\",\n            \"VORU\": \"VÕRUMAA\", \"VÕRU\": \"VÕRUMAA\",\n            # versiones ya canónicas\n            **{name: name for name in self.county_id_to_name_map.values()}\n        }\n    \n    def strip_accents(self, s: str) -> str:\n        \"\"\"Eliminar acentos de string\"\"\"\n        return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n    \n    def normalize_county_name(self, x):\n        \"\"\"Normalizar nombre de county\"\"\"\n        if pd.isna(x):\n            return np.nan\n        s = str(x).upper().strip()\n        # eliminar sufijos frecuentes\n        for suf in [\" COUNTY\", \" MAAKOND\"]:\n            if s.endswith(suf):\n                s = s[: -len(suf)]\n        s = s.replace(\"  \", \" \").replace(\"–\", \"-\")\n        s_noacc = self.strip_accents(s)\n        s_noacc = s_noacc.replace(\"  \", \" \")\n        \n        token = s_noacc.split()[0]\n        if \"-\" in s_noacc:\n            token = s_noacc\n        \n        # intentos de match\n        if s_noacc in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s_noacc]\n        if token in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[token]\n        if s in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s]\n        \n        return np.nan\n    \n    def process_weather_stations(self, weather_station):\n        \"\"\"Procesar estaciones meteorológicas para asignar counties\"\"\"\n        weather_station = weather_station.copy()\n        \n        # Normalizar nombres de county\n        weather_station['county_name_norm'] = weather_station['county_name'].apply(self.normalize_county_name)\n        \n        # Si tengo 'county' numérico pero falta nombre, lo inferimos del ID\n        mask = weather_station['county_name_norm'].isna() & weather_station['county'].notna()\n        weather_station.loc[mask, 'county_name_norm'] = (\n            weather_station.loc[mask, 'county'].astype(int).map(self.county_id_to_name_map)\n        )\n        \n        # Completar faltantes por k-NN\n        fcols = ['latitude', 'longitude']\n        known = weather_station.dropna(subset=['county_name_norm']).copy()\n        unknown = weather_station[weather_station['county_name_norm'].isna()].copy()\n        \n        if not unknown.empty and not known.empty:\n            known_rad = np.radians(known[fcols].values)\n            unknown_rad = np.radians(unknown[fcols].values)\n            \n            nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n            nbrs.fit(known_rad)\n            distances, idxs = nbrs.kneighbors(unknown_rad)\n            \n            nn_names = known['county_name_norm'].values\n            filled = []\n            for neigh_idx in idxs:\n                cands = nn_names[neigh_idx]\n                vals, counts = np.unique(cands, return_counts=True)\n                filled.append(vals[np.argmax(counts)])\n            \n            weather_station.loc[unknown.index, 'county_name_norm'] = filled\n        \n        # Sincronizar columnas finales\n        weather_station['county_name'] = weather_station['county_name_norm']\n        weather_station['county'] = weather_station['county_name'].map(self.county_name_to_id_map).astype('Int64')\n        \n        # Cualquier remanente a UNKNOWN (12)\n        weather_station.loc[weather_station['county_name'].isna(), 'county'] = 12\n        weather_station['county'] = weather_station['county'].astype('Int64')\n        \n        weather_station.drop(columns=['county_name_norm'], inplace=True)\n        \n        return weather_station\n    \n    def optimize_dtypes(self, df, is_train=True):\n        \"\"\"Optimizar tipos de datos para reducir memoria\"\"\"\n        df = df.copy()\n        \n        # Tipos comunes\n        if 'county' in df.columns:\n            df['county'] = df['county'].astype('uint8')\n        if 'is_business' in df.columns:\n            df['is_business'] = df['is_business'].astype('uint8')\n        if 'product_type' in df.columns:\n            df['product_type'] = df['product_type'].astype('uint8')\n        if 'is_consumption' in df.columns:\n            df['is_consumption'] = df['is_consumption'].astype('uint8')\n        if 'hour' in df.columns:\n            df['hour'] = df['hour'].astype('uint8')\n        \n        # IDs\n        if 'data_block_id' in df.columns:\n            df['data_block_id'] = df['data_block_id'].astype('uint16')\n        if 'row_id' in df.columns:\n            df['row_id'] = df['row_id'].astype('uint32')\n        if 'prediction_unit_id' in df.columns:\n            df['prediction_unit_id'] = df['prediction_unit_id'].astype('uint8')\n        \n        # Coordenadas y variables meteorológicas - float32\n        float_cols = ['latitude', 'longitude', 'target', 'lowest_price_per_mwh', \n                     'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', \n                     'installed_capacity']\n        \n        # Variables meteorológicas\n        weather_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                       'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                       'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                       'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                       'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                       'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                       'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                       'direct_solar_radiation', 'diffuse_radiation']\n        \n        # Aplicar float32 a columnas que existen\n        for col in float_cols + weather_cols:\n            if col in df.columns:\n                df[col] = df[col].astype('float32')\n        \n        # Weather forecast hour\n        if 'weather_forecast_hour' in df.columns:\n            df['weather_forecast_hour'] = df['weather_forecast_hour'].astype('uint8')\n        \n        return df\n\n\nclass TrainPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline específico para datos de entrenamiento\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.temp_dir = Path(\"temp_chunks\")\n        self.temp_dir.mkdir(exist_ok=True)\n    \n    def part1_prepare_base_merges(self, train, gas_prices, electricity_prices, client):\n        \"\"\"Parte 1: Preparar datos base y hacer primeros merges\"\"\"\n        print(\"=== PARTE 1: Preparación y merges base ===\")\n        \n        # Preparar copias\n        train1 = train.dropna().copy()\n        gas_prices1 = gas_prices.copy()\n        electricity_prices1 = electricity_prices.copy()\n        client1 = client.copy()\n        \n        # Procesar train\n        train1['datetime'] = pd.to_datetime(train1['datetime'])\n        train1['hour'] = train1['datetime'].dt.hour\n        train1['forecast_date'] = train1['datetime']\n        \n        # Merge 1: Gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        gas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n        \n        # Expandir gas a nivel horario\n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices1_hourly = pd.DataFrame(gas_hourly)\n        train2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')\n        \n        # Forward fill gas prices\n        train2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\n        train2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge 2: Electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        elec_columns = ['forecast_date', 'euros_per_mwh']\n        train3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')\n        train3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge 3: Client data\n        print(\"Procesando client data...\")\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        train4 = train3.merge(client1.drop('data_block_id', axis=1),\n                             left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                             right_on=['product_type', 'county', 'is_business', 'date'],\n                             how='left')\n        \n        train4 = train4.drop('date', axis=1)\n        train4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        print(\"Completando datos de clientes...\")\n        train4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        train4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        train4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        train4['eic_count'] = train4['eic_count'].fillna(train4.groupby('county')['eic_count'].transform('mean'))\n        train4['installed_capacity'] = train4['installed_capacity'].fillna(train4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Optimizar tipos\n        train4 = self.optimize_dtypes(train4, is_train=True)\n        \n        print(f\"Parte 1 completada. Dataset: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")\n        return train4\n    \n    def part2_prepare_weather_merge(self, train4, forecast_weather, historical_weather, weather_station):\n        \"\"\"Parte 2: Preparar datos meteorológicos y hacer merge\"\"\"\n        print(\"=== PARTE 2: Preparación datos meteorológicos ===\")\n        \n        # Procesar weather stations\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        forecast_weather1 = forecast_weather.dropna().copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Merge con historical weather\n        print(\"Mergeando forecast con historical weather...\")\n        historical_weather1 = historical_weather.copy()\n        historical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n        \n        forecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n        \n        merged_weather = pd.merge(\n            forecast_weather2,\n            historical_weather1,\n            left_on=['latitude', 'longitude', 'forecast_datetime'],\n            right_on=['latitude', 'longitude', 'datetime'],\n            how='inner'\n        )\n        \n        # Limpiar y optimizar\n        merged_weather = merged_weather.drop(['f_data_block_id', 'forecast_datetime'], axis=1)\n        merged_weather = self.optimize_dtypes(merged_weather, is_train=True)\n        \n        print(f\"Weather data preparado: {merged_weather.shape[0]:,} filas, {merged_weather.shape[1]} columnas\")\n        return train4, merged_weather\n    \n    def part3_final_merge_and_cleanup(self, train4, merged_weather, chunk_size=1_000_000):\n        \"\"\"Parte 3: Merge final y limpieza\"\"\"\n        print(\"=== PARTE 3: Merge final y limpieza ===\")\n        \n        # Definir períodos para procesar por chunks\n        periods = [\n            ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), \n            ('2021-12-01', '2022-01-01'), ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), \n            ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'), ('2022-05-01', '2022-06-01'), \n            ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n            ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), \n            ('2022-12-01', '2023-01-01'), ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), \n            ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'), ('2023-05-01', '2023-05-31')\n        ]\n        \n        # Procesar por chunks temporales\n        chunk_files = []\n        for i, (start_date, end_date) in enumerate(periods):\n            print(f\"Procesando período {i+1}/{len(periods)}: {start_date} a {end_date}\")\n            \n            # Filtrar por período\n            mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n            train4_chunk = train4[mask_train].copy()\n            \n            mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n            weather_chunk = merged_weather[mask_weather].copy()\n            \n            if len(train4_chunk) > 0 and len(weather_chunk) > 0:\n                # Merge chunk\n                chunk_result = pd.merge(train4_chunk, weather_chunk, \n                               on=['county', 'datetime'], \n                               how='left')\n                \n                # Guardar chunk temporal\n                chunk_file = self.temp_dir / f'train5_chunk_{i+1}.parquet'\n                chunk_result.to_parquet(chunk_file, index=False)\n                chunk_files.append(chunk_file)\n                \n                print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n                \n                # Liberar memoria\n                del train4_chunk, weather_chunk, chunk_result\n                gc.collect()\n        \n        # Concatenar chunks\n        print(\"Concatenando chunks...\")\n        final_chunks = []\n        for chunk_file in chunk_files:\n            chunk_data = pd.read_parquet(chunk_file)\n            final_chunks.append(chunk_data)\n        \n        train5 = pd.concat(final_chunks, ignore_index=True)\n        \n        # Limpieza final\n        print(\"Aplicando limpieza final...\")\n        train5 = self._cleanup_merged_data(train5, chunk_size)\n        \n        # Limpiar archivos temporales\n        for chunk_file in chunk_files:\n            chunk_file.unlink()\n        \n        return train5\n    \n    def _cleanup_merged_data(self, train5, chunk_size=1_000_000):\n        \"\"\"Limpiar datos después del merge\"\"\"\n        # Identificar columnas a eliminar y renombrar\n        sample = train5.head(1000)\n        cols_to_drop = [col for col in sample.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\n        \n        print(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n        \n        # Procesar por chunks\n        chunks_processed = []\n        total_rows = len(train5)\n        n_chunks = (total_rows // chunk_size) + 1\n        \n        for i in range(n_chunks):\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, total_rows)\n            \n            print(f\"Procesando chunk limpieza {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n            \n            chunk = train5.iloc[start_idx:end_idx].copy()\n            \n            # Filtrar county != 12\n            chunk = chunk[chunk['county'] != 12]\n            \n            if len(chunk) > 0:\n                # Eliminar columnas _y y renombrar _x\n                chunk = chunk.drop(columns=cols_to_drop)\n                chunk.rename(columns=rename_dict, inplace=True)\n                chunks_processed.append(chunk)\n            \n            del chunk\n            gc.collect()\n        \n        # Concatenar final\n        train5_clean = pd.concat(chunks_processed, ignore_index=True)\n        del chunks_processed\n        gc.collect()\n        \n        # Eliminar datos de clima faltantes\n        weather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n        initial_rows = len(train5_clean)\n        train5_clean = train5_clean.dropna(subset=weather_key_cols)\n        print(f\"Filas eliminadas por clima faltante: {initial_rows - len(train5_clean):,}\")\n        \n        # Reordenar columnas\n        train5_clean = self._reorder_columns(train5_clean)\n        \n        return train5_clean\n    \n    def _reorder_columns(self, df):\n        \"\"\"Reordenar columnas en el orden especificado\"\"\"\n        main_cols = [\n            'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = main_cols.copy()\n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n    \n    def run_full_pipeline(self, train, gas_prices, electricity_prices, client, \n                         forecast_weather, historical_weather, weather_station):\n        \"\"\"Ejecutar pipeline completo de entrenamiento\"\"\"\n        print(\"Iniciando pipeline completo de entrenamiento...\")\n        \n        # Parte 1: Merges base\n        train4 = self.part1_prepare_base_merges(train, gas_prices, electricity_prices, client)\n        \n        # Parte 2: Preparar datos meteorológicos\n        train4, merged_weather = self.part2_prepare_weather_merge(\n            train4, forecast_weather, historical_weather, weather_station\n        )\n        \n        # Parte 3: Merge final y limpieza\n        train5 = self.part3_final_merge_and_cleanup(train4, merged_weather)\n        \n        print(f\"Pipeline completado! Dataset final: {train5.shape[0]:,} filas, {train5.shape[1]} columnas\")\n        return train5\n\n\nclass TestPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline simplificado para datos de test\"\"\"\n    \n    def run_test_pipeline(self, test, gas_prices_t, electricity_prices_t, client_t, \n                         forecast_weather_t, weather_station):\n        \"\"\"Pipeline completo para test set (más simple, sin historical weather)\"\"\"\n        print(\"=== PIPELINE TEST ===\")\n        \n        # Preparar datos base\n        test1 = test.copy()\n        test1['datetime'] = pd.to_datetime(test1['prediction_datetime'])\n        test1['hour'] = test1['datetime'].dt.hour\n        test1['forecast_date'] = test1['datetime']\n        \n        # Merge gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1 = gas_prices_t.copy()\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        \n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices_hourly = pd.DataFrame(gas_hourly)\n        test2 = pd.merge(test1, gas_prices_hourly, on='forecast_date', how='left')\n        test2['lowest_price_per_mwh'] = test2['lowest_price_per_mwh'].fillna(method='ffill')\n        test2['highest_price_per_mwh'] = test2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1 = electricity_prices_t.copy()\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        test3 = pd.merge(test2, electricity_prices1[['forecast_date', 'euros_per_mwh']], \n                        on='forecast_date', how='left')\n        test3['euros_per_mwh'] = test3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge client data\n        print(\"Procesando client data...\")\n        client1 = client_t.copy()\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        test4 = test3.merge(client1.drop('data_block_id', axis=1),\n                           left_on=['product_type', 'county', 'is_business', test3['datetime'].dt.date],\n                           right_on=['product_type', 'county', 'is_business', 'date'],\n                           how='left')\n        \n        test4 = test4.drop('date', axis=1)\n        test4 = test4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        test4 = test4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        test4['eic_count'] = test4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        test4['installed_capacity'] = test4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        test4['eic_count'] = test4['eic_count'].fillna(test4.groupby('county')['eic_count'].transform('mean'))\n        test4['installed_capacity'] = test4['installed_capacity'].fillna(test4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        forecast_weather1 = forecast_weather_t.copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Para test, no tenemos historical weather, así que creamos columnas dummy o usamos solo forecast\n        # Opción 1: Solo usar forecast weather (renombrar las f columns)\n        # Opción 2: Crear columnas históricas como NaN y llenar después\n        \n        # Vamos con opción 1: usar forecast como histórico también (aproximación)\n        historical_cols_mapping = {\n            'ftemperature': 'temperature',\n            'fdewpoint': 'dewpoint', \n            'fcloudcover_high': 'cloudcover_high',\n            'fcloudcover_low': 'cloudcover_low',\n            'fcloudcover_mid': 'cloudcover_mid', \n            'fcloudcover_total': 'cloudcover_total',\n            'fdirect_solar_radiation': 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards': 'shortwave_radiation',\n            'fsnowfall': 'snowfall',\n            'ftotal_precipitation': 'rain'\n        }\n        \n        # Crear versiones históricas basadas en forecast\n        for fcol, hcol in historical_cols_mapping.items():\n            if fcol in forecast_weather2.columns:\n                forecast_weather2[hcol] = forecast_weather2[fcol]\n        \n        # Agregar columnas que solo existen en historical\n        forecast_weather2['surface_pressure'] = 1013.25  # valor típico\n        forecast_weather2['windspeed_10m'] = np.sqrt(\n            forecast_weather2['f10_metre_u_wind_component']**2 + \n            forecast_weather2['f10_metre_v_wind_component']**2\n        ) if 'f10_metre_u_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['winddirection_10m'] = np.arctan2(\n            forecast_weather2['f10_metre_v_wind_component'], \n            forecast_weather2['f10_metre_u_wind_component']\n        ) * 180 / np.pi if 'f10_metre_v_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['diffuse_radiation'] = forecast_weather2.get('fsurface_solar_radiation_downwards', 0) - forecast_weather2.get('fdirect_solar_radiation', 0)\n        forecast_weather2['diffuse_radiation'] = forecast_weather2['diffuse_radiation'].clip(lower=0)\n        \n        # Limpiar y optimizar\n        forecast_weather2 = forecast_weather2.drop(columns=['data_block_id'], errors='ignore')\n        forecast_weather2 = self.optimize_dtypes(forecast_weather2, is_train=False)\n        \n        # Merge final con weather\n        print(\"Merge final con datos meteorológicos...\")\n        test5 = pd.merge(test4, forecast_weather2, \n                        left_on=['datetime', 'county'], \n                        right_on=['forecast_datetime', 'county'], \n                        how='left')\n\n\n\n        # Limpiar columnas duplicadas y renombrar\n        cols_to_drop = [col for col in test5.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in test5.columns if col.endswith('_x')}\n        \n        test5 = test5.drop(columns=cols_to_drop + ['forecast_datetime'], errors='ignore')\n        test5.rename(columns=rename_dict, inplace=True)\n        \n        # Optimizar tipos finales\n        test5 = self.optimize_dtypes(test5, is_train=False)\n        \n        # Reordenar columnas igual que train\n        test5 = self._reorder_columns_test(test5)\n        \n        print(f\"Pipeline test completado! Dataset: {test5.shape[0]:,} filas, {test5.shape[1]} columnas\")\n        return test5\n    \n    def _reorder_columns_test(self, df):\n        \"\"\"Reordenar columnas para test (mismo orden que train)\"\"\"\n        main_cols = [\n            'row_id', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = []\n        # Solo agregar columnas que existen\n        for col in main_cols:\n            if col in df.columns:\n                final_order.append(col)\n        \n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n\n\n# Funciones de utilidad para usar el pipeline\ndef run_train_pipeline(train, gas_prices, electricity_prices, client, \n                      forecast_weather, historical_weather, weather_station):\n    \"\"\"\n    Función principal para ejecutar pipeline de entrenamiento\n    \n    Returns:\n        pd.DataFrame: Dataset procesado train5\n    \"\"\"\n    pipeline = TrainPipeline()\n    return pipeline.run_full_pipeline(\n        train, gas_prices, electricity_prices, client, \n        forecast_weather, historical_weather, weather_station\n    )\n\ndef run_test_pipeline(test, gas_prices_t, electricity_prices_t, client_t, \n                     forecast_weather_t, weather_station):\n    \"\"\"\n    Función principal para ejecutar pipeline de test\n    \n    Returns:\n        pd.DataFrame: Dataset procesado test5\n    \"\"\"\n    pipeline = TestPipeline()\n    return pipeline.run_test_pipeline(\n        test, gas_prices_t, electricity_prices_t, client_t, \n        forecast_weather_t, weather_station\n    )\n\n# Ejemplo de uso:\n\"\"\"\n# Para train:\ntrain5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)\n\n# Para test:\ntest5 = run_test_pipeline(\n    test, gas_prices_t, electricity_prices_t, client_t, \n    forecast_weather_t, weather_station\n)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:07:04.838724Z","iopub.execute_input":"2025-09-17T20:07:04.839033Z","iopub.status.idle":"2025-09-17T20:07:04.946212Z","shell.execute_reply.started":"2025-09-17T20:07:04.839009Z","shell.execute_reply":"2025-09-17T20:07:04.944928Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n# Para train:\\ntrain5 = run_train_pipeline(\\n    train, gas_prices, electricity_prices, client, \\n    forecast_weather, historical_weather, weather_station\\n)\\n\\n# Para test:\\ntest5 = run_test_pipeline(\\n    test, gas_prices_t, electricity_prices_t, client_t, \\n    forecast_weather_t, weather_station\\n)\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:07:04.947519Z","iopub.execute_input":"2025-09-17T20:07:04.947956Z","iopub.status.idle":"2025-09-17T20:09:25.437993Z","shell.execute_reply.started":"2025-09-17T20:07:04.947928Z","shell.execute_reply":"2025-09-17T20:09:25.436662Z"}},"outputs":[{"name":"stdout","text":"Iniciando pipeline completo de entrenamiento...\n=== PARTE 1: Preparación y merges base ===\nProcesando gas prices...\nProcesando electricity prices...\nProcesando client data...\nCompletando datos de clientes...\nParte 1 completada. Dataset: 2,017,824 filas, 16 columnas\n=== PARTE 2: Preparación datos meteorológicos ===\nProcesando forecast weather...\nCompletando 2140318 counties faltantes con k-NN...\nMergeando forecast con historical weather...\nWeather data preparado: 3,418,242 filas, 32 columnas\n=== PARTE 3: Merge final y limpieza ===\nProcesando período 1/21: 2021-09-01 a 2021-10-01\nChunk 1 guardado: (1423080, 46)\nProcesando período 2/21: 2021-10-01 a 2021-11-01\nChunk 2 guardado: (1532066, 46)\nProcesando período 3/21: 2021-11-01 a 2021-12-01\nChunk 3 guardado: (1484640, 46)\nProcesando período 4/21: 2021-12-01 a 2022-01-01\nChunk 4 guardado: (1543056, 46)\nProcesando período 5/21: 2022-01-01 a 2022-02-01\nChunk 5 guardado: (1590672, 46)\nProcesando período 6/21: 2022-02-01 a 2022-03-01\nChunk 6 guardado: (1441632, 46)\nProcesando período 7/21: 2022-03-01 a 2022-04-01\nChunk 7 guardado: (1597678, 46)\nProcesando período 8/21: 2022-04-01 a 2022-05-01\nChunk 8 guardado: (1630080, 46)\nProcesando período 9/21: 2022-05-01 a 2022-06-01\nChunk 9 guardado: (1644240, 46)\nProcesando período 10/21: 2022-06-01 a 2022-07-01\nChunk 10 guardado: (1573920, 46)\nProcesando período 11/21: 2022-07-01 a 2022-08-01\nChunk 11 guardado: (1581744, 46)\nProcesando período 12/21: 2022-08-01 a 2022-09-01\nChunk 12 guardado: (1581716, 46)\nProcesando período 13/21: 2022-09-01 a 2022-10-01\nChunk 13 guardado: (1573920, 46)\nProcesando período 14/21: 2022-10-01 a 2022-11-01\nChunk 14 guardado: (1686610, 46)\nProcesando período 15/21: 2022-11-01 a 2022-12-01\nChunk 15 guardado: (1643084, 46)\nProcesando período 16/21: 2022-12-01 a 2023-01-01\nChunk 16 guardado: (1669008, 46)\nProcesando período 17/21: 2023-01-01 a 2023-02-01\nChunk 17 guardado: (1654608, 46)\nProcesando período 18/21: 2023-02-01 a 2023-03-01\nChunk 18 guardado: (1494048, 46)\nProcesando período 19/21: 2023-03-01 a 2023-04-01\nChunk 19 guardado: (1604926, 46)\nProcesando período 20/21: 2023-04-01 a 2023-05-01\nChunk 20 guardado: (1514016, 46)\nProcesando período 21/21: 2023-05-01 a 2023-05-31\nChunk 21 guardado: (1530872, 46)\nConcatenando chunks...\nAplicando limpieza final...\nEliminando 1 columnas, renombrando 1\nProcesando chunk limpieza 1/33: filas 0 a 1,000,000\nProcesando chunk limpieza 2/33: filas 1,000,000 a 2,000,000\nProcesando chunk limpieza 3/33: filas 2,000,000 a 3,000,000\nProcesando chunk limpieza 4/33: filas 3,000,000 a 4,000,000\nProcesando chunk limpieza 5/33: filas 4,000,000 a 5,000,000\nProcesando chunk limpieza 6/33: filas 5,000,000 a 6,000,000\nProcesando chunk limpieza 7/33: filas 6,000,000 a 7,000,000\nProcesando chunk limpieza 8/33: filas 7,000,000 a 8,000,000\nProcesando chunk limpieza 9/33: filas 8,000,000 a 9,000,000\nProcesando chunk limpieza 10/33: filas 9,000,000 a 10,000,000\nProcesando chunk limpieza 11/33: filas 10,000,000 a 11,000,000\nProcesando chunk limpieza 12/33: filas 11,000,000 a 12,000,000\nProcesando chunk limpieza 13/33: filas 12,000,000 a 13,000,000\nProcesando chunk limpieza 14/33: filas 13,000,000 a 14,000,000\nProcesando chunk limpieza 15/33: filas 14,000,000 a 15,000,000\nProcesando chunk limpieza 16/33: filas 15,000,000 a 16,000,000\nProcesando chunk limpieza 17/33: filas 16,000,000 a 17,000,000\nProcesando chunk limpieza 18/33: filas 17,000,000 a 18,000,000\nProcesando chunk limpieza 19/33: filas 18,000,000 a 19,000,000\nProcesando chunk limpieza 20/33: filas 19,000,000 a 20,000,000\nProcesando chunk limpieza 21/33: filas 20,000,000 a 21,000,000\nProcesando chunk limpieza 22/33: filas 21,000,000 a 22,000,000\nProcesando chunk limpieza 23/33: filas 22,000,000 a 23,000,000\nProcesando chunk limpieza 24/33: filas 23,000,000 a 24,000,000\nProcesando chunk limpieza 25/33: filas 24,000,000 a 25,000,000\nProcesando chunk limpieza 26/33: filas 25,000,000 a 26,000,000\nProcesando chunk limpieza 27/33: filas 26,000,000 a 27,000,000\nProcesando chunk limpieza 28/33: filas 27,000,000 a 28,000,000\nProcesando chunk limpieza 29/33: filas 28,000,000 a 29,000,000\nProcesando chunk limpieza 30/33: filas 29,000,000 a 30,000,000\nProcesando chunk limpieza 31/33: filas 30,000,000 a 31,000,000\nProcesando chunk limpieza 32/33: filas 31,000,000 a 32,000,000\nProcesando chunk limpieza 33/33: filas 32,000,000 a 32,995,616\nFilas eliminadas por clima faltante: 2,024\nPipeline completado! Dataset final: 32,963,024 filas, 45 columnas\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train5.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:25.442594Z","iopub.execute_input":"2025-09-17T20:09:25.443145Z","iopub.status.idle":"2025-09-17T20:09:25.508602Z","shell.execute_reply.started":"2025-09-17T20:09:25.443107Z","shell.execute_reply":"2025-09-17T20:09:25.507097Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"           row_id      target            datetime       forecast_date  hour  \\\n32965017  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965018  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965019  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965020  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965021  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n\n          data_block_id  prediction_unit_id  is_business  product_type  \\\n32965017            636                  60            1             3   \n32965018            636                  60            1             3   \n32965019            636                  60            1             3   \n32965020            636                  60            1             3   \n32965021            636                  60            1             3   \n\n          county   latitude  longitude  lowest_price_per_mwh  \\\n32965017      15  57.599998  28.200001                  29.0   \n32965018      15  57.900002  26.700001                  29.0   \n32965019      15  57.900002  27.200001                  29.0   \n32965020      15  57.900002  27.700001                  29.0   \n32965021      15  57.900002  28.200001                  29.0   \n\n          highest_price_per_mwh  euros_per_mwh  eic_count  installed_capacity  \\\n32965017                   34.0      82.370003       55.0         2188.199951   \n32965018                   34.0      82.370003       55.0         2188.199951   \n32965019                   34.0      82.370003       55.0         2188.199951   \n32965020                   34.0      82.370003       55.0         2188.199951   \n32965021                   34.0      82.370003       55.0         2188.199951   \n\n          ftemperature  temperature  fdewpoint  dewpoint  fcloudcover_high  \\\n32965017     12.136377         13.8   2.330225       3.9               0.0   \n32965018     14.534815         13.9   2.355859       3.3               0.0   \n32965019     13.503565         13.9   3.533350       2.9               0.0   \n32965020     13.150782         13.6   2.600976       3.2               0.0   \n32965021     12.648584         13.6   1.409814       3.6               0.0   \n\n          cloudcover_high  fcloudcover_low  cloudcover_low  fcloudcover_mid  \\\n32965017              0.0         0.000000             7.0         0.966461   \n32965018              0.0         0.270050            15.0         0.544281   \n32965019              0.0         0.024902            18.0         0.181091   \n32965020              0.0         0.000153            18.0         0.866364   \n32965021              0.0         0.000000             9.0         0.997070   \n\n          cloudcover_mid  fcloudcover_total  cloudcover_total  \\\n32965017            71.0           0.966461              49.0   \n32965018            73.0           0.652069              57.0   \n32965019            66.0           0.184845              56.0   \n32965020            66.0           0.866455              56.0   \n32965021            78.0           0.997070              55.0   \n\n          f10_metre_u_wind_component  windspeed_10m  \\\n32965017                   -1.009770       2.055556   \n32965018                    2.205562       2.916667   \n32965019                    0.538569       3.194444   \n32965020                   -0.223149       3.527778   \n32965021                   -0.198003       3.194444   \n\n          f10_metre_v_wind_component  winddirection_10m  \\\n32965017                   -0.927298              346.0   \n32965018                   -1.008352              297.0   \n32965019                   -1.362844              309.0   \n32965020                   -1.600393              317.0   \n32965021                   -2.190237              319.0   \n\n          fdirect_solar_radiation  direct_solar_radiation  \\\n32965017                 1.858164                   155.0   \n32965018               832.791504                   249.0   \n32965019               486.835938                   257.0   \n32965020                26.035942                   240.0   \n32965021                63.440388                   186.0   \n\n          fsurface_solar_radiation_downwards  shortwave_radiation  \\\n32965017                          191.511948                410.0   \n32965018                          685.467529                490.0   \n32965019                          500.649719                498.0   \n32965020                          268.223053                482.0   \n32965021                          304.685272                446.0   \n\n          diffuse_radiation  fsnowfall  snowfall  ftotal_precipitation  rain  \\\n32965017              255.0        0.0       0.0              0.000000   0.1   \n32965018              241.0        0.0       0.0              0.000056   0.1   \n32965019              241.0        0.0       0.0              0.000000   0.0   \n32965020              242.0        0.0       0.0              0.000000   0.0   \n32965021              260.0        0.0       0.0              0.000000   0.0   \n\n          surface_pressure  weather_forecast_hour  is_consumption  \n32965017       1012.799988                    8.0               1  \n32965018       1005.200012                    8.0               1  \n32965019       1007.200012                    8.0               1  \n32965020       1011.099976                    8.0               1  \n32965021       1013.000000                    8.0               1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>32965017</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.599998</td>\n      <td>28.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>12.136377</td>\n      <td>13.8</td>\n      <td>2.330225</td>\n      <td>3.9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>7.0</td>\n      <td>0.966461</td>\n      <td>71.0</td>\n      <td>0.966461</td>\n      <td>49.0</td>\n      <td>-1.009770</td>\n      <td>2.055556</td>\n      <td>-0.927298</td>\n      <td>346.0</td>\n      <td>1.858164</td>\n      <td>155.0</td>\n      <td>191.511948</td>\n      <td>410.0</td>\n      <td>255.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>1012.799988</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965018</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>26.700001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>14.534815</td>\n      <td>13.9</td>\n      <td>2.355859</td>\n      <td>3.3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.270050</td>\n      <td>15.0</td>\n      <td>0.544281</td>\n      <td>73.0</td>\n      <td>0.652069</td>\n      <td>57.0</td>\n      <td>2.205562</td>\n      <td>2.916667</td>\n      <td>-1.008352</td>\n      <td>297.0</td>\n      <td>832.791504</td>\n      <td>249.0</td>\n      <td>685.467529</td>\n      <td>490.0</td>\n      <td>241.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000056</td>\n      <td>0.1</td>\n      <td>1005.200012</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965019</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>27.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>13.503565</td>\n      <td>13.9</td>\n      <td>3.533350</td>\n      <td>2.9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.024902</td>\n      <td>18.0</td>\n      <td>0.181091</td>\n      <td>66.0</td>\n      <td>0.184845</td>\n      <td>56.0</td>\n      <td>0.538569</td>\n      <td>3.194444</td>\n      <td>-1.362844</td>\n      <td>309.0</td>\n      <td>486.835938</td>\n      <td>257.0</td>\n      <td>500.649719</td>\n      <td>498.0</td>\n      <td>241.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1007.200012</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965020</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>27.700001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>13.150782</td>\n      <td>13.6</td>\n      <td>2.600976</td>\n      <td>3.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000153</td>\n      <td>18.0</td>\n      <td>0.866364</td>\n      <td>66.0</td>\n      <td>0.866455</td>\n      <td>56.0</td>\n      <td>-0.223149</td>\n      <td>3.527778</td>\n      <td>-1.600393</td>\n      <td>317.0</td>\n      <td>26.035942</td>\n      <td>240.0</td>\n      <td>268.223053</td>\n      <td>482.0</td>\n      <td>242.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1011.099976</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965021</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>28.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>12.648584</td>\n      <td>13.6</td>\n      <td>1.409814</td>\n      <td>3.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.997070</td>\n      <td>78.0</td>\n      <td>0.997070</td>\n      <td>55.0</td>\n      <td>-0.198003</td>\n      <td>3.194444</td>\n      <td>-2.190237</td>\n      <td>319.0</td>\n      <td>63.440388</td>\n      <td>186.0</td>\n      <td>304.685272</td>\n      <td>446.0</td>\n      <td>260.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1013.000000</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Features temporales\ntrain5['day_of_week'] = train5['datetime'].dt.dayofweek\ntrain5['month'] = train5['datetime'].dt.month\n# Crear is_daylight (1 si hay radiación solar, 0 si es noche)\ntrain5['is_daylight'] = (train5['direct_solar_radiation'] > 0).astype(int)\n# Crear weekend (1 para sábado/domingo, 0 para lunes-viernes)\ntrain5['weekend'] = (train5['day_of_week'] >= 5).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:25.509590Z","iopub.execute_input":"2025-09-17T20:09:25.509862Z","iopub.status.idle":"2025-09-17T20:09:27.957092Z","shell.execute_reply.started":"2025-09-17T20:09:25.509843Z","shell.execute_reply":"2025-09-17T20:09:27.954851Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Mae puesto n1 kaggle 52.3090\n#1er primer modelo xgb sobre train5 haciendo split 80/20 de train y test \n#Mae = 107.9896\n#sin lag sin feature, sin nada , solo haciendo el merge\n\n\n#agrego day of week y month para todos los siguientes\n\n#2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# estas variables de lags las quito\n\n#agrego is_daylight binaria\n\n#4to modelo lgb 14.2% y sobre esto split 80/20 de train y test  \n#de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses 14.2% aprox\n# se que no deberia ser comparable al usar de base distinto , pero parte de aca ahora\n#Mae = 86.6747\n\n#agrego weekend binaria\n\n#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n\n#para todos los demas modelos ya tienen incluido\n#day of week\n#month\n#is_daylight\n#weekend\n\n#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo la saco por ahora a is_active_hours\n\n#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que  la saco a is_sleeping_hours\n\n#empiezo a probar con variables\n##Energéticas:\n\n# Ratio production/consumption por prediction_unit_id (histórico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n\n#9no modelo lgb mantengo las 3 anteriores y prueba una nueva Ratio production/cn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:27.958969Z","iopub.execute_input":"2025-09-17T20:09:27.959545Z","iopub.status.idle":"2025-09-17T20:09:27.969183Z","shell.execute_reply.started":"2025-09-17T20:09:27.959505Z","shell.execute_reply":"2025-09-17T20:09:27.966863Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n# # Sample 10% manteniendo orden temporal\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# sample_df = train5.sample(frac=0.1, random_state=42).sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"Sample 10%: {len(sample_df):,}\")\n\n# # Convertir datetime\n# sample_df['datetime'] = pd.to_datetime(sample_df['datetime'])\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# # LightGBM (más eficiente en memoria)\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE: {mae:.4f}\")\n# print(f\"Features usadas: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# Dataset original: 32,963,024\n# Sample 10%: 3,296,302\n# MAE: 89.3026\n# Features usadas: 43\n# Train: 2,637,041 | Test: 659,261","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:27.971612Z","iopub.execute_input":"2025-09-17T20:09:27.973347Z","iopub.status.idle":"2025-09-17T20:09:28.023368Z","shell.execute_reply.started":"2025-09-17T20:09:27.973281Z","shell.execute_reply":"2025-09-17T20:09:28.021084Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # Feature importance\n# importance = model.feature_importances_\n# feature_names = X_train.columns\n\n# # Crear DataFrame y ordenar\n# importance_df = pd.DataFrame({\n#     'feature': feature_names,\n#     'importance': importance\n# }).sort_values('importance', ascending=False)\n\n# # Ver top 15\n# print(\"Top 15 features más importantes:\")\n# print(importance_df.head(15))\n\n# # Plot opcional (si quieres visualizar)\n# plt.figure(figsize=(10, 8))\n# plt.barh(importance_df.head(15)['feature'], importance_df.head(15)['importance'])\n# plt.title('Top 15 Feature Importance')\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.026021Z","iopub.execute_input":"2025-09-17T20:09:28.026528Z","iopub.status.idle":"2025-09-17T20:09:28.073240Z","shell.execute_reply.started":"2025-09-17T20:09:28.026476Z","shell.execute_reply":"2025-09-17T20:09:28.071474Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# importance_df['importance_pct'] = (importance_df['importance'] / importance_df['importance'].sum()) * 100\n# print(importance_df.head(15)[['feature', 'importance', 'importance_pct']])\n\n#                               feature  importance  importance_pct\n# 40                      is_consumption         469       15.633333\n# 12                  installed_capacity         436       14.533333\n# 11                           eic_count         292        9.733333\n# 2                   prediction_unit_id         228        7.600000\n# 31  fsurface_solar_radiation_downwards         227        7.566667\n# 0                                 hour         180        6.000000\n# 1                        data_block_id         164        5.466667\n# 41                         day_of_week         161        5.366667\n# 32                 shortwave_radiation         111        3.700000\n# 3                          is_business         105        3.500000\n# 10                       euros_per_mwh          83        2.766667\n# 14                         temperature          73        2.433333\n# 42                               month          72        2.400000\n# 30              direct_solar_radiation          60        2.000000\n# 29             fdirect_solar_radiation          48        1.600000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.074818Z","iopub.execute_input":"2025-09-17T20:09:28.075247Z","iopub.status.idle":"2025-09-17T20:09:28.122844Z","shell.execute_reply.started":"2025-09-17T20:09:28.075206Z","shell.execute_reply":"2025-09-17T20:09:28.120859Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n# Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# Sample 10%\n\n\n\n\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# sample_df = train5.sample(frac=0.1, random_state=42).sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Sample 10%: {len(sample_df):,}\")\n\n# # Convertir datetime\n# sample_df['datetime'] = pd.to_datetime(sample_df['datetime'])\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Variables climáticas importantes para lags\n# climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                 'temperature', 'direct_solar_radiation']\n\n# # Crear lags por prediction_unit_id\n# print(\"🔄 Creando lags climáticos...\")\n# lag_dfs = []\n\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id].copy()\n    \n#     # Lags de 1h, 4h, 12h para cada variable climática\n#     for var in climate_vars:\n#         if var in unit_data.columns:\n#             unit_data[f'{var}_lag1h'] = unit_data[var].shift(1)\n#             unit_data[f'{var}_lag4h'] = unit_data[var].shift(4)\n#             unit_data[f'{var}_lag12h'] = unit_data[var].shift(12)\n    \n#     lag_dfs.append(unit_data)\n\n# sample_with_lags = pd.concat(lag_dfs, ignore_index=True)\n# sample_with_lags = sample_with_lags.sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Datos con lags: {len(sample_with_lags):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_with_lags.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal\n# split_idx = int(len(sample_with_lags) * 0.8)\n# train_data = sample_with_lags.iloc[:split_idx]\n# test_data = sample_with_lags.iloc[split_idx:]\n\n# # Dropear NaN (por los lags)\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train final: {len(X_train):,}\")\n# print(f\"Test final: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con lags: {mae:.4f}\")\n# print(f\"Mejora vs 89.3: {89.3 - mae:.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.124109Z","iopub.execute_input":"2025-09-17T20:09:28.124912Z","iopub.status.idle":"2025-09-17T20:09:28.168652Z","shell.execute_reply.started":"2025-09-17T20:09:28.124874Z","shell.execute_reply":"2025-09-17T20:09:28.167055Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.171211Z","iopub.execute_input":"2025-09-17T20:09:28.171666Z","iopub.status.idle":"2025-09-17T20:09:28.219560Z","shell.execute_reply.started":"2025-09-17T20:09:28.171634Z","shell.execute_reply":"2025-09-17T20:09:28.218147Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#4to modelo lgb de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses por memoria \n#Mae = 86.6747\n\n\n# Preparar datos y tomar últimos 3 meses\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# train5['datetime'] = pd.to_datetime(train5['datetime'])\n\n# # Últimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"Últimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# # Features temporales\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Crear is_daylight (1 si hay radiación solar, 0 si es noche)\n# sample_df['is_daylight'] = (sample_df['direct_solar_radiation'] > 0).astype(int)\n\n# print(f\"Daylight distribution:\")\n# print(sample_df['is_daylight'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_daylight (3 meses): {mae:.4f}\")\n# print(f\"Baseline original era: 89.3\")\n\n# # #Dataset original: 32,963,024\n# # Últimos 3 meses: 4,674,024\n# # Porcentaje: 14.2%\n# # Daylight distribution:\n# # is_daylight\n# # 1    2592660\n# # 0    2081364\n# # Name: count, dtype: int64\n# # Features totales: 44\n# # Train: 3,739,219 | Test: 934,805\n# # MAE con is_daylight (3 meses): 86.6747\n# # Baseline original era: 89.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.220786Z","iopub.execute_input":"2025-09-17T20:09:28.221072Z","iopub.status.idle":"2025-09-17T20:09:28.261512Z","shell.execute_reply.started":"2025-09-17T20:09:28.221052Z","shell.execute_reply":"2025-09-17T20:09:28.260070Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n# Preparar datos y tomar últimos 3 meses\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# train5['datetime'] = pd.to_datetime(train5['datetime'])\n\n# # Últimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"Últimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# # Features temporales\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Crear is_daylight (1 si hay radiación solar, 0 si es noche)\n# sample_df['is_daylight'] = (sample_df['direct_solar_radiation'] > 0).astype(int)\n\n# # Crear weekend (1 para sábado/domingo, 0 para lunes-viernes)\n# sample_df['weekend'] = (sample_df['day_of_week'] >= 5).astype(int)\n\n# print(f\"Daylight distribution:\")\n# print(sample_df['is_daylight'].value_counts())\n# print(f\"Weekend distribution:\")\n# print(sample_df['weekend'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_daylight + weekend (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior era: 86.67\")\n\n# # Dataset original: 32,963,024\n# # Últimos 3 meses: 4,674,024\n# # Porcentaje: 14.2%\n# # Daylight distribution:\n# # is_daylight\n# # 1    2592660\n# # 0    2081364\n# # Name: count, dtype: int64\n# # Weekend distribution:\n# # weekend\n# # 0    3341592\n# # 1    1332432\n# # Name: count, dtype: int64\n# # Features totales: 45\n# # Train: 3,739,219 | Test: 934,805\n# # MAE con is_daylight + weekend (3 meses): 86.6747\n# # MAE anterior era: 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.262862Z","iopub.execute_input":"2025-09-17T20:09:28.263267Z","iopub.status.idle":"2025-09-17T20:09:28.317573Z","shell.execute_reply.started":"2025-09-17T20:09:28.263240Z","shell.execute_reply":"2025-09-17T20:09:28.315469Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo\n\n# # Usar train5 con las features ya agregadas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Agregar is_active_hours (horario laboral/escolar lunes-viernes 8-17h)\n# train5['is_active_hours'] = (\n#     (train5['day_of_week'] < 5) &  # Lunes-viernes\n#     (train5['hour'] >= 8) & \n#     (train5['hour'] <= 17)\n# ).astype(int)\n\n# # Últimos 3 meses para evitar problemas de memoria\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_active_hours (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior (is_daylight): 86.67\")\n\n# MAE con is_active_hours (3 meses): 90.4776\n# MAE anterior (is_daylight): 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.319424Z","iopub.execute_input":"2025-09-17T20:09:28.320930Z","iopub.status.idle":"2025-09-17T20:09:28.390262Z","shell.execute_reply.started":"2025-09-17T20:09:28.320877Z","shell.execute_reply":"2025-09-17T20:09:28.388232Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que no la saco\n\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Agregar is_sleeping_hours (22h-6h)\n# train5['is_sleeping_hours'] = (\n#     (train5['hour'] >= 22) | (train5['hour'] <= 6)\n# ).astype(int)\n\n# # Últimos 3 meses para evitar problemas de memoria\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"Últimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# print(f\"Sleeping hours distribution:\")\n# print(sample_df['is_sleeping_hours'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_active_hours (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior (is_daylight): 86.67\")\n\n# Dataset original: 32,963,024\n# Últimos 3 meses: 4,674,024\n# Porcentaje: 14.2%\n# Sleeping hours distribution:\n# is_sleeping_hours\n# 0    2923632\n# 1    1750392\n# Name: count, dtype: int64\n# Features totales: 46\n# Train: 3,739,219 | Test: 934,805\n# MAE con is_active_hours (3 meses): 98.3152\n# MAE anterior (is_daylight): 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.392682Z","iopub.execute_input":"2025-09-17T20:09:28.393402Z","iopub.status.idle":"2025-09-17T20:09:28.434815Z","shell.execute_reply.started":"2025-09-17T20:09:28.393335Z","shell.execute_reply":"2025-09-17T20:09:28.432717Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n# Usar train5 con features básicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Últimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# # Features energéticas \n# print(\"🔄 Calculando features energéticas...\")\n\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     # Usar installed_capacity como proxy de capacidad productiva\n#     capacity = unit_data['installed_capacity'].iloc[0]\n    \n#     # Ratio de tipos: cuántas observaciones son production vs consumption\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     production_ratio = production_obs / total_obs if total_obs > 0 else 0\n#     consumption_ratio = consumption_obs / total_obs if total_obs > 0 else 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_ratio,\n#         'consumption_obs_ratio': consumption_ratio\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n\n# # Merge con datos principales\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Últimos 3 meses: {len(sample_df):,}\")\n# print(f\"Nuevas features: capacity_per_obs, production_obs_ratio, consumption_obs_ratio\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con features energéticas: {mae:.4f}\")\n# print(f\"MAE anterior: 86.67\")\n\n# # BORRAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data, energy_df, energy_stats\n# import gc\n# gc.collect()\n\n# Últimos 3 meses: 4,674,024\n# Nuevas features: capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n# Features totales: 48\n# Train: 3,739,219 | Test: 934,805\n# MAE con features energéticas: 76.2703\n# MAE anterior: 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.437123Z","iopub.execute_input":"2025-09-17T20:09:28.437508Z","iopub.status.idle":"2025-09-17T20:09:28.484372Z","shell.execute_reply.started":"2025-09-17T20:09:28.437478Z","shell.execute_reply":"2025-09-17T20:09:28.482853Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#v9 no optimizado para poder usarlo #\n\n# # Usar train5 con features básicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 4 MESES para calcular ratio, luego reducir a 3\n# max_date = train5['datetime'].max()\n# cutoff_4m = max_date - pd.DateOffset(months=4)\n# cutoff_3m = max_date - pd.DateOffset(months=3)\n\n# sample_4m = train5[train5['datetime'] >= cutoff_4m].copy()\n# print(\"🔄 Calculando ratio production/consumption histórico (30d)...\")\n\n# # MÉTODO SIMPLE Y DIRECTO - sin reindexing problemático\n# sample_4m['prod_cons_ratio_30d'] = 0.0\n\n# for unit_id in sample_4m['prediction_unit_id'].unique():\n#     unit_data = sample_4m[sample_4m['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     # Para cada fila, calcular ratio con ventana de 30 días hacia atrás (SIMPLE)\n#     for i in range(len(unit_data)):\n#         current_time = unit_data.iloc[i]['datetime']\n#         start_time = current_time - pd.Timedelta(days=30)\n        \n#         # Datos en ventana de 30 días\n#         window_data = unit_data[\n#             (unit_data['datetime'] >= start_time) & \n#             (unit_data['datetime'] < current_time)\n#         ]\n        \n#         if len(window_data) > 0:\n#             # Promedios simples\n#             prod_mean = window_data[window_data['is_consumption'] == 0]['target'].mean()\n#             cons_mean = window_data[window_data['is_consumption'] == 1]['target'].mean()\n            \n#             # Manejar NaN y división por cero\n#             if pd.isna(prod_mean): prod_mean = 0\n#             if pd.isna(cons_mean): cons_mean = 1\n            \n#             ratio = prod_mean / cons_mean if cons_mean != 0 else 0\n#             sample_4m.iloc[i, sample_4m.columns.get_loc('prod_cons_ratio_30d')] = ratio\n\n# # Reducir a 3 meses finales\n# sample_df = sample_4m[sample_4m['datetime'] >= cutoff_3m].copy()\n# print(f\"Datos finales (3 meses): {len(sample_df):,}\")\n\n# # Features energéticas\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# # Split y entrenamiento\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# split_idx = int(len(sample_df) * 0.8)\n# X_train = sample_df.iloc[:split_idx][feature_cols].dropna()\n# y_train = sample_df.iloc[:split_idx].loc[X_train.index, 'target']\n# X_test = sample_df.iloc[split_idx:][feature_cols].dropna()\n# y_test = sample_df.iloc[split_idx:].loc[X_test.index, 'target']\n\n# print(f\"Features: {len(feature_cols)} | Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                          random_state=42, n_jobs=-1, verbose=-1)\n# model.fit(X_train, y_train)\n\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"✅ MAE con ratio 30d: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 76.27\")\n\n# print(f\"📈 Ratio stats: min={sample_df['prod_cons_ratio_30d'].min():.3f}, max={sample_df['prod_cons_ratio_30d'].max():.3f}, NaNs={sample_df['prod_cons_ratio_30d'].isna().sum()}\")\n\n# # Limpiar memoria\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_4m, energy_df, energy_stats\n# import gc\n# gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.485860Z","iopub.execute_input":"2025-09-17T20:09:28.486281Z","iopub.status.idle":"2025-09-17T20:09:28.542158Z","shell.execute_reply.started":"2025-09-17T20:09:28.486235Z","shell.execute_reply":"2025-09-17T20:09:28.538700Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# #10mo modelo lgb mantengo las 3 anteriores y pruebo  Capacidad instalada per capita (installed_capacity / eic_count)\n# # Usar train5 con features básicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Últimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"🔄 Calculando features energéticas + capacity per capita...\")\n\n# # Features energéticas existentes + NUEVA: capacity per capita\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0  # NUEVA FEATURE\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energéticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"✅ MAE con capacity per capita: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 76.27\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats, feature_importance\n# import gc\n# gc.collect()\n# print(\"🧹 Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.545367Z","iopub.execute_input":"2025-09-17T20:09:28.546495Z","iopub.status.idle":"2025-09-17T20:09:28.592083Z","shell.execute_reply.started":"2025-09-17T20:09:28.546455Z","shell.execute_reply":"2025-09-17T20:09:28.589172Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.593419Z","iopub.execute_input":"2025-09-17T20:09:28.593765Z","iopub.status.idle":"2025-09-17T20:09:28.702747Z","shell.execute_reply.started":"2025-09-17T20:09:28.593736Z","shell.execute_reply":"2025-09-17T20:09:28.701023Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n6      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n7      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n8      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n9      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n10     366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n\n    data_block_id  prediction_unit_id  is_business  product_type  county  \\\n6               0                   0            0             1       0   \n7               0                   0            0             1       0   \n8               0                   0            0             1       0   \n9               0                   0            0             1       0   \n10              0                   0            0             1       0   \n\n     latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n6   59.099998  24.200001                 45.23                  46.32   \n7   59.099998  25.200001                 45.23                  46.32   \n8   59.400002  22.700001                 45.23                  46.32   \n9   59.400002  23.200001                 45.23                  46.32   \n10  59.400002  23.700001                 45.23                  46.32   \n\n    euros_per_mwh  eic_count  installed_capacity  ftemperature  temperature  \\\n6       86.879997      108.0          952.890015     12.681543         12.4   \n7       86.879997      108.0          952.890015     12.868921         12.3   \n8       86.879997      108.0          952.890015     15.041773         15.2   \n9       86.879997      108.0          952.890015     14.632105         14.9   \n10      86.879997      108.0          952.890015     14.480005         12.8   \n\n    fdewpoint  dewpoint  fcloudcover_high  cloudcover_high  fcloudcover_low  \\\n6    9.783228       9.7          0.023590             11.0         0.002380   \n7    9.498316       9.6          0.431854              6.0         0.211182   \n8   11.860376      11.8          0.134674             18.0         0.202515   \n9   11.773584      11.5          0.255188             19.0         0.036774   \n10  11.581568      10.4          0.338074             12.0         0.037109   \n\n    cloudcover_low  fcloudcover_mid  cloudcover_mid  fcloudcover_total  \\\n6             10.0         0.001251             0.0           0.026398   \n7             23.0         0.006790             1.0           0.548508   \n8              7.0         0.003906             0.0           0.308930   \n9              6.0         0.026245             0.0           0.286194   \n10             4.0         0.016510             1.0           0.368134   \n\n    cloudcover_total  f10_metre_u_wind_component  windspeed_10m  \\\n6               12.0                    1.840991       4.222222   \n7               23.0                    1.505298       4.027778   \n8               12.0                    3.185351       9.055555   \n9               11.0                    3.474780       8.361111   \n10               8.0                    3.211841       5.416667   \n\n    f10_metre_v_wind_component  winddirection_10m  fdirect_solar_radiation  \\\n6                    -3.857846              338.0                      0.0   \n7                    -3.590024              337.0                      0.0   \n8                    -8.173276              338.0                      0.0   \n9                    -8.008969              335.0                      0.0   \n10                   -7.426206              337.0                      0.0   \n\n    direct_solar_radiation  fsurface_solar_radiation_downwards  \\\n6                      0.0                                 0.0   \n7                      0.0                                 0.0   \n8                      0.0                                 0.0   \n9                      0.0                                 0.0   \n10                     0.0                                 0.0   \n\n    shortwave_radiation  diffuse_radiation  fsnowfall  snowfall  \\\n6                   0.0                0.0        0.0       0.0   \n7                   0.0                0.0        0.0       0.0   \n8                   0.0                0.0        0.0       0.0   \n9                   0.0                0.0        0.0       0.0   \n10                  0.0                0.0        0.0       0.0   \n\n    ftotal_precipitation  rain  surface_pressure  weather_forecast_hour  \\\n6                    0.0   0.0       1009.200012                    1.0   \n7                    0.0   0.0       1004.200012                    1.0   \n8                    0.0   0.0       1015.000000                    1.0   \n9                    0.0   0.0       1014.500000                    1.0   \n10                   0.0   0.0       1014.000000                    1.0   \n\n    is_consumption  day_of_week  month  is_daylight  weekend  \n6                0            2      9            0        0  \n7                0            2      9            0        0  \n8                0            2      9            0        0  \n9                0            2      9            0        0  \n10               0            2      9            0        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.681543</td>\n      <td>12.4</td>\n      <td>9.783228</td>\n      <td>9.7</td>\n      <td>0.023590</td>\n      <td>11.0</td>\n      <td>0.002380</td>\n      <td>10.0</td>\n      <td>0.001251</td>\n      <td>0.0</td>\n      <td>0.026398</td>\n      <td>12.0</td>\n      <td>1.840991</td>\n      <td>4.222222</td>\n      <td>-3.857846</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1009.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.868921</td>\n      <td>12.3</td>\n      <td>9.498316</td>\n      <td>9.6</td>\n      <td>0.431854</td>\n      <td>6.0</td>\n      <td>0.211182</td>\n      <td>23.0</td>\n      <td>0.006790</td>\n      <td>1.0</td>\n      <td>0.548508</td>\n      <td>23.0</td>\n      <td>1.505298</td>\n      <td>4.027778</td>\n      <td>-3.590024</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1004.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>15.041773</td>\n      <td>15.2</td>\n      <td>11.860376</td>\n      <td>11.8</td>\n      <td>0.134674</td>\n      <td>18.0</td>\n      <td>0.202515</td>\n      <td>7.0</td>\n      <td>0.003906</td>\n      <td>0.0</td>\n      <td>0.308930</td>\n      <td>12.0</td>\n      <td>3.185351</td>\n      <td>9.055555</td>\n      <td>-8.173276</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1015.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.632105</td>\n      <td>14.9</td>\n      <td>11.773584</td>\n      <td>11.5</td>\n      <td>0.255188</td>\n      <td>19.0</td>\n      <td>0.036774</td>\n      <td>6.0</td>\n      <td>0.026245</td>\n      <td>0.0</td>\n      <td>0.286194</td>\n      <td>11.0</td>\n      <td>3.474780</td>\n      <td>8.361111</td>\n      <td>-8.008969</td>\n      <td>335.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.500000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.480005</td>\n      <td>12.8</td>\n      <td>11.581568</td>\n      <td>10.4</td>\n      <td>0.338074</td>\n      <td>12.0</td>\n      <td>0.037109</td>\n      <td>4.0</td>\n      <td>0.016510</td>\n      <td>1.0</td>\n      <td>0.368134</td>\n      <td>8.0</td>\n      <td>3.211841</td>\n      <td>5.416667</td>\n      <td>-7.426206</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# train5[\"fsurface_solar_radiation_downwards\"].describe()\n\n# count    3.296302e+07\n# mean     1.103279e+02\n# std      1.714488e+02\n# min     -3.258333e-01\n# 25%      0.000000e+00\n# 50%      5.706424e-01\n# 75%      1.429561e+02\n# max      8.487144e+02\n# Name: fsurface_solar_radiation_downwards, dtype: float64\n\ntrain5[\"fsurface_solar_radiation_downwards\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.703941Z","iopub.execute_input":"2025-09-17T20:09:28.704309Z","iopub.status.idle":"2025-09-17T20:09:29.817604Z","shell.execute_reply.started":"2025-09-17T20:09:28.704282Z","shell.execute_reply":"2025-09-17T20:09:29.816440Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"fsurface_solar_radiation_downwards\n0.000000      15297128\n0.284444         39564\n0.142222         35932\n0.568889         28742\n0.071111         21062\n                ...   \n169.200272           2\n320.213043           2\n48.876667            2\n42.334446            2\n268.698120           2\nName: count, Length: 1291576, dtype: int64"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# #11vo modelo lgb 4 feature electrica y pruebo panel_efficiency\n# # ✅ MAE con panel efficiency: 85.0309\n# # Usar train5 con features básicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Últimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"🔄 Calculando features energéticas + capacity per capita + panel efficiency...\")\n\n# # Features energéticas existentes + NUEVAS: capacity per capita + panel efficiency\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     # Calcular eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n#     production_data = unit_data[unit_data['is_consumption'] == 0]\n#     if len(production_data) > 0 and 'fsurface_solar_radiation_downwards' in production_data.columns:\n#         solar_rad = production_data['fsurface_solar_radiation_downwards']\n#         production = production_data['target']\n#         # Filtrar valores válidos (solar > 0 para evitar división por 0)\n#         valid_mask = (solar_rad > 0) & (production >= 0)\n#         if valid_mask.sum() > 0:\n#             panel_efficiency = (production[valid_mask] / solar_rad[valid_mask]).mean()\n#         else:\n#             panel_efficiency = 0\n#     else:\n#         panel_efficiency = 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0,\n#         'panel_efficiency': panel_efficiency  # NUEVA FEATURE\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energéticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"✅ MAE con panel efficiency: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"📈 Panel efficiency stats:\")\n# print(f\"   - Min: {sample_df['panel_efficiency'].min():.4f}\")\n# print(f\"   - Max: {sample_df['panel_efficiency'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['panel_efficiency'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['panel_efficiency'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"🧹 Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.818786Z","iopub.execute_input":"2025-09-17T20:09:29.819152Z","iopub.status.idle":"2025-09-17T20:09:29.826123Z","shell.execute_reply.started":"2025-09-17T20:09:29.819126Z","shell.execute_reply":"2025-09-17T20:09:29.825039Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# #12vo modelo lgb , lo anterior pero ahora efficiency panel es log\n# # ✅ MAE con panel efficiency LOG: 85.3200\n# # 📊 MAE anterior: 72.3572\n\n# # Usar train5 con features básicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Últimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"🔄 Calculando features energéticas + capacity per capita + panel efficiency LOG...\")\n\n# # Features energéticas existentes + NUEVAS: capacity per capita + panel efficiency LOG\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     # Calcular eficiencia de paneles LOG (production vs solar_radiation cuando is_consumption=False)\n#     production_data = unit_data[unit_data['is_consumption'] == 0]\n#     if len(production_data) > 0 and 'fsurface_solar_radiation_downwards' in production_data.columns:\n#         solar_rad = production_data['fsurface_solar_radiation_downwards']\n#         production = production_data['target']\n#         # Filtrar valores válidos (solar > 0 para evitar división por 0)\n#         valid_mask = (solar_rad > 0) & (production >= 0)\n#         if valid_mask.sum() > 0:\n#             efficiency_ratio = (production[valid_mask] / solar_rad[valid_mask]).mean()\n#             # Aplicar log pero manejando valores <= 1\n#             if efficiency_ratio > 1:\n#                 panel_efficiency_log = np.log(efficiency_ratio)\n#             elif efficiency_ratio > 0:\n#                 panel_efficiency_log = -np.log(1/efficiency_ratio)  # log inverso para valores < 1\n#             else:\n#                 panel_efficiency_log = 0\n#         else:\n#             panel_efficiency_log = 0\n#     else:\n#         panel_efficiency_log = 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0,\n#         'panel_efficiency_log': panel_efficiency_log  # NUEVA FEATURE CON LOG\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energéticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"✅ MAE con panel efficiency LOG: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"📈 Capacity per capita stats:\")\n# print(f\"   - Min: {sample_df['capacity_per_capita'].min():.4f}\")\n# print(f\"   - Max: {sample_df['capacity_per_capita'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['capacity_per_capita'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['capacity_per_capita'].isna().sum()}\")\n\n# print(f\"📈 Panel efficiency LOG stats:\")\n# print(f\"   - Min: {sample_df['panel_efficiency_log'].min():.4f}\")\n# print(f\"   - Max: {sample_df['panel_efficiency_log'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['panel_efficiency_log'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['panel_efficiency_log'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"🧹 Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.831600Z","iopub.execute_input":"2025-09-17T20:09:29.832025Z","iopub.status.idle":"2025-09-17T20:09:29.852545Z","shell.execute_reply.started":"2025-09-17T20:09:29.831997Z","shell.execute_reply":"2025-09-17T20:09:29.851420Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# #13vo modelo probando volatilidad de precio de electricidad 7 dias\n# # ✅ MAE con electricity price volatility: 73.2246\n# # 📊 MAE anterior: 72.3572\n# # Usar train5 con features básicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 120 días para calcular volatilidad, luego reducir a 90\n# max_date = train5['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 días para calcular volatilidad\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 días finales\n\n# # Datos de 120 días para calcular volatilidad\n# sample_120d = train5[train5['datetime'] >= cutoff_120d].copy()\n\n# print(\"🔄 Calculando volatilidad de precios (7d rolling) con 120 días...\")\n\n# # Calcular volatilidad de electricity prices (rolling std 7 días)\n# sample_120d['electricity_price_volatility_7d'] = 0.0\n\n# for unit_id in sample_120d['prediction_unit_id'].unique():\n#     unit_data = sample_120d[sample_120d['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     if 'euros_per_mwh' in unit_data.columns and len(unit_data) > 0:\n#         # Rolling std de 7 días para electricity prices\n#         unit_data = unit_data.set_index('datetime')\n#         price_vol = unit_data['euros_per_mwh'].rolling('7D', min_periods=1).std()\n        \n#         # Manejar NaN y resetear index\n#         price_vol = price_vol.fillna(0)\n#         unit_data = unit_data.reset_index()\n        \n#         # Asignar volatilidad al dataframe principal\n#         mask = sample_120d['prediction_unit_id'] == unit_id\n#         sample_120d.loc[mask, 'electricity_price_volatility_7d'] = price_vol.values\n\n# # AHORA reducir a últimos 90 días (con volatilidad ya calculada)\n# sample_df = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n# print(f\"Datos finales (90 días con volatilidad): {len(sample_df):,}\")\n\n# print(\"🔄 Calculando features energéticas + capacity per capita...\")\n\n# # Features energéticas existentes + capacity per capita (SIN panel_efficiency)\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energéticas + volatilidad: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"✅ MAE con electricity price volatility: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"📈 Electricity price volatility 7d stats:\")\n# print(f\"   - Min: {sample_df['electricity_price_volatility_7d'].min():.4f}\")\n# print(f\"   - Max: {sample_df['electricity_price_volatility_7d'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['electricity_price_volatility_7d'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['electricity_price_volatility_7d'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_120d, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"🧹 Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.853579Z","iopub.execute_input":"2025-09-17T20:09:29.854080Z","iopub.status.idle":"2025-09-17T20:09:29.885563Z","shell.execute_reply.started":"2025-09-17T20:09:29.854002Z","shell.execute_reply":"2025-09-17T20:09:29.884031Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# #14vo modelo ademas de electricity_volatility pruebo con gas\n# # ✅ MAE con electricity + gas price volatility: 70.6517\n# # 📊 MAE anterior: 73.2246\n\n# # Usar train5 con features básicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 120 días para calcular volatilidad, luego reducir a 90\n# max_date = train5['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 días para calcular volatilidad\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 días finales\n\n# # Datos de 120 días para calcular volatilidad\n# sample_120d = train5[train5['datetime'] >= cutoff_120d].copy()\n\n# print(\"🔄 Calculando volatilidad de electricity y gas prices (7d rolling) con 120 días...\")\n\n# # Calcular volatilidad de electricity y gas prices (rolling std 7 días)\n# sample_120d['electricity_price_volatility_7d'] = 0.0\n# sample_120d['gas_price_volatility_7d'] = 0.0\n\n# for unit_id in sample_120d['prediction_unit_id'].unique():\n#     unit_data = sample_120d[sample_120d['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     if len(unit_data) > 0:\n#         unit_data_indexed = unit_data.set_index('datetime')\n        \n#         # Volatilidad electricity prices\n#         if 'euros_per_mwh' in unit_data.columns:\n#             elec_vol = unit_data_indexed['euros_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n#         else:\n#             elec_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n#         # Volatilidad gas prices\n#         if 'lowest_price_per_mwh' in unit_data.columns:\n#             gas_vol = unit_data_indexed['lowest_price_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n#         else:\n#             gas_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n#         # Asignar volatilidades al dataframe principal usando iloc\n#         mask = sample_120d['prediction_unit_id'] == unit_id\n#         indices = sample_120d[mask].index\n        \n#         sample_120d.loc[indices, 'electricity_price_volatility_7d'] = elec_vol.values\n#         sample_120d.loc[indices, 'gas_price_volatility_7d'] = gas_vol.values\n\n# # AHORA reducir a últimos 90 días (conservando TODAS las columnas)\n# sample_df = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n# print(f\"Datos finales (90 días con volatilidades): {len(sample_df):,}\")\n\n# print(\"🔄 Calculando features energéticas + capacity per capita...\")\n\n# # Features energéticas existentes + capacity per capita\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energéticas + volatilidades: {len(sample_df):,}\")\n\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"✅ MAE con electricity + gas price volatility: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 73.2246\")\n\n# # Verificar nuevas features\n\n# if 'gas_price_volatility_7d' in sample_df.columns:\n#     print(f\"📈 Gas price volatility 7d stats:\")\n#     print(f\"   - Min: {sample_df['gas_price_volatility_7d'].min():.4f}\")\n#     print(f\"   - Max: {sample_df['gas_price_volatility_7d'].max():.4f}\")\n#     print(f\"   - Mean: {sample_df['gas_price_volatility_7d'].mean():.4f}\")\n#     print(f\"   - NaN count: {sample_df['gas_price_volatility_7d'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_120d, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"🧹 Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.887381Z","iopub.execute_input":"2025-09-17T20:09:29.887769Z","iopub.status.idle":"2025-09-17T20:09:29.918729Z","shell.execute_reply.started":"2025-09-17T20:09:29.887740Z","shell.execute_reply":"2025-09-17T20:09:29.917484Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# CREAR TRAIN6 con 150 días y todas las features consolidadas\nprint(\"🔄 Creando train6 con 150 días...\")\n\n# Usar train5 como base\ntrain5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# Últimos 150 días para train6\nmax_date = train5['datetime'].max()\ncutoff_150d = max_date - pd.DateOffset(days=150)\ntrain6 = train5[train5['datetime'] >= cutoff_150d].copy()\n\nprint(f\"📊 train6 creado: {len(train6):,} filas ({len(train6['prediction_unit_id'].unique())} units)\")\n\nprint(\"🔄 Calculando features energéticas...\")\n\n# Features energéticas\nenergy_stats = []\nfor unit_id in train6['prediction_unit_id'].unique():\n    unit_data = train6[train6['prediction_unit_id'] == unit_id]\n    \n    capacity = unit_data['installed_capacity'].iloc[0]\n    eic_count = unit_data['eic_count'].iloc[0]\n    total_obs = len(unit_data)\n    production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n    consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n    energy_stats.append({\n        'prediction_unit_id': unit_id,\n        'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n        'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n        'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n        'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n    })\n\nenergy_df = pd.DataFrame(energy_stats)\ntrain6 = train6.merge(energy_df, on='prediction_unit_id', how='left')\n\nprint(\"🔄 Calculando volatilidades de precios (7d rolling)...\")\n\n# Volatilidades de precios (7d rolling)\ntrain6['electricity_price_volatility_7d'] = 0.0\ntrain6['gas_price_volatility_7d'] = 0.0\n\nfor unit_id in train6['prediction_unit_id'].unique():\n    unit_data = train6[train6['prediction_unit_id'] == unit_id].copy()\n    unit_data = unit_data.sort_values('datetime')\n    \n    if len(unit_data) > 0:\n        unit_data_indexed = unit_data.set_index('datetime')\n        \n        # Volatilidad electricity prices\n        if 'euros_per_mwh' in unit_data.columns:\n            elec_vol = unit_data_indexed['euros_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n        else:\n            elec_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n        # Volatilidad gas prices\n        if 'lowest_price_per_mwh' in unit_data.columns:\n            gas_vol = unit_data_indexed['lowest_price_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n        else:\n            gas_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n        # Asignar volatilidades\n        mask = train6['prediction_unit_id'] == unit_id\n        indices = train6[mask].index\n        \n        train6.loc[indices, 'electricity_price_volatility_7d'] = elec_vol.values\n        train6.loc[indices, 'gas_price_volatility_7d'] = gas_vol.values\n\nprint(\"✅ train6 creado con todas las features:\")\nprint(f\"   📊 Datos: {len(train6):,} filas\")\nprint(f\"   📊 Período: {train6['datetime'].min()} a {train6['datetime'].max()}\")\nprint(f\"   🔧 Features energéticas: 4 (capacity_per_obs, production_obs_ratio, consumption_obs_ratio, capacity_per_capita)\")\nprint(f\"   📈 Volatilidades: 2 (electricity_price_volatility_7d, gas_price_volatility_7d)\")\n\n# Verificar features creadas\nfeature_check = [\n    'capacity_per_obs', 'production_obs_ratio', 'consumption_obs_ratio', \n    'capacity_per_capita', 'electricity_price_volatility_7d', 'gas_price_volatility_7d'\n]\n\nprint(\"\\n🔍 Verificación de features:\")\nfor feature in feature_check:\n    if feature in train6.columns:\n        print(f\"   ✅ {feature}: OK\")\n    else:\n        print(f\"   ❌ {feature}: FALTA\")\n\n# Estadísticas básicas de las nuevas features\nprint(f\"\\n📈 Estadísticas rápidas:\")\nfor feature in feature_check:\n    if feature in train6.columns:\n        print(f\"   {feature}: min={train6[feature].min():.3f}, max={train6[feature].max():.3f}, mean={train6[feature].mean():.3f}\")\n\nprint(f\"\\n✨ train6 listo para usar con {len(train6.columns)} columnas totales\")\n\n# Limpiar memoria temporal\ndel energy_df, energy_stats\nimport gc\ngc.collect()\nprint(\"🧹 Memoria temporal liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.919970Z","iopub.execute_input":"2025-09-17T20:09:29.920381Z","iopub.status.idle":"2025-09-17T20:09:51.859530Z","shell.execute_reply.started":"2025-09-17T20:09:29.920323Z","shell.execute_reply":"2025-09-17T20:09:51.858313Z"}},"outputs":[{"name":"stdout","text":"🔄 Creando train6 con 150 días...\n📊 train6 creado: 7,821,024 filas (68 units)\n🔄 Calculando features energéticas...\n🔄 Calculando volatilidades de precios (7d rolling)...\n✅ train6 creado con todas las features:\n   📊 Datos: 7,821,024 filas\n   📊 Período: 2022-12-31 10:00:00 a 2023-05-30 10:00:00\n   🔧 Features energéticas: 4 (capacity_per_obs, production_obs_ratio, consumption_obs_ratio, capacity_per_capita)\n   📈 Volatilidades: 2 (electricity_price_volatility_7d, gas_price_volatility_7d)\n\n🔍 Verificación de features:\n   ✅ capacity_per_obs: OK\n   ✅ production_obs_ratio: OK\n   ✅ consumption_obs_ratio: OK\n   ✅ capacity_per_capita: OK\n   ✅ electricity_price_volatility_7d: OK\n   ✅ gas_price_volatility_7d: OK\n\n📈 Estadísticas rápidas:\n   capacity_per_obs: min=0.000, max=0.091, mean=0.014\n   production_obs_ratio: min=0.500, max=0.500, mean=0.500\n   consumption_obs_ratio: min=0.500, max=0.500, mean=0.500\n   capacity_per_capita: min=0.438, max=213.750, mean=26.458\n   electricity_price_volatility_7d: min=0.000, max=66.608, mean=39.864\n   gas_price_volatility_7d: min=0.000, max=12.594, mean=2.619\n\n✨ train6 listo para usar con 55 columnas totales\n🧹 Memoria temporal liberada\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Modelo #16: Lag 24h de target por prediction_unit_id\n# #  últimos 90 días \n\n# print(\"🔄 Modelo #16: Lag 24h con últimos 90 días...\")\n\n# # Filtrar últimos 90 días de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h por prediction_unit_id\n# sample_90d['target_lag_24h'] = sample_90d.groupby('prediction_unit_id')['target'].shift(24)\n\n# # Stats del lag\n# valid_lags = sample_90d['target_lag_24h'].notna().sum()\n# print(f\"📈 Lag 24h válidos: {valid_lags:,} ({valid_lags/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en lag\n# train_clean = sample_90d.dropna(subset=['target_lag_24h'])\n# print(f\"📊 Datos finales: {len(train_clean):,}\")\n\n# # Features (incluye nueva lag)\n# feature_cols = [col for col in train_clean.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(train_clean) * 0.8)\n# train_data = train_clean.iloc[:split_idx]\n# test_data = train_clean.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con lag 24h (90 días): {mae:.4f}\")\n# print(f\"📊 MAE anterior: 70.6517\")\n# print(f\"📈 Cambio: {mae - 70.6517:+.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.860898Z","iopub.execute_input":"2025-09-17T20:09:51.861329Z","iopub.status.idle":"2025-09-17T20:09:51.868378Z","shell.execute_reply.started":"2025-09-17T20:09:51.861297Z","shell.execute_reply":"2025-09-17T20:09:51.867321Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# # 17vo modelo Lag 48h de target por prediction_unit_id , no va\n# #✅ MAE con lag 48h (90 días): 104.3003\n# #📊 MAE anterior: 70.6517\n# #  últimos 90 días \n\n# print(\"🔄 Modelo #17: Lag 48h con últimos 90 días...\")\n\n# # Filtrar últimos 90 días de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h por prediction_unit_id\n# sample_90d['target_lag_48h'] = sample_90d.groupby('prediction_unit_id')['target'].shift(24)\n\n# # Stats del lag\n# valid_lags = sample_90d['target_lag_48h'].notna().sum()\n# print(f\"📈 Lag 24h válidos: {valid_lags:,} ({valid_lags/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en lag\n# train_clean = sample_90d.dropna(subset=['target_lag_48h'])\n# print(f\"📊 Datos finales: {len(train_clean):,}\")\n\n# # Features (incluye nueva lag)\n# feature_cols = [col for col in train_clean.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(train_clean) * 0.8)\n# train_data = train_clean.iloc[:split_idx]\n# test_data = train_clean.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con lag 48h (90 días): {mae:.4f}\")\n# print(f\"📊 MAE anterior: 70.6517\")\n# print(f\"📈 Cambio: {mae - 70.6517:+.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.869615Z","iopub.execute_input":"2025-09-17T20:09:51.869948Z","iopub.status.idle":"2025-09-17T20:09:51.898746Z","shell.execute_reply.started":"2025-09-17T20:09:51.869924Z","shell.execute_reply":"2025-09-17T20:09:51.897763Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# # Modelo #18: Cyclical encoding de hora (hour_sin, hour_cos)\n# ✅ MAE con hour_sin/cos: 122.4835 # algo esta pasado ver como corregirlo\n# 📊 MAE anterior: 70.6517\n\n# # Usando últimos 90 días\n\n# print(\"🔄 Modelo #18: Agregando hour_sin y hour_cos...\")\n\n# # Filtrar últimos 90 días de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Crear cyclical encoding de hora\n# sample_90d['hour_sin'] = np.sin(2 * np.pi * sample_90d['hour'] / 24)\n# sample_90d['hour_cos'] = np.cos(2 * np.pi * sample_90d['hour'] / 24)\n\n# print(f\"📈 Hour_sin range: [{sample_90d['hour_sin'].min():.3f}, {sample_90d['hour_sin'].max():.3f}]\")\n# print(f\"📈 Hour_cos range: [{sample_90d['hour_cos'].min():.3f}, {sample_90d['hour_cos'].max():.3f}]\")\n\n# # Features (incluye nuevas cyclical)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con hour_sin/cos: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 70.6517\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.899887Z","iopub.execute_input":"2025-09-17T20:09:51.900252Z","iopub.status.idle":"2025-09-17T20:09:51.926287Z","shell.execute_reply.started":"2025-09-17T20:09:51.900218Z","shell.execute_reply":"2025-09-17T20:09:51.925096Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# # Modelo #19: Cyclical encoding de semana (week_sin, week_cos)\n# #✅ MAE con week_sin/cos: 118.3710 los temporales , de la forma que los planteo no estan funcionando\n# #📊 MAE anterior: 70.6517\n# # Usando últimos 90 días\n\n# print(\"🔄 Modelo #19: Agregando week_sin y week_cos...\")\n\n# # Filtrar últimos 90 días de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Crear cyclical encoding de semana del año\n# sample_90d['week_of_year'] = sample_90d['datetime'].dt.isocalendar().week\n# sample_90d['week_sin'] = np.sin(2 * np.pi * sample_90d['week_of_year'] / 52)\n# sample_90d['week_cos'] = np.cos(2 * np.pi * sample_90d['week_of_year'] / 52)\n\n# print(f\"📈 Week range: [{sample_90d['week_of_year'].min()}, {sample_90d['week_of_year'].max()}]\")\n# print(f\"📈 Week_sin range: [{sample_90d['week_sin'].min():.3f}, {sample_90d['week_sin'].max():.3f}]\")\n# print(f\"📈 Week_cos range: [{sample_90d['week_cos'].min():.3f}, {sample_90d['week_cos'].max():.3f}]\")\n\n# # Features (incluye nuevas cyclical de semana)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con week_sin/cos: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 70.6517\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.927450Z","iopub.execute_input":"2025-09-17T20:09:51.927893Z","iopub.status.idle":"2025-09-17T20:09:51.960720Z","shell.execute_reply.started":"2025-09-17T20:09:51.927858Z","shell.execute_reply":"2025-09-17T20:09:51.959416Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# # Modelo #20: Cambios día a día (delta vs día anterior) 'temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation'\n# # ✅ MAE con deltas weather 24h: 124.1003 algo mal estoy haciendo o el overfit es grande por 90dias ,capaz con mas dias menos overfit\n# # 📊 MAE anterior: 70.6517\n# # Usando últimos 90 días\n\n# print(\"🔄 Modelo #20: Agregando deltas día a día...\")\n\n# # Filtrar últimos 90 días de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear deltas día a día de weather features \n# weather_cols = ['temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation']\n\n# for col in weather_cols:\n#     if col in sample_90d.columns:\n#         # Delta de 24h hacia atrás por prediction_unit_id\n#         sample_90d[f'{col}_delta_24h'] = (sample_90d[col] - \n#                                          sample_90d.groupby('prediction_unit_id')[col].shift(24))\n        \n#         # Stats del delta\n#         valid_deltas = sample_90d[f'{col}_delta_24h'].notna().sum()\n#         print(f\"📈 {col}_delta_24h válidos: {valid_deltas:,}\")\n\n# # Limpiar NaN en deltas (al menos uno debe ser válido)\n# delta_cols = [f'{col}_delta_24h' for col in weather_cols if col in sample_90d.columns]\n# sample_90d = sample_90d.dropna(subset=delta_cols, how='all')\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos deltas)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con deltas weather 24h: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 70.6517\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.962292Z","iopub.execute_input":"2025-09-17T20:09:51.963095Z","iopub.status.idle":"2025-09-17T20:09:51.995702Z","shell.execute_reply.started":"2025-09-17T20:09:51.963047Z","shell.execute_reply":"2025-09-17T20:09:51.994369Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# # Modelo #21: Media móvil de target por prediction_unit_id\n# # ✅ MAE con media móvil 7d: 56.1320 MMM data leakage por como estan los datos , veo como lo cambio\n# # 📊 MAE anterior: 70.6517\n# # Usando últimos 90 días\n\n# print(\"🔄 Modelo #21: Agregando media móvil de target...\")\n\n# # Filtrar últimos 90 días de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media móvil de target por prediction_unit_id (ventana 7 días)\n# sample_90d['target_ma_7d'] = (sample_90d.groupby('prediction_unit_id')['target']\n#                               .shift(1)  # Evitar data leakage\n#                               .rolling(window=168, min_periods=24)  # 7 días = 168 horas\n#                               .mean())\n\n# # Stats de la media móvil\n# valid_ma = sample_90d['target_ma_7d'].notna().sum()\n# print(f\"📈 Target MA 7d válidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media móvil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d'])\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media móvil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con media móvil 7d: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 70.6517\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.996940Z","iopub.execute_input":"2025-09-17T20:09:51.997280Z","iopub.status.idle":"2025-09-17T20:09:52.026142Z","shell.execute_reply.started":"2025-09-17T20:09:51.997244Z","shell.execute_reply":"2025-09-17T20:09:52.024899Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# # Modelo #22: Media móvil de target por prediction_unit_id lag 5 dias\n# # ✅ MAE con media móvil 7d (lag 5d): 61.2636\n# # 📊 MAE anterior: 70.6517\n# # Usando 120 días para calcular, reducir a 90 días\n\n# print(\"🔄 Modelo #22: Media móvil con datos suficientes...\")\n\n# # USAR 120 días para calcular media móvil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 días para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 días finales\n\n# # Datos de 120 días para calcular media móvil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"📊 Dataset 120 días para cálculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media móvil de target por prediction_unit_id (ventana 7 días, retraso 5 días)\n# sample_120d['target_ma_7d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 días = 120 horas\n#                                    .rolling(window=168, min_periods=168)  # 7 días = 168 horas\n#                                    .mean())\n\n# # AHORA reducir a últimos 90 días\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media móvil\n# valid_ma = sample_90d['target_ma_7d_lag5d'].notna().sum()\n# print(f\"📊 Datos finales (90 días): {len(sample_90d):,} registros\")\n# print(f\"📈 Target MA 7d (lag 5d) válidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media móvil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos después de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media móvil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con media móvil 7d (lag 5d): {mae:.4f}\")\n# print(f\"📊 MAE anterior: 70.6517\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.027482Z","iopub.execute_input":"2025-09-17T20:09:52.027962Z","iopub.status.idle":"2025-09-17T20:09:52.058001Z","shell.execute_reply.started":"2025-09-17T20:09:52.027928Z","shell.execute_reply":"2025-09-17T20:09:52.056916Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# # Modelo #23: Media móvil de target por prediction_unit_id lag 5 dias\n# #✅ MAE con ambas media móvil 7d y 3d (lag 5d): 64.0943\n# #📊 MAE anterior: 61.2636\n# # Usando 120 días para calcular, reducir a 90 días\n\n# print(\"🔄 Modelo #23: Media móvil con datos suficientes...\")\n\n# # USAR 120 días para calcular media móvil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 días para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 días finales\n\n# # Datos de 120 días para calcular media móvil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"📊 Dataset 120 días para cálculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media móvil de target por prediction_unit_id (ventana 7 días, retraso 5 días)\n# sample_120d['target_ma_7d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 días = 120 horas\n#                                    .rolling(window=168, min_periods=168)  # 7 días = 168 horas\n#                                    .mean())\n\n# sample_120d['target_ma_3d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 días = 120 horas\n#                                    .rolling(window=72, min_periods=72)  # 3 días = 72 horas\n#                                    .mean())\n\n\n# # AHORA reducir a últimos 90 días\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media móvil\n# valid_ma = sample_90d['target_ma_7d_lag5d'].notna().sum()\n# print(f\"📊 Datos finales (90 días): {len(sample_90d):,} registros\")\n# print(f\"📈 Target MA 7d (lag 5d) válidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Stats de la media móvil\n# valid_ma1 = sample_90d['target_ma_3d_lag5d'].notna().sum()\n# print(f\"📊 Datos finales (90 días): {len(sample_90d):,} registros\")\n# print(f\"📈 Target MA 3d (lag 5d) válidos: {valid_ma1:,} ({valid_ma1/len(sample_90d)*100:.1f}%)\")\n\n\n# # Limpiar NaN en media móvil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos después de limpiar NaN: {len(sample_90d):,}\")\n\n# sample_90d = sample_90d.dropna(subset=['target_ma_3d_lag5d'])\n# print(f\"📊 Datos después de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media móvil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con media móvil 3d (lag 5d): {mae:.4f}\")\n# print(f\"📊 MAE anterior: 61.2636\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.059289Z","iopub.execute_input":"2025-09-17T20:09:52.059673Z","iopub.status.idle":"2025-09-17T20:09:52.088535Z","shell.execute_reply.started":"2025-09-17T20:09:52.059627Z","shell.execute_reply":"2025-09-17T20:09:52.087318Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# # Modelo #24: Media móvil de target por prediction_unit_id lag 5 dias\n# #✅ MAE con media móvil 3d (lag 5d): 63.5897\n# #📊 MAE anterior: 61.2636\n# # Usando 120 días para calcular, reducir a 90 días\n\n# print(\"🔄 Modelo #24: Media móvil 3d...\")\n\n# # USAR 120 días para calcular media móvil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 días para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 días finales\n\n# # Datos de 120 días para calcular media móvil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"📊 Dataset 120 días para cálculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media móvil de target por prediction_unit_id (ventana 7 días, retraso 5 días)\n# sample_120d['target_ma_3d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 días = 120 horas\n#                                    .rolling(window=72, min_periods=72)  # 3 días = 72 horas\n#                                    .mean())\n\n# # AHORA reducir a últimos 90 días\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media móvil\n# valid_ma = sample_90d['target_ma_3d_lag5d'].notna().sum()\n# print(f\"📊 Datos finales (90 días): {len(sample_90d):,} registros\")\n# print(f\"📈 Target MA 3d (lag 5d) válidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media móvil\n# sample_90d = sample_90d.dropna(subset=['target_ma_3d_lag5d'])\n# print(f\"📊 Datos después de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media móvil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con media móvil 3d (lag 5d): {mae:.4f}\")\n# print(f\"📊 MAE anterior: 61.2636\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.089993Z","iopub.execute_input":"2025-09-17T20:09:52.090402Z","iopub.status.idle":"2025-09-17T20:09:52.118719Z","shell.execute_reply.started":"2025-09-17T20:09:52.090373Z","shell.execute_reply":"2025-09-17T20:09:52.117535Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# # Modelo #25: Media móvil de target por prediction_unit_id lag 5 dias\n# #✅ MAE con media móvil 5d (lag 5d): 71.2301\n# #📊 MAE anterior: 61.2636\n# # Usando 120 días para calcular, reducir a 90 días\n\n# print(\"🔄 Modelo #25: Media móvil 5d...\")\n\n# # USAR 120 días para calcular media móvil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 días para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 días finales\n\n# # Datos de 120 días para calcular media móvil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"📊 Dataset 120 días para cálculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media móvil de target por prediction_unit_id (ventana 7 días, retraso 5 días)\n# sample_120d['target_ma_5d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 días = 120 horas\n#                                    .rolling(window=120, min_periods=120)  # 3 días = 72 horas\n#                                    .mean())\n\n# # AHORA reducir a últimos 90 días\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media móvil\n# valid_ma = sample_90d['target_ma_5d_lag5d'].notna().sum()\n# print(f\"📊 Datos finales (90 días): {len(sample_90d):,} registros\")\n# print(f\"📈 Target MA 5d (lag 5d) válidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media móvil\n# sample_90d = sample_90d.dropna(subset=['target_ma_5d_lag5d'])\n# print(f\"📊 Datos después de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media móvil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con media móvil 5d (lag 5d): {mae:.4f}\")\n# print(f\"📊 MAE anterior: 61.2636\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.120037Z","iopub.execute_input":"2025-09-17T20:09:52.120439Z","iopub.status.idle":"2025-09-17T20:09:52.150302Z","shell.execute_reply.started":"2025-09-17T20:09:52.120403Z","shell.execute_reply":"2025-09-17T20:09:52.149201Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Crear train7: train6 + media móvil 7d lag 5d + sin weather histórico\n# train6 tiene 150 días, primeros días tendrán NaN en MA\n\nprint(\"🔄 Creando train7: train6 + target_ma_7d_lag5d + sin weather histórico...\")\n\n# Copiar train6 y ordenar\ntrain7 = train6.copy()\ntrain7 = train7.sort_values(['prediction_unit_id', 'datetime'])\n\nprint(f\"📊 train6 original: {len(train7):,} registros\")\n\n# Crear media móvil de target por prediction_unit_id (ventana 7 días, retraso 5 días)\ntrain7['target_ma_7d_lag5d'] = (train7.groupby('prediction_unit_id')['target']\n                               .shift(120)  # 5 días = 120 horas\n                               .rolling(window=168, min_periods=168)  # 7 días = 168 horas\n                               .mean())\n\n# Stats de la nueva feature\nvalid_ma = train7['target_ma_7d_lag5d'].notna().sum()\nprint(f\"📈 target_ma_7d_lag5d válidos: {valid_ma:,} ({valid_ma/len(train7)*100:.1f}%)\")\n\n# Eliminar columnas de historical_weather (no están en test set)\nhistorical_weather_cols = [\n    'temperature',      # histórico\n    'dewpoint',         # histórico  \n    'rain',\n    'snowfall', \n    'surface_pressure',\n    'cloudcover_low',\n    'cloudcover_mid', \n    'cloudcover_high',\n    'cloudcover_total',\n    'windspeed_10m',\n    'winddirection_10m', \n    'shortwave_radiation',\n    'direct_solar_radiation',\n    'diffuse_radiation'\n]\n\n# Verificar qué columnas existen y eliminarlas\ncols_to_drop = [col for col in historical_weather_cols if col in train7.columns]\nprint(f\"🗑️ Eliminando {len(cols_to_drop)} columnas weather históricas:\")\nfor col in cols_to_drop:\n    print(f\"   - {col}\")\n\ntrain7 = train7.drop(columns=cols_to_drop)\n\nprint(f\"✅ train7 creado: {len(train7):,} registros, {len(train7.columns)} columnas\")\n\n# Borrar train6 para liberar memoria\ndel train6\ngc.collect()\nprint(\"🧹 train6 eliminado de memoria\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.151544Z","iopub.execute_input":"2025-09-17T20:09:52.151922Z","iopub.status.idle":"2025-09-17T20:09:56.995276Z","shell.execute_reply.started":"2025-09-17T20:09:52.151890Z","shell.execute_reply":"2025-09-17T20:09:56.994072Z"}},"outputs":[{"name":"stdout","text":"🔄 Creando train7: train6 + target_ma_7d_lag5d + sin weather histórico...\n📊 train6 original: 7,821,024 registros\n📈 target_ma_7d_lag5d válidos: 7,801,508 (99.8%)\n🗑️ Eliminando 14 columnas weather históricas:\n   - temperature\n   - dewpoint\n   - rain\n   - snowfall\n   - surface_pressure\n   - cloudcover_low\n   - cloudcover_mid\n   - cloudcover_high\n   - cloudcover_total\n   - windspeed_10m\n   - winddirection_10m\n   - shortwave_radiation\n   - direct_solar_radiation\n   - diffuse_radiation\n✅ train7 creado: 7,821,024 registros, 42 columnas\n🧹 train6 eliminado de memoria\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# # Modelo #26: Sin histórico weather (solo forecast weather)\n# #✅ MAE sin weather histórico: 62.9796\n# #📊 MAE con weather histórico: 61.2636\n# # Eliminar columnas de historical_weather para ver impacto\n\n# print(\"🔄 Modelo #26: Sin histórico weather...\")\n\n# # Filtrar últimos 90 días de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Eliminar columnas de historical_weather (excepto datetime, latitude, longitude, data_block_id)\n# historical_weather_cols = [\n#     'temperature',      # histórico\n#     'dewpoint',         # histórico  \n#     'rain',\n#     'snowfall', \n#     'surface_pressure',\n#     'cloudcover_low',\n#     'cloudcover_mid', \n#     'cloudcover_high',\n#     'cloudcover_total',\n#     'windspeed_10m',\n#     'winddirection_10m', \n#     'shortwave_radiation',\n#     'direct_solar_radiation',\n#     'diffuse_radiation'\n# ]\n\n# # Verificar qué columnas existen y eliminarlas\n# cols_to_drop = [col for col in historical_weather_cols if col in sample_90d.columns]\n# print(f\"🗑️ Eliminando {len(cols_to_drop)} columnas weather históricas:\")\n# for col in cols_to_drop:\n#     print(f\"   - {col}\")\n\n# sample_90d = sample_90d.drop(columns=cols_to_drop)\n\n# # Limpiar NaN en media móvil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (excluye target, datetime, forecast_date, row_id)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"🎯 Features totales: {len(feature_cols)} (sin weather histórico)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE sin weather histórico: {mae:.4f}\")\n# print(f\"📊 MAE con weather histórico: 61.2636\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:56.996467Z","iopub.execute_input":"2025-09-17T20:09:56.996983Z","iopub.status.idle":"2025-09-17T20:09:57.004342Z","shell.execute_reply.started":"2025-09-17T20:09:56.996956Z","shell.execute_reply":"2025-09-17T20:09:57.003012Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# # Modelo #27: Weather lags (variables solares) - 24h lag\n# # ✅ MAE con weather lags 24h: 64.9544\n# # 📊 MAE anterior: 62.9796\n\n\n# print(\"🔄 Modelo #27: Weather lags 24h (variables solares)...\")\n\n# # Filtrar últimos 90 días de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Variables solares para lag 24h\n# solar_vars = ['ftemperature', 'fdirect_solar_radiation', 'fsurface_solar_radiation_downwards', \n#               'fcloudcover_total', 'fsnowfall']\n\n# # Crear lags 24h de variables solares por prediction_unit_id\n# for var in solar_vars:\n#     if var in sample_90d.columns:\n#         sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n        \n#         # Stats del lag\n#         valid_lags = sample_90d[f'{var}_lag24h'].notna().sum()\n#         print(f\"📈 {var}_lag24h válidos: {valid_lags:,}\")\n\n# # Limpiar NaN en media móvil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos weather lags)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"🎯 Features totales: {len(feature_cols)} (incluye weather lags)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con weather lags 24h: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 62.9796\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.005556Z","iopub.execute_input":"2025-09-17T20:09:57.005988Z","iopub.status.idle":"2025-09-17T20:09:57.033203Z","shell.execute_reply.started":"2025-09-17T20:09:57.005962Z","shell.execute_reply":"2025-09-17T20:09:57.032034Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# # Modelo #28: Weather lag 24h - solo ftemperature\n# # ✅ MAE con ftemperature lag 24h: 63.2238\n# # 📊 MAE anterior: 62.9796\n\n\n# print(\"🔄 Modelo #28: Weather lag 24h - solo ftemperature...\")\n\n# # Filtrar últimos 90 días de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h solo de ftemperature\n# if 'ftemperature' in sample_90d.columns:\n#     sample_90d['ftemperature_lag24h'] = sample_90d.groupby('prediction_unit_id')['ftemperature'].shift(24)\n    \n#     # Stats del lag\n#     valid_lags = sample_90d['ftemperature_lag24h'].notna().sum()\n#     print(f\"📈 ftemperature_lag24h válidos: {valid_lags:,}\")\n# else:\n#     print(\"⚠️ ftemperature no encontrada\")\n\n# # Limpiar NaN en media móvil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevo weather lag)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"🎯 Features totales: {len(feature_cols)} (incluye ftemperature lag)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con ftemperature lag 24h: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 62.9796\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.034406Z","iopub.execute_input":"2025-09-17T20:09:57.034772Z","iopub.status.idle":"2025-09-17T20:09:57.066705Z","shell.execute_reply.started":"2025-09-17T20:09:57.034740Z","shell.execute_reply":"2025-09-17T20:09:57.065691Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# # Modelo #29: Weather lag 48h - solo ftemperature\n# # ✅ MAE con ftemperature lag 48h: 62.3245\n# # 📊 MAE anterior: 62.9796\n\n\n# print(\"🔄 Modelo #29: Weather lag 48h - solo ftemperature...\")\n\n# # Filtrar últimos 90 días de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h solo de ftemperature\n# if 'ftemperature' in sample_90d.columns:\n#     sample_90d['ftemperature_lag48h'] = sample_90d.groupby('prediction_unit_id')['ftemperature'].shift(48)\n    \n#     # Stats del lag\n#     valid_lags = sample_90d['ftemperature_lag48h'].notna().sum()\n#     print(f\"📈 ftemperature_lag48h válidos: {valid_lags:,}\")\n# else:\n#     print(\"⚠️ ftemperature no encontrada\")\n\n# # Limpiar NaN en media móvil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevo weather lag)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"🎯 Features totales: {len(feature_cols)} (incluye ftemperature lag)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con ftemperature lag 48h: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 62.9796\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.067839Z","iopub.execute_input":"2025-09-17T20:09:57.068138Z","iopub.status.idle":"2025-09-17T20:09:57.096001Z","shell.execute_reply.started":"2025-09-17T20:09:57.068110Z","shell.execute_reply":"2025-09-17T20:09:57.094623Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# # Modelo 30\n# # #🎉 Variables que MEJORAN (ordenadas por MAE):\n# #    fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n# #    ftemperature_lag48h: 62.3245 (-0.6551)\n# #    fcloudcover_total_lag48h: 62.3520 (-0.6276)\n# def test_weather_lags():\n#     \"\"\"\n#     Prueba cada variable weather con lag 24h y 48h individualmente\n#     \"\"\"\n#     print(\"🔄 Creando todos los weather lags...\")\n    \n#     # Variables solares a probar\n#     solar_vars = ['ftemperature', 'fdirect_solar_radiation', 'fsurface_solar_radiation_downwards', \n#                   'fcloudcover_total', 'fsnowfall']\n    \n#     # Filtrar últimos 90 días de train7\n#     max_date = train7['datetime'].max()\n#     cutoff_90d = max_date - pd.DateOffset(days=90)\n#     sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n    \n#     # Ordenar por prediction_unit_id y datetime\n#     sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n    \n#     # Crear TODOS los lags de una vez\n#     print(\"📊 Creando lags...\")\n#     for var in solar_vars:\n#         if var in sample_90d.columns:\n#             # Lag 24h\n#             sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n#             # Lag 48h  \n#             sample_90d[f'{var}_lag48h'] = sample_90d.groupby('prediction_unit_id')[var].shift(48)\n#             print(f\"   ✅ {var} lags creados\")\n#         else:\n#             print(f\"   ⚠️ {var} no encontrada\")\n    \n#     # Limpiar NaN en media móvil\n#     sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n    \n#     # Features base (sin los nuevos lags)\n#     base_features = [col for col in sample_90d.columns \n#                      if col not in ['target', 'datetime', 'forecast_date', 'row_id'] \n#                      and not col.endswith('_lag24h') and not col.endswith('_lag48h')]\n    \n#     # Split 80/20 una sola vez\n#     split_idx = int(len(sample_90d) * 0.8)\n#     train_data = sample_90d.iloc[:split_idx]\n#     test_data = sample_90d.iloc[split_idx:]\n    \n#     baseline_mae = 62.9796\n#     results = []\n    \n#     print(f\"\\n🧪 Probando cada lag individualmente contra baseline {baseline_mae}...\")\n    \n#     # Probar cada lag individualmente\n#     for var in solar_vars:\n#         if var in sample_90d.columns:\n#             for lag_hours in [24, 48]:\n#                 lag_col = f'{var}_lag{lag_hours}h'\n                \n#                 # Features = base + este lag específico\n#                 test_features = base_features + [lag_col]\n                \n#                 X_train = train_data[test_features].fillna(0)\n#                 y_train = train_data['target']\n#                 X_test = test_data[test_features].fillna(0)\n#                 y_test = test_data['target']\n                \n#                 # Modelo\n#                 model = lgb.LGBMRegressor(\n#                     n_estimators=100,\n#                     max_depth=6,\n#                     learning_rate=0.1,\n#                     random_state=42,\n#                     n_jobs=-1,\n#                     verbose=-1\n#                 )\n                \n#                 model.fit(X_train, y_train)\n#                 pred = model.predict(X_test)\n#                 mae = mean_absolute_error(y_test, pred)\n                \n#                 # A1: Resultado individual\n#                 change = mae - baseline_mae\n#                 status = \"🎉 MEJORA\" if mae < baseline_mae else \"❌ EMPEORA\"\n                \n#                 print(f\"\\n✅ MAE con {var} lag {lag_hours}h: {mae:.4f}\")\n#                 print(f\"📊 Cambio: {change:+.4f} - {status}\")\n                \n#                 # Guardar resultado\n#                 results.append({\n#                     'variable': var,\n#                     'lag_hours': lag_hours,\n#                     'mae': mae,\n#                     'change': change,\n#                     'improves': mae < baseline_mae\n#                 })\n    \n#     # Resumen final\n#     print(\"\\n\" + \"=\"*60)\n#     print(\"📊 RESUMEN DE RESULTADOS:\")\n#     print(\"=\"*60)\n    \n#     # Mejores resultados\n#     improving = [r for r in results if r['improves']]\n#     if improving:\n#         improving.sort(key=lambda x: x['mae'])\n#         print(\"\\n🎉 Variables que MEJORAN (ordenadas por MAE):\")\n#         for r in improving:\n#             print(f\"   {r['variable']}_lag{r['lag_hours']}h: {r['mae']:.4f} ({r['change']:+.4f})\")\n    \n#     # Peores resultados  \n#     worsening = [r for r in results if not r['improves']]\n#     if worsening:\n#         worsening.sort(key=lambda x: x['change'], reverse=True)  # Menos empeoramiento primero\n#         print(f\"\\n❌ Variables que EMPEORAN:\")\n#         for r in worsening:\n#             print(f\"   {r['variable']}_lag{r['lag_hours']}h: {r['mae']:.4f} ({r['change']:+.4f})\")\n    \n#     return results\n\n# # Ejecutar la función\n# results = test_weather_lags()\n\n\n# # #🎉 Variables que MEJORAN (ordenadas por MAE):\n# #    fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n# #    ftemperature_lag48h: 62.3245 (-0.6551)\n# #    fcloudcover_total_lag48h: 62.3520 (-0.6276)\n\n# # ❌ Variables que EMPEORAN:\n# #    fdirect_solar_radiation_lag48h: 67.6610 (+4.6814)\n# #    fsurface_solar_radiation_downwards_lag48h: 65.9643 (+2.9847)\n# #    fsnowfall_lag48h: 65.7171 (+2.7375)\n# #    fsurface_solar_radiation_downwards_lag24h: 65.3518 (+2.3722)\n# #    fcloudcover_total_lag24h: 64.2290 (+1.2494)\n# #    fsnowfall_lag24h: 63.7094 (+0.7298)\n# #    ftemperature_lag24h: 63.2238 (+0.2442)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.097535Z","iopub.execute_input":"2025-09-17T20:09:57.098463Z","iopub.status.idle":"2025-09-17T20:09:57.131611Z","shell.execute_reply.started":"2025-09-17T20:09:57.098420Z","shell.execute_reply":"2025-09-17T20:09:57.130444Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# # Modelo #31: Weather lags (variables solares) \n# # fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n# # ftemperature_lag48h: 62.3245 (-0.6551)\n# # fcloudcover_total_lag48h: 62.3520 (-0.6276)\n\n\n# print(\"🔄 Modelo #31: Weather lags 24h (variables solares)...\")\n\n# # Filtrar últimos 90 días de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Variables solares para lag 24h\n# solar_vars24 = ['fdirect_solar_radiation']\n\n# #Variables solares para lag 48h\n# #solar_vars48 = ['ftemperature', 'fcloudcover_total']\n# #solar_vars48 = ['ftemperature']\n# #solar_vars48 = ['fcloudcover_total']\n\n# #Crear lags 24h de variables solares por prediction_unit_id\n# for var in solar_vars24:\n#     if var in sample_90d.columns:\n#         sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n        \n#         # Stats del lag\n#         valid_lags = sample_90d[f'{var}_lag24h'].notna().sum()\n#         print(f\"📈 {var}_lag24h válidos: {valid_lags:,}\")\n\n# # for var in solar_vars48:\n# #     if var in sample_90d.columns:\n# #         sample_90d[f'{var}_lag48h'] = sample_90d.groupby('prediction_unit_id')[var].shift(48)\n        \n# #         # Stats del lag\n# #         valid_lags1 = sample_90d[f'{var}_lag48h'].notna().sum()\n# #         print(f\"📈 {var}_lag48h válidos: {valid_lags1:,}\")\n\n# # Limpiar NaN en media móvil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos weather lags)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"🎯 Features totales: {len(feature_cols)} (incluye weather lags)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE con weather lags 24 y 48h: {mae:.4f}\")\n# print(f\"📊 MAE anterior: 62.9796\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.132736Z","iopub.execute_input":"2025-09-17T20:09:57.133061Z","iopub.status.idle":"2025-09-17T20:09:57.162905Z","shell.execute_reply.started":"2025-09-17T20:09:57.133033Z","shell.execute_reply":"2025-09-17T20:09:57.161462Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# incorporo fdirect_solar_radiation_lag24h\ntrain8 = train7.sort_values(['prediction_unit_id', 'datetime'])\ntrain8['fdirect_solar_radiation_lag24h'] = train8.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(24)\nprint(\"✅ fdirect_solar_radiation_lag24h agregado a train8\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.164225Z","iopub.execute_input":"2025-09-17T20:09:57.165641Z","iopub.status.idle":"2025-09-17T20:09:58.680949Z","shell.execute_reply.started":"2025-09-17T20:09:57.165595Z","shell.execute_reply":"2025-09-17T20:09:58.678762Z"}},"outputs":[{"name":"stdout","text":"✅ fdirect_solar_radiation_lag24h agregado a train8\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# # Modelo 36\n# # \n# # fdewpoint_lag24h: 64.4357 (+2.8546) ❌ EMPEORA\n# # fdewpoint_lag48h: 67.4056 (+5.8245) ❌ EMPEORA\n# # fcloudcover_high_lag24h: 66.1002 (+4.5191) ❌ EMPEORA\n# # fcloudcover_high_lag48h: 66.3328 (+4.7517) ❌ EMPEORA\n# # fcloudcover_low_lag24h: 62.4589 (+0.8778) ❌ EMPEORA\n# # fcloudcover_low_lag48h: 66.0350 (+4.4539) ❌ EMPEORA\n# # fcloudcover_mid_lag24h: 64.7723 (+3.1912) ❌ EMPEORA\n# # fcloudcover_mid_lag48h: 67.0185 (+5.4374) ❌ EMPEORA\n# # f10_metre_u_wind_component_lag24h: 63.2744 (+1.6933) ❌ EMPEORA\n# # f10_metre_u_wind_component_lag48h: 64.4923 (+2.9112) ❌ EMPEORA\n# # f10_metre_v_wind_component_lag24h: 68.0118 (+6.4307) ❌ EMPEORA\n# # f10_metre_v_wind_component_lag48h: 64.3495 (+2.7684) ❌ EMPEORA\n# # ftotal_precipitation_lag24h: 64.6852 (+3.1041) ❌ EMPEORA\n# # ftotal_precipitation_lag48h: 67.5245 (+5.9434) ❌ EMPEORA\n\n# climate_vars = ['fdewpoint', 'fcloudcover_high', 'fcloudcover_low', 'fcloudcover_mid',\n#                 'f10_metre_u_wind_component', 'f10_metre_v_wind_component', 'ftotal_precipitation']\n\n# # Preparar datos\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear todos los lags climáticos\n# for var in climate_vars:\n#     sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n#     sample_90d[f'{var}_lag48h'] = sample_90d.groupby('prediction_unit_id')[var].shift(48)\n\n# # Limpiar y split\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# # Features base\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id'] \n#                  and not any(f'{var}_lag' in col for var in climate_vars)]\n\n# baseline_mae = 61.5811\n# results = []\n\n# # Probar cada lag individual\n# for var in climate_vars:\n#     for lag_hours in [24, 48]:\n#         lag_col = f'{var}_lag{lag_hours}h'\n#         test_features = base_features + [lag_col]\n        \n#         X_train = train_data[test_features].fillna(0)\n#         X_test = test_data[test_features].fillna(0)\n        \n#         model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                                  random_state=42, n_jobs=-1, verbose=-1)\n#         model.fit(X_train, train_data['target'])\n#         mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n        \n#         change = mae - baseline_mae\n#         status = \"🎉 MEJORA\" if mae < baseline_mae else \"❌ EMPEORA\"\n#         print(f\"{var}_lag{lag_hours}h: {mae:.4f} ({change:+.4f}) {status}\")\n        \n#         results.append({'variable': var, 'lag_hours': lag_hours, 'mae': mae, 'change': change})\n\n# # Mejores resultados\n# improving = [r for r in results if r['mae'] < baseline_mae]\n# if improving:\n#     improving.sort(key=lambda x: x['mae'])\n#     print(\"\\n🎉 MEJORAN:\")\n#     for r in improving:\n#         print(f\"   {r['variable']}_lag{r['lag_hours']}h: {r['mae']:.4f} ({r['change']:+.4f})\")\n# else:\n#     print(\"😔 Ninguna variable climática mejora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.682834Z","iopub.execute_input":"2025-09-17T20:09:58.684043Z","iopub.status.idle":"2025-09-17T20:09:58.691946Z","shell.execute_reply.started":"2025-09-17T20:09:58.683990Z","shell.execute_reply":"2025-09-17T20:09:58.690314Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# # Modelo #37\n# # temp_solar_interaction: 63.5858 (+2.0047) ❌ EMPEORA\n# # cloud_solar_ratio: 64.5474 (+2.9663) ❌ EMPEORA\n# # wind_magnitude: 66.2764 (+4.6953) ❌ EMPEORA\n# # temp_dewpoint_diff: 65.7131 (+4.1320) ❌ EMPEORA\n# # surface_direct_ratio: 65.2653 (+3.6842) ❌ EMPEORA\n# # ftemperature_roll3h: 64.2013 (+2.6202) ❌ EMPEORA\n# # ftemperature_roll6h: 64.3444 (+2.7633) ❌ EMPEORA\n# # fdirect_solar_radiation_roll3h: 64.2987 (+2.7176) ❌ EMPEORA\n# # fdirect_solar_radiation_roll6h: 66.8305 (+5.2494) ❌ EMPEORA\n# # fcloudcover_total_roll3h: 64.4380 (+2.8569) ❌ EMPEORA\n# # fcloudcover_total_roll6h: 65.9477 (+4.3666) ❌ EMPEORA\n\n\n# # Preparar datos base\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # 1. INTERACCIONES WEATHER\n# print(\"🔄 Creando interacciones weather...\")\n# sample_90d['temp_solar_interaction'] = sample_90d['ftemperature'] * sample_90d['fdirect_solar_radiation']\n# sample_90d['cloud_solar_ratio'] = sample_90d['fdirect_solar_radiation'] / (sample_90d['fcloudcover_total'] + 0.01)\n# sample_90d['wind_magnitude'] = np.sqrt(sample_90d['f10_metre_u_wind_component']**2 + sample_90d['f10_metre_v_wind_component']**2)\n\n# # 2. DIFERENCIAS Y RATIOS\n# sample_90d['temp_dewpoint_diff'] = sample_90d['ftemperature'] - sample_90d['fdewpoint']\n# sample_90d['surface_direct_ratio'] = sample_90d['fsurface_solar_radiation_downwards'] / (sample_90d['fdirect_solar_radiation'] + 1)\n\n# # 3. ROLLING FEATURES (ventanas más cortas)\n# for var in ['ftemperature', 'fdirect_solar_radiation', 'fcloudcover_total']:\n#     sample_90d[f'{var}_roll3h'] = sample_90d.groupby('prediction_unit_id')[var].rolling(3, min_periods=1).mean().values\n#     sample_90d[f'{var}_roll6h'] = sample_90d.groupby('prediction_unit_id')[var].rolling(6, min_periods=1).mean().values\n\n# # Limpiar y split\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# # Features base (con fdirect_solar_radiation_lag24h)\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Nuevas features a probar\n# new_features = [\n#     'temp_solar_interaction',\n#     'cloud_solar_ratio', \n#     'wind_magnitude',\n#     'temp_dewpoint_diff',\n#     'surface_direct_ratio',\n#     'ftemperature_roll3h',\n#     'ftemperature_roll6h',\n#     'fdirect_solar_radiation_roll3h',\n#     'fdirect_solar_radiation_roll6h',\n#     'fcloudcover_total_roll3h',\n#     'fcloudcover_total_roll6h'\n# ]\n\n# # Features base sin las nuevas\n# base_only = [col for col in base_features if col not in new_features]\n# baseline_mae = 61.5811\n\n# print(f\"🧪 Probando {len(new_features)} nuevas features...\")\n\n# # Probar cada feature nueva individualmente\n# results = []\n# for feature in new_features:\n#     if feature in sample_90d.columns:\n#         test_features = base_only + [feature]\n        \n#         X_train = train_data[test_features].fillna(0)\n#         X_test = test_data[test_features].fillna(0)\n        \n#         model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                                  random_state=42, n_jobs=-1, verbose=-1)\n#         model.fit(X_train, train_data['target'])\n#         mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n        \n#         change = mae - baseline_mae\n#         status = \"🎉 MEJORA\" if mae < baseline_mae else \"❌ EMPEORA\"\n#         print(f\"{feature}: {mae:.4f} ({change:+.4f}) {status}\")\n        \n#         results.append({'feature': feature, 'mae': mae, 'change': change})\n\n# # Mejores resultados\n# improving = [r for r in results if r['mae'] < baseline_mae]\n# if improving:\n#     improving.sort(key=lambda x: x['mae'])\n#     print(f\"\\n🎉 MEJORAN ({len(improving)}):\")\n#     for r in improving:\n#         print(f\"   {r['feature']}: {r['mae']:.4f} ({r['change']:+.4f})\")\n# else:\n#     # Mostrar las menos malas\n#     results.sort(key=lambda x: x['change'])\n#     print(f\"\\n🔝 TOP 3 menos malas:\")\n#     for r in results[:3]:\n#         print(f\"   {r['feature']}: {r['mae']:.4f} ({r['change']:+.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.693393Z","iopub.execute_input":"2025-09-17T20:09:58.693751Z","iopub.status.idle":"2025-09-17T20:09:58.728007Z","shell.execute_reply.started":"2025-09-17T20:09:58.693717Z","shell.execute_reply":"2025-09-17T20:09:58.726741Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# # Modelo #38 la forma de calculo tiene filtracion de datos y aun asi no funciona\n# # Clustering de prediction_units por comportamiento similar\n# # cluster: 64.9886 (+3.4075) ❌ EMPEORA\n# # cluster_hour_avg_target: 61.8005 (+0.2194) ❌ EMPEORA\n# # 🔥 Probando ambas cluster features juntas...\n# # cluster + cluster_hour_avg_target: 62.4489 (+0.8678) ❌ EMPEORA\n\n# from sklearn.cluster import KMeans\n# from sklearn.preprocessing import StandardScaler\n\n# # 1. CREAR PERFIL DE CADA PREDICTION_UNIT\n# print(\"📊 Creando perfiles de prediction_units...\")\n\n# # Usar train8 completo (150 días) para clustering\n# # Agregaciones por prediction_unit_id para caracterizar comportamiento\n# unit_profiles = train8.groupby('prediction_unit_id').agg({\n#     'target': ['mean', 'std', 'min', 'max'],\n#     'hour': lambda x: x.value_counts().idxmax(),  # hora más común\n#     'is_consumption': 'mean',  # % tiempo consumiendo\n#     'weekend': 'mean',  # % tiempo en weekend  \n#     'installed_capacity': 'first',\n#     'eic_count': 'first',\n#     'is_business': 'first',\n#     'county': 'first'\n# }).round(4)\n\n# # Aplanar nombres de columnas\n# unit_profiles.columns = ['_'.join(col).strip() for col in unit_profiles.columns]\n# unit_profiles = unit_profiles.reset_index()\n\n# # 2. CLUSTERING\n# print(\"🔄 Aplicando KMeans clustering...\")\n\n# # Features para clustering (solo numéricas)\n# cluster_features = [\n#     'target_mean', 'target_std', 'target_min', 'target_max',\n#     'hour_<lambda>', 'is_consumption_mean', 'weekend_mean',\n#     'installed_capacity_first', 'eic_count_first'\n# ]\n\n# # Preparar datos\n# X_cluster = unit_profiles[cluster_features].fillna(0)\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X_cluster)\n\n# # Probar diferentes números de clusters\n# silhouette_scores = []\n# for n_clusters in range(3, 8):\n#     kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n#     labels = kmeans.fit_predict(X_scaled)\n    \n#     from sklearn.metrics import silhouette_score\n#     score = silhouette_score(X_scaled, labels)\n#     silhouette_scores.append((n_clusters, score))\n#     print(f\"   K={n_clusters}: silhouette={score:.3f}\")\n\n# # Mejor número de clusters\n# best_k = max(silhouette_scores, key=lambda x: x[1])[0]\n# print(f\"🎯 Mejor K: {best_k}\")\n\n# # Clustering final\n# kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n# unit_profiles['cluster'] = kmeans_final.fit_predict(X_scaled)\n\n# print(f\"\\n📈 Distribución de clusters:\")\n# print(unit_profiles['cluster'].value_counts().sort_index())\n\n# # 3. AGREGAR CLUSTER AL DATASET ORIGINAL\n# print(\"\\n🔄 Agregando clusters a muestra de 90 días...\")\n\n# # Sample 90 días para testing rápido\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.merge(\n#     unit_profiles[['prediction_unit_id', 'cluster']], \n#     on='prediction_unit_id', \n#     how='left'\n# )\n\n# # Crear features adicionales basadas en cluster\n# cluster_stats = sample_90d.groupby(['cluster', 'hour'])['target'].mean().reset_index()\n# cluster_stats.columns = ['cluster', 'hour', 'cluster_hour_avg_target']\n\n# sample_90d = sample_90d.merge(cluster_stats, on=['cluster', 'hour'], how='left')\n\n# # 4. PROBAR CLUSTER FEATURES\n# print(\"\\n🧪 Probando cluster features...\")\n\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id', 'cluster', 'cluster_hour_avg_target']]\n\n# baseline_mae = 61.5811\n\n# # Probar features de cluster individualmente\n# cluster_features_test = ['cluster', 'cluster_hour_avg_target']\n# results = []\n\n# for feature in cluster_features_test:\n#     test_features = base_features + [feature]\n    \n#     X_train = train_data[test_features].fillna(0)\n#     X_test = test_data[test_features].fillna(0)\n    \n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, train_data['target'])\n#     mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n    \n#     change = mae - baseline_mae\n#     status = \"🎉 MEJORA\" if mae < baseline_mae else \"❌ EMPEORA\"\n#     print(f\"{feature}: {mae:.4f} ({change:+.4f}) {status}\")\n    \n#     results.append({'feature': feature, 'mae': mae, 'change': change})\n\n# # Ambas features juntas\n# print(f\"\\n🔥 Probando ambas cluster features juntas...\")\n# test_features = base_features + cluster_features_test\n\n# X_train = train_data[test_features].fillna(0)\n# X_test = test_data[test_features].fillna(0)\n\n# model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                          random_state=42, n_jobs=-1, verbose=-1)\n# model.fit(X_train, train_data['target'])\n# mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n\n# change = mae - baseline_mae\n# status = \"🎉 MEJORA\" if mae < baseline_mae else \"❌ EMPEORA\"\n# print(f\"cluster + cluster_hour_avg_target: {mae:.4f} ({change:+.4f}) {status}\")\n\n# # Mostrar características de cada cluster\n# print(f\"\\n📊 Perfil de clusters:\")\n# for cluster_id in sorted(unit_profiles['cluster'].unique()):\n#     cluster_data = unit_profiles[unit_profiles['cluster'] == cluster_id]\n#     n_units = len(cluster_data)\n#     avg_target = cluster_data['target_mean_'].mean()\n#     avg_consumption = cluster_data['is_consumption_mean'].mean()\n    \n#     print(f\"   Cluster {cluster_id}: {n_units} units, target_avg={avg_target:.1f}, consumption%={avg_consumption:.1%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.729294Z","iopub.execute_input":"2025-09-17T20:09:58.729666Z","iopub.status.idle":"2025-09-17T20:09:58.750257Z","shell.execute_reply.started":"2025-09-17T20:09:58.729636Z","shell.execute_reply":"2025-09-17T20:09:58.748755Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# #Modelo #39 long_lag_features = ['fdirect_solar_radiation_lag168h', 'fdirect_solar_radiation_lag336h', 'ftemperature_lag168h']\n# # fdirect_solar_radiation_lag168h: 65.1855 (+3.6044) ❌ EMPEORA\n# # fdirect_solar_radiation_lag336h: 65.7386 (+4.1575) ❌ EMPEORA\n# # ftemperature_lag168h: 63.7261 (+2.1450) ❌ EMPEORA\n\n# #LAGS MÁS LARGOS (evitar data leakage)\n# print(\"🕐 Probando lags más largos de variables exitosas...\")\n\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Lags de 7 días (168h) y 14 días (336h) de la variable que ya funciona\n# sample_90d['fdirect_solar_radiation_lag168h'] = sample_90d.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(168)\n# sample_90d['fdirect_solar_radiation_lag336h'] = sample_90d.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(336)\n\n# # También probar otros lags de temperatura que casi funcionaba\n# sample_90d['ftemperature_lag168h'] = sample_90d.groupby('prediction_unit_id')['ftemperature'].shift(168)\n\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id'] \n#                  and not col.endswith('_lag168h') and not col.endswith('_lag336h')]\n\n# baseline_mae = 61.5811\n# long_lag_features = ['fdirect_solar_radiation_lag168h', 'fdirect_solar_radiation_lag336h', 'ftemperature_lag168h']\n\n# for feature in long_lag_features:\n#     test_features = base_features + [feature]\n    \n#     X_train = train_data[test_features].fillna(0)\n#     X_test = test_data[test_features].fillna(0)\n    \n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, train_data['target'])\n#     mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n    \n#     change = mae - baseline_mae\n#     status = \"🎉 MEJORA\" if mae < baseline_mae else \"❌ EMPEORA\"\n#     print(f\"{feature}: {mae:.4f} ({change:+.4f}) {status}\")\n\n# print(\"\\n\" + \"=\"*50)\n\n# print(f\"\\n💰 RESUMEN: Actualmente tienes MAE {baseline_mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.751946Z","iopub.execute_input":"2025-09-17T20:09:58.752292Z","iopub.status.idle":"2025-09-17T20:09:58.780091Z","shell.execute_reply.started":"2025-09-17T20:09:58.752268Z","shell.execute_reply":"2025-09-17T20:09:58.778945Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# # Modelo #40 fdirect_solar_radiation la problematica , train8 tiene 150dias y luego corta a 90dias train8 tiene incorporada la variable\n# #                                                      train7 corta primero a 90dias y luego calcula , incorpora aca la variable\n# #✅ MAE: 64.9886 con train8\n# # MAE: 62.9796 con train7\n\n# # Filtrar últimos 90 días de train8\n# max_date = train8['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train8[train8['datetime'] >= cutoff_90d].copy()\n\n# print(f\"📊 Dataset 90 días: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Limpiar NaN en media móvil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"📊 Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos weather lags)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n✅ MAE: {mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.781436Z","iopub.execute_input":"2025-09-17T20:09:58.781801Z","iopub.status.idle":"2025-09-17T20:09:58.808973Z","shell.execute_reply.started":"2025-09-17T20:09:58.781770Z","shell.execute_reply":"2025-09-17T20:09:58.807742Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Modelo #41 pruebas con distintios dias ,parace haber concept drift\n\n# Días\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7 #modelo 31\n# 100\t63.9248\t\t63.4369\t\t-0.4878\t\tTRAIN8\n# 110\t64.4976\t\t65.3186\t\t+0.8210\t\tTRAIN7\n# 120\t65.7664\t\t66.9320\t\t+1.1656\t\tTRAIN7\n\n# def test_single_cutoff_train7(days):\n#     \"\"\"Probar UN cutoff con método train7\"\"\"\n#     print(f\"\\n🔄 TRAIN7 - {days} días...\")\n    \n#     max_date = train7['datetime'].max()\n#     cutoff = max_date - pd.DateOffset(days=days)\n    \n#     # Cortar primero\n#     sample = train7[train7['datetime'] >= cutoff].copy()\n    \n#     # Agregar lag después\n#     sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n#     sample['fdirect_solar_radiation_lag24h'] = sample.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(24)\n    \n#     # Limpiar\n#     sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n#     feature_cols = [col for col in sample.columns \n#                    if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n#     # Stats\n#     valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n#     total_records = len(sample)\n    \n#     # Split y modelo\n#     split_idx = int(len(sample) * 0.8)\n#     train_data = sample.iloc[:split_idx]\n#     test_data = sample.iloc[split_idx:]\n    \n#     X_train = train_data[feature_cols].fillna(0)\n#     y_train = train_data['target']\n#     X_test = test_data[feature_cols].fillna(0)\n#     y_test = test_data['target']\n    \n#     # Limpiar muestras grandes\n#     del sample, train_data, test_data\n#     gc.collect()\n    \n#     # Modelo\n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, y_train)\n#     mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n#     # Limpiar modelo\n#     del model, X_train, X_test, y_train, y_test\n#     gc.collect()\n    \n#     print(f\"   📈 Registros: {total_records:,} | Lags válidos: {valid_lags:,} | MAE: {mae:.4f}\")\n#     return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\n# def test_single_cutoff_train8(days):\n#     \"\"\"Probar UN cutoff con método train8\"\"\"\n#     print(f\"\\n🔄 TRAIN8 - {days} días...\")\n    \n#     max_date = train8['datetime'].max()\n#     cutoff = max_date - pd.DateOffset(days=days)\n    \n#     # Cortar (lag ya existe)\n#     sample = train8[train8['datetime'] >= cutoff].copy()\n#     sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n#     sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n    \n#     feature_cols = [col for col in sample.columns \n#                    if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n#     # Stats\n#     valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n#     total_records = len(sample)\n    \n#     # Split y modelo\n#     split_idx = int(len(sample) * 0.8)\n#     train_data = sample.iloc[:split_idx]\n#     test_data = sample.iloc[split_idx:]\n    \n#     X_train = train_data[feature_cols].fillna(0)\n#     y_train = train_data['target']\n#     X_test = test_data[feature_cols].fillna(0)\n#     y_test = test_data['target']\n    \n#     # Limpiar muestras grandes\n#     del sample, train_data, test_data\n#     gc.collect()\n    \n#     # Modelo\n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, y_train)\n#     mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n#     # Limpiar modelo\n#     del model, X_train, X_test, y_train, y_test\n#     gc.collect()\n    \n#     print(f\"   📈 Registros: {total_records:,} | Lags válidos: {valid_lags:,} | MAE: {mae:.4f}\")\n#     return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\n# # EJECUTAR DE A UNO\n# print(\"🚀 Probando cutoffs de a uno...\")\n# results = []\n\n# cutoff_days = [90, 100, 110, 120]\n\n# for days in cutoff_days:\n#     print(f\"\\n{'='*50}\")\n#     print(f\"PROBANDO {days} DÍAS\")\n#     print(f\"{'='*50}\")\n    \n#     # Train7\n#     result_t7 = test_single_cutoff_train7(days)\n    \n#     # Train8  \n#     result_t8 = test_single_cutoff_train8(days)\n    \n#     # Comparar\n#     diff = result_t8['mae'] - result_t7['mae']\n#     winner = \"TRAIN7\" if result_t7['mae'] < result_t8['mae'] else \"TRAIN8\"\n    \n#     print(f\"\\n🎯 RESUMEN {days} días:\")\n#     print(f\"   Train7: {result_t7['mae']:.4f}\")\n#     print(f\"   Train8: {result_t8['mae']:.4f}\")\n#     print(f\"   Diferencia: {diff:+.4f}\")\n#     print(f\"   Ganador: {winner}\")\n    \n#     results.append({\n#         'days': days,\n#         'train7_mae': result_t7['mae'],\n#         'train8_mae': result_t8['mae'],\n#         'difference': diff,\n#         'winner': winner\n#     })\n    \n#     # Forzar limpieza memoria\n#     gc.collect()\n\n# # RESUMEN FINAL\n# print(f\"\\n{'='*60}\")\n# print(\"📊 RESUMEN FINAL\")\n# print(f\"{'='*60}\")\n\n# print(\"\\nDías\\tTrain7 MAE\\tTrain8 MAE\\tDiferencia\\tGanador\")\n# print(\"-\" * 55)\n# for r in results:\n#     print(f\"{r['days']}\\t{r['train7_mae']:.4f}\\t\\t{r['train8_mae']:.4f}\\t\\t{r['difference']:+.4f}\\t\\t{r['winner']}\")\n\n# # Mejor configuración general\n# best_overall = min(results, key=lambda x: min(x['train7_mae'], x['train8_mae']))\n# best_mae = min(best_overall['train7_mae'], best_overall['train8_mae'])\n# best_method = \"Train7\" if best_overall['train7_mae'] < best_overall['train8_mae'] else \"Train8\"\n\n# print(f\"\\n🏆 MEJOR CONFIGURACIÓN GENERAL:\")\n# print(f\"   {best_method} con {best_overall['days']} días → MAE {best_mae:.4f}\")\n\n# # Contar victorias\n# train7_wins = sum(1 for r in results if r['winner'] == 'TRAIN7')\n# train8_wins = sum(1 for r in results if r['winner'] == 'TRAIN8')\n\n# print(f\"\\n📊 ESTADÍSTICAS:\")\n# print(f\"   Train7 ganó: {train7_wins}/4 veces\")\n# print(f\"   Train8 ganó: {train8_wins}/4 veces\")\n\n# if train7_wins > train8_wins:\n#     print(\"🎯 CONCLUSIÓN: Método Train7 (corta → lag) es mejor\")\n# elif train8_wins > train7_wins:\n#     print(\"🎯 CONCLUSIÓN: Método Train8 (lag → corta) es mejor\") \n# else:\n#     print(\"🎯 CONCLUSIÓN: Ambos métodos son equivalentes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.810205Z","iopub.execute_input":"2025-09-17T20:09:58.810524Z","iopub.status.idle":"2025-09-17T20:15:33.208429Z","shell.execute_reply.started":"2025-09-17T20:09:58.810500Z","shell.execute_reply":"2025-09-17T20:15:33.206628Z"}},"outputs":[{"name":"stdout","text":"🚀 Probando cutoffs de a uno...\n\n==================================================\nPROBANDO 90 DÍAS\n==================================================\n\n🔄 TRAIN7 - 90 días...\n   📈 Registros: 4,622,184 | Lags válidos: 4,620,552 | MAE: 61.5811\n\n🔄 TRAIN8 - 90 días...\n   📈 Registros: 4,622,184 | Lags válidos: 4,622,184 | MAE: 64.9886\n\n🎯 RESUMEN 90 días:\n   Train7: 61.5811\n   Train8: 64.9886\n   Diferencia: +3.4075\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 100 DÍAS\n==================================================\n\n🔄 TRAIN7 - 100 días...\n   📈 Registros: 5,150,664 | Lags válidos: 5,149,032 | MAE: 63.9248\n\n🔄 TRAIN8 - 100 días...\n   📈 Registros: 5,150,664 | Lags válidos: 5,150,664 | MAE: 63.4369\n\n🎯 RESUMEN 100 días:\n   Train7: 63.9248\n   Train8: 63.4369\n   Diferencia: -0.4878\n   Ganador: TRAIN8\n\n==================================================\nPROBANDO 110 DÍAS\n==================================================\n\n🔄 TRAIN7 - 110 días...\n   📈 Registros: 5,682,913 | Lags válidos: 5,681,305 | MAE: 64.4976\n\n🔄 TRAIN8 - 110 días...\n   📈 Registros: 5,682,913 | Lags válidos: 5,682,913 | MAE: 65.3186\n\n🎯 RESUMEN 110 días:\n   Train7: 64.4976\n   Train8: 65.3186\n   Diferencia: +0.8210\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 120 DÍAS\n==================================================\n\n🔄 TRAIN7 - 120 días...\n   📈 Registros: 6,219,529 | Lags válidos: 6,217,921 | MAE: 65.7664\n\n🔄 TRAIN8 - 120 días...\n   📈 Registros: 6,219,529 | Lags válidos: 6,219,529 | MAE: 66.9320\n\n🎯 RESUMEN 120 días:\n   Train7: 65.7664\n   Train8: 66.9320\n   Diferencia: +1.1656\n   Ganador: TRAIN7\n\n============================================================\n📊 RESUMEN FINAL\n============================================================\n\nDías\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n-------------------------------------------------------\n90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7\n100\t63.9248\t\t63.4369\t\t-0.4878\t\tTRAIN8\n110\t64.4976\t\t65.3186\t\t+0.8210\t\tTRAIN7\n120\t65.7664\t\t66.9320\t\t+1.1656\t\tTRAIN7\n\n🏆 MEJOR CONFIGURACIÓN GENERAL:\n   Train7 con 90 días → MAE 61.5811\n\n📊 ESTADÍSTICAS:\n   Train7 ganó: 3/4 veces\n   Train8 ganó: 1/4 veces\n🎯 CONCLUSIÓN: Método Train7 (corta → lag) es mejor\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Modelo 42\n# Días\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 60\t62.6199\t\t63.9901\t\t+1.3702\t\tTRAIN7\n# 70\t66.6343\t\t62.8047\t\t-3.8296\t\tTRAIN8\n# 80\t62.7194\t\t63.1605\t\t+0.4411\t\tTRAIN7\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7\n\ndef test_single_cutoff_train7(days):\n    \"\"\"Probar UN cutoff con método train7\"\"\"\n    print(f\"\\n🔄 TRAIN7 - {days} días...\")\n    \n    max_date = train7['datetime'].max()\n    cutoff = max_date - pd.DateOffset(days=days)\n    \n    # Cortar primero\n    sample = train7[train7['datetime'] >= cutoff].copy()\n    \n    # Agregar lag después\n    sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n    sample['fdirect_solar_radiation_lag24h'] = sample.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(24)\n    \n    # Limpiar\n    sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n    feature_cols = [col for col in sample.columns \n                   if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n    # Stats\n    valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n    total_records = len(sample)\n    \n    # Split y modelo\n    split_idx = int(len(sample) * 0.8)\n    train_data = sample.iloc[:split_idx]\n    test_data = sample.iloc[split_idx:]\n    \n    X_train = train_data[feature_cols].fillna(0)\n    y_train = train_data['target']\n    X_test = test_data[feature_cols].fillna(0)\n    y_test = test_data['target']\n    \n    # Limpiar muestras grandes\n    del sample, train_data, test_data\n    gc.collect()\n    \n    # Modelo\n    model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n                             random_state=42, n_jobs=-1, verbose=-1)\n    model.fit(X_train, y_train)\n    mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n    # Limpiar modelo\n    del model, X_train, X_test, y_train, y_test\n    gc.collect()\n    \n    print(f\"   📈 Registros: {total_records:,} | Lags válidos: {valid_lags:,} | MAE: {mae:.4f}\")\n    return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\ndef test_single_cutoff_train8(days):\n    \"\"\"Probar UN cutoff con método train8\"\"\"\n    print(f\"\\n🔄 TRAIN8 - {days} días...\")\n    \n    max_date = train8['datetime'].max()\n    cutoff = max_date - pd.DateOffset(days=days)\n    \n    # Cortar (lag ya existe)\n    sample = train8[train8['datetime'] >= cutoff].copy()\n    sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n    sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n    \n    feature_cols = [col for col in sample.columns \n                   if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n    # Stats\n    valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n    total_records = len(sample)\n    \n    # Split y modelo\n    split_idx = int(len(sample) * 0.8)\n    train_data = sample.iloc[:split_idx]\n    test_data = sample.iloc[split_idx:]\n    \n    X_train = train_data[feature_cols].fillna(0)\n    y_train = train_data['target']\n    X_test = test_data[feature_cols].fillna(0)\n    y_test = test_data['target']\n    \n    # Limpiar muestras grandes\n    del sample, train_data, test_data\n    gc.collect()\n    \n    # Modelo\n    model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n                             random_state=42, n_jobs=-1, verbose=-1)\n    model.fit(X_train, y_train)\n    mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n    # Limpiar modelo\n    del model, X_train, X_test, y_train, y_test\n    gc.collect()\n    \n    print(f\"   📈 Registros: {total_records:,} | Lags válidos: {valid_lags:,} | MAE: {mae:.4f}\")\n    return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\n# EJECUTAR DE A UNO\nprint(\"🚀 Probando cutoffs de a uno...\")\nresults = []\n\ncutoff_days = [60, 70, 80, 90]\n\nfor days in cutoff_days:\n    print(f\"\\n{'='*50}\")\n    print(f\"PROBANDO {days} DÍAS\")\n    print(f\"{'='*50}\")\n    \n    # Train7\n    result_t7 = test_single_cutoff_train7(days)\n    \n    # Train8  \n    result_t8 = test_single_cutoff_train8(days)\n    \n    # Comparar\n    diff = result_t8['mae'] - result_t7['mae']\n    winner = \"TRAIN7\" if result_t7['mae'] < result_t8['mae'] else \"TRAIN8\"\n    \n    print(f\"\\n🎯 RESUMEN {days} días:\")\n    print(f\"   Train7: {result_t7['mae']:.4f}\")\n    print(f\"   Train8: {result_t8['mae']:.4f}\")\n    print(f\"   Diferencia: {diff:+.4f}\")\n    print(f\"   Ganador: {winner}\")\n    \n    results.append({\n        'days': days,\n        'train7_mae': result_t7['mae'],\n        'train8_mae': result_t8['mae'],\n        'difference': diff,\n        'winner': winner\n    })\n    \n    # Forzar limpieza memoria\n    gc.collect()\n\n# RESUMEN FINAL\nprint(f\"\\n{'='*60}\")\nprint(\"📊 RESUMEN FINAL\")\nprint(f\"{'='*60}\")\n\nprint(\"\\nDías\\tTrain7 MAE\\tTrain8 MAE\\tDiferencia\\tGanador\")\nprint(\"-\" * 55)\nfor r in results:\n    print(f\"{r['days']}\\t{r['train7_mae']:.4f}\\t\\t{r['train8_mae']:.4f}\\t\\t{r['difference']:+.4f}\\t\\t{r['winner']}\")\n\n# Mejor configuración general\nbest_overall = min(results, key=lambda x: min(x['train7_mae'], x['train8_mae']))\nbest_mae = min(best_overall['train7_mae'], best_overall['train8_mae'])\nbest_method = \"Train7\" if best_overall['train7_mae'] < best_overall['train8_mae'] else \"Train8\"\n\nprint(f\"\\n🏆 MEJOR CONFIGURACIÓN GENERAL:\")\nprint(f\"   {best_method} con {best_overall['days']} días → MAE {best_mae:.4f}\")\n\n# Contar victorias\ntrain7_wins = sum(1 for r in results if r['winner'] == 'TRAIN7')\ntrain8_wins = sum(1 for r in results if r['winner'] == 'TRAIN8')\n\nprint(f\"\\n📊 ESTADÍSTICAS:\")\nprint(f\"   Train7 ganó: {train7_wins}/4 veces\")\nprint(f\"   Train8 ganó: {train8_wins}/4 veces\")\n\nif train7_wins > train8_wins:\n    print(\"🎯 CONCLUSIÓN: Método Train7 (corta → lag) es mejor\")\nelif train8_wins > train7_wins:\n    print(\"🎯 CONCLUSIÓN: Método Train8 (lag → corta) es mejor\") \nelse:\n    print(\"🎯 CONCLUSIÓN: Ambos métodos son equivalentes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:32:54.403715Z","iopub.execute_input":"2025-09-17T20:32:54.404311Z","iopub.status.idle":"2025-09-17T20:36:45.948509Z","shell.execute_reply.started":"2025-09-17T20:32:54.404280Z","shell.execute_reply":"2025-09-17T20:36:45.947556Z"}},"outputs":[{"name":"stdout","text":"🚀 Probando cutoffs de a uno...\n\n==================================================\nPROBANDO 60 DÍAS\n==================================================\n\n🔄 TRAIN7 - 60 días...\n   📈 Registros: 3,069,912 | Lags válidos: 3,068,304 | MAE: 62.6199\n\n🔄 TRAIN8 - 60 días...\n   📈 Registros: 3,069,912 | Lags válidos: 3,069,912 | MAE: 63.9901\n\n🎯 RESUMEN 60 días:\n   Train7: 62.6199\n   Train8: 63.9901\n   Diferencia: +1.3702\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 70 DÍAS\n==================================================\n\n🔄 TRAIN7 - 70 días...\n   📈 Registros: 3,585,384 | Lags válidos: 3,583,752 | MAE: 66.6343\n\n🔄 TRAIN8 - 70 días...\n   📈 Registros: 3,585,384 | Lags válidos: 3,585,384 | MAE: 62.8047\n\n🎯 RESUMEN 70 días:\n   Train7: 66.6343\n   Train8: 62.8047\n   Diferencia: -3.8296\n   Ganador: TRAIN8\n\n==================================================\nPROBANDO 80 DÍAS\n==================================================\n\n🔄 TRAIN7 - 80 días...\n   📈 Registros: 4,103,784 | Lags válidos: 4,102,152 | MAE: 62.7194\n\n🔄 TRAIN8 - 80 días...\n   📈 Registros: 4,103,784 | Lags válidos: 4,103,784 | MAE: 63.1605\n\n🎯 RESUMEN 80 días:\n   Train7: 62.7194\n   Train8: 63.1605\n   Diferencia: +0.4411\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 90 DÍAS\n==================================================\n\n🔄 TRAIN7 - 90 días...\n   📈 Registros: 4,622,184 | Lags válidos: 4,620,552 | MAE: 61.5811\n\n🔄 TRAIN8 - 90 días...\n   📈 Registros: 4,622,184 | Lags válidos: 4,622,184 | MAE: 64.9886\n\n🎯 RESUMEN 90 días:\n   Train7: 61.5811\n   Train8: 64.9886\n   Diferencia: +3.4075\n   Ganador: TRAIN7\n\n============================================================\n📊 RESUMEN FINAL\n============================================================\n\nDías\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n-------------------------------------------------------\n60\t62.6199\t\t63.9901\t\t+1.3702\t\tTRAIN7\n70\t66.6343\t\t62.8047\t\t-3.8296\t\tTRAIN8\n80\t62.7194\t\t63.1605\t\t+0.4411\t\tTRAIN7\n90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7\n\n🏆 MEJOR CONFIGURACIÓN GENERAL:\n   Train7 con 90 días → MAE 61.5811\n\n📊 ESTADÍSTICAS:\n   Train7 ganó: 3/4 veces\n   Train8 ganó: 1/4 veces\n🎯 CONCLUSIÓN: Método Train7 (corta → lag) es mejor\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"train7.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.210237Z","iopub.execute_input":"2025-09-17T20:15:33.210586Z","iopub.status.idle":"2025-09-17T20:15:33.247030Z","shell.execute_reply.started":"2025-09-17T20:15:33.210560Z","shell.execute_reply":"2025-09-17T20:15:33.246069Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n0  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n1  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n2  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n3  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n4  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n\n   data_block_id  prediction_unit_id  is_business  product_type  county  \\\n0            486                   0            0             1       0   \n1            486                   0            0             1       0   \n2            486                   0            0             1       0   \n3            486                   0            0             1       0   \n4            486                   0            0             1       0   \n\n    latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n0  59.099998  24.200001                  74.0              86.199997   \n1  59.099998  25.200001                  74.0              86.199997   \n2  59.400002  22.700001                  74.0              86.199997   \n3  59.400002  23.200001                  74.0              86.199997   \n4  59.400002  23.700001                  74.0              86.199997   \n\n   euros_per_mwh  eic_count  installed_capacity  ftemperature  fdewpoint  \\\n0           3.29      417.0         4001.080078      2.458154   1.860742   \n1           3.29      417.0         4001.080078      1.464502   0.857690   \n2           3.29      417.0         4001.080078      5.330591   3.928491   \n3           3.29      417.0         4001.080078      4.218042   2.536279   \n4           3.29      417.0         4001.080078      3.586084   2.155054   \n\n   fcloudcover_high  fcloudcover_low  fcloudcover_mid  fcloudcover_total  \\\n0          0.989075              1.0         1.000000           0.999996   \n1          0.897797              1.0         1.000000           0.999996   \n2          0.064026              1.0         0.054596           0.999996   \n3          0.132874              1.0         0.371002           0.999996   \n4          0.618713              1.0         1.000000           0.999996   \n\n   f10_metre_u_wind_component  f10_metre_v_wind_component  \\\n0                    0.320052                    8.630277   \n1                    0.415511                    7.309232   \n2                    4.160628                   12.238919   \n3                    1.823714                   13.567777   \n4                    0.730208                   12.648099   \n\n   fdirect_solar_radiation  fsurface_solar_radiation_downwards  fsnowfall  \\\n0                -0.008889                            5.534878   0.000023   \n1                 0.008889                           15.983768   0.000063   \n2                 0.008889                            5.841545   0.000000   \n3                 0.000000                            5.081545   0.000000   \n4                 0.000000                            3.925990   0.000000   \n\n   ftotal_precipitation  weather_forecast_hour  is_consumption  day_of_week  \\\n0              0.000508                   33.0               0            5   \n1              0.000489                   33.0               0            5   \n2              0.000222                   33.0               0            5   \n3              0.000197                   33.0               0            5   \n4              0.000252                   33.0               0            5   \n\n   month  is_daylight  weekend  capacity_per_obs  production_obs_ratio  \\\n0     12            0        1          0.016344                   0.5   \n1     12            0        1          0.016344                   0.5   \n2     12            0        1          0.016344                   0.5   \n3     12            0        1          0.016344                   0.5   \n4     12            0        1          0.016344                   0.5   \n\n   consumption_obs_ratio  capacity_per_capita  \\\n0                    0.5             9.594916   \n1                    0.5             9.594916   \n2                    0.5             9.594916   \n3                    0.5             9.594916   \n4                    0.5             9.594916   \n\n   electricity_price_volatility_7d  gas_price_volatility_7d  \\\n0                              0.0                      0.0   \n1                              0.0                      0.0   \n2                              0.0                      0.0   \n3                              0.0                      0.0   \n4                              0.0                      0.0   \n\n   target_ma_7d_lag5d  \n0                 NaN  \n1                 NaN  \n2                 NaN  \n3                 NaN  \n4                 NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>fdewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>fdirect_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>fsnowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n      <th>capacity_per_obs</th>\n      <th>production_obs_ratio</th>\n      <th>consumption_obs_ratio</th>\n      <th>capacity_per_capita</th>\n      <th>electricity_price_volatility_7d</th>\n      <th>gas_price_volatility_7d</th>\n      <th>target_ma_7d_lag5d</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>2.458154</td>\n      <td>1.860742</td>\n      <td>0.989075</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.320052</td>\n      <td>8.630277</td>\n      <td>-0.008889</td>\n      <td>5.534878</td>\n      <td>0.000023</td>\n      <td>0.000508</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>1.464502</td>\n      <td>0.857690</td>\n      <td>0.897797</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.415511</td>\n      <td>7.309232</td>\n      <td>0.008889</td>\n      <td>15.983768</td>\n      <td>0.000063</td>\n      <td>0.000489</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>5.330591</td>\n      <td>3.928491</td>\n      <td>0.064026</td>\n      <td>1.0</td>\n      <td>0.054596</td>\n      <td>0.999996</td>\n      <td>4.160628</td>\n      <td>12.238919</td>\n      <td>0.008889</td>\n      <td>5.841545</td>\n      <td>0.000000</td>\n      <td>0.000222</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>4.218042</td>\n      <td>2.536279</td>\n      <td>0.132874</td>\n      <td>1.0</td>\n      <td>0.371002</td>\n      <td>0.999996</td>\n      <td>1.823714</td>\n      <td>13.567777</td>\n      <td>0.000000</td>\n      <td>5.081545</td>\n      <td>0.000000</td>\n      <td>0.000197</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>3.586084</td>\n      <td>2.155054</td>\n      <td>0.618713</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.730208</td>\n      <td>12.648099</td>\n      <td>0.000000</td>\n      <td>3.925990</td>\n      <td>0.000000</td>\n      <td>0.000252</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"train8.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.248221Z","iopub.execute_input":"2025-09-17T20:15:33.248568Z","iopub.status.idle":"2025-09-17T20:15:33.294053Z","shell.execute_reply.started":"2025-09-17T20:15:33.248533Z","shell.execute_reply":"2025-09-17T20:15:33.292447Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n0  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n1  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n2  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n3  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n4  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n\n   data_block_id  prediction_unit_id  is_business  product_type  county  \\\n0            486                   0            0             1       0   \n1            486                   0            0             1       0   \n2            486                   0            0             1       0   \n3            486                   0            0             1       0   \n4            486                   0            0             1       0   \n\n    latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n0  59.099998  24.200001                  74.0              86.199997   \n1  59.099998  25.200001                  74.0              86.199997   \n2  59.400002  22.700001                  74.0              86.199997   \n3  59.400002  23.200001                  74.0              86.199997   \n4  59.400002  23.700001                  74.0              86.199997   \n\n   euros_per_mwh  eic_count  installed_capacity  ftemperature  fdewpoint  \\\n0           3.29      417.0         4001.080078      2.458154   1.860742   \n1           3.29      417.0         4001.080078      1.464502   0.857690   \n2           3.29      417.0         4001.080078      5.330591   3.928491   \n3           3.29      417.0         4001.080078      4.218042   2.536279   \n4           3.29      417.0         4001.080078      3.586084   2.155054   \n\n   fcloudcover_high  fcloudcover_low  fcloudcover_mid  fcloudcover_total  \\\n0          0.989075              1.0         1.000000           0.999996   \n1          0.897797              1.0         1.000000           0.999996   \n2          0.064026              1.0         0.054596           0.999996   \n3          0.132874              1.0         0.371002           0.999996   \n4          0.618713              1.0         1.000000           0.999996   \n\n   f10_metre_u_wind_component  f10_metre_v_wind_component  \\\n0                    0.320052                    8.630277   \n1                    0.415511                    7.309232   \n2                    4.160628                   12.238919   \n3                    1.823714                   13.567777   \n4                    0.730208                   12.648099   \n\n   fdirect_solar_radiation  fsurface_solar_radiation_downwards  fsnowfall  \\\n0                -0.008889                            5.534878   0.000023   \n1                 0.008889                           15.983768   0.000063   \n2                 0.008889                            5.841545   0.000000   \n3                 0.000000                            5.081545   0.000000   \n4                 0.000000                            3.925990   0.000000   \n\n   ftotal_precipitation  weather_forecast_hour  is_consumption  day_of_week  \\\n0              0.000508                   33.0               0            5   \n1              0.000489                   33.0               0            5   \n2              0.000222                   33.0               0            5   \n3              0.000197                   33.0               0            5   \n4              0.000252                   33.0               0            5   \n\n   month  is_daylight  weekend  capacity_per_obs  production_obs_ratio  \\\n0     12            0        1          0.016344                   0.5   \n1     12            0        1          0.016344                   0.5   \n2     12            0        1          0.016344                   0.5   \n3     12            0        1          0.016344                   0.5   \n4     12            0        1          0.016344                   0.5   \n\n   consumption_obs_ratio  capacity_per_capita  \\\n0                    0.5             9.594916   \n1                    0.5             9.594916   \n2                    0.5             9.594916   \n3                    0.5             9.594916   \n4                    0.5             9.594916   \n\n   electricity_price_volatility_7d  gas_price_volatility_7d  \\\n0                              0.0                      0.0   \n1                              0.0                      0.0   \n2                              0.0                      0.0   \n3                              0.0                      0.0   \n4                              0.0                      0.0   \n\n   target_ma_7d_lag5d  fdirect_solar_radiation_lag24h  \n0                 NaN                             NaN  \n1                 NaN                             NaN  \n2                 NaN                             NaN  \n3                 NaN                             NaN  \n4                 NaN                             NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>fdewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>fdirect_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>fsnowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n      <th>capacity_per_obs</th>\n      <th>production_obs_ratio</th>\n      <th>consumption_obs_ratio</th>\n      <th>capacity_per_capita</th>\n      <th>electricity_price_volatility_7d</th>\n      <th>gas_price_volatility_7d</th>\n      <th>target_ma_7d_lag5d</th>\n      <th>fdirect_solar_radiation_lag24h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>2.458154</td>\n      <td>1.860742</td>\n      <td>0.989075</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.320052</td>\n      <td>8.630277</td>\n      <td>-0.008889</td>\n      <td>5.534878</td>\n      <td>0.000023</td>\n      <td>0.000508</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>1.464502</td>\n      <td>0.857690</td>\n      <td>0.897797</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.415511</td>\n      <td>7.309232</td>\n      <td>0.008889</td>\n      <td>15.983768</td>\n      <td>0.000063</td>\n      <td>0.000489</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>5.330591</td>\n      <td>3.928491</td>\n      <td>0.064026</td>\n      <td>1.0</td>\n      <td>0.054596</td>\n      <td>0.999996</td>\n      <td>4.160628</td>\n      <td>12.238919</td>\n      <td>0.008889</td>\n      <td>5.841545</td>\n      <td>0.000000</td>\n      <td>0.000222</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>4.218042</td>\n      <td>2.536279</td>\n      <td>0.132874</td>\n      <td>1.0</td>\n      <td>0.371002</td>\n      <td>0.999996</td>\n      <td>1.823714</td>\n      <td>13.567777</td>\n      <td>0.000000</td>\n      <td>5.081545</td>\n      <td>0.000000</td>\n      <td>0.000197</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>3.586084</td>\n      <td>2.155054</td>\n      <td>0.618713</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.730208</td>\n      <td>12.648099</td>\n      <td>0.000000</td>\n      <td>3.925990</td>\n      <td>0.000000</td>\n      <td>0.000252</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"#Mae puesto n1 kaggle 52.3090\n#1er primer modelo xgb sobre train5 haciendo split 80/20 de train y test \n#Mae = 107.9896\n#sin lag sin feature, sin nada , solo haciendo el merge\n\n\n#agrego day of week y month para todos los siguientes\n\n#2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# estas variables de lags las quito\n\n#agrego is_daylight binaria\n\n#4to modelo lgb 14.2% y sobre esto split 80/20 de train y test  \n#de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses 14.2% aprox\n# se que no deberia ser comparable al usar de base distinto , pero parte de aca ahora\n#Mae = 86.6747\n\n#agrego weekend binaria\n\n#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n\n#para todos los demas modelos ya tienen incluido\n#day of week\n#month\n#is_daylight\n#weekend\n\n#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo la saco por ahora a is_active_hours\n\n#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que  la saco a is_sleeping_hours\n\n#empiezo a probar con variables\n##Energéticas:\n\n# Ratio production/consumption por prediction_unit_id (histórico) # no se pudo probar\n# Capacidad instalada per capita (installed_capacity / eic_count) # si mejora el modelo\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n\n#9no modelo lgb  mantengo las 3 anteriores y prueba una nueva Ratio production/cn\n# no optimizado no pudo correr\n\n#10mo modelo lgb mantengo las 3 anteriores y pruebo  Capacidad instalada per capita (installed_capacity / eic_count)\n#Mae = 72.3572 mejora el modelo \n\n#para los demas modelos sigo usando las siguientes variables\n#capacity_per_obs\n#production_obs_ratio\n#consumption_obs_ratio\n#capacity_per_capita\n\n#11vo modelo lgb con esto #Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)#\n#Mae anterior = 72.3572\n#Mae con eficiencia de paneles = 85.0309 empeora , pruebo con lag de esto  y sino no la uso por ahora\n\n\n#12vo modelo lgb , lo anterior pero ahora efficiency panel es log , lo saco\n# ✅ MAE con panel efficiency LOG: 85.3200\n# 📊 MAE anterior: 72.3572\n\n\n# #13vo modelo probando volatilidad de precio de electricidad 7 dias\n# # ✅ MAE con electricity price volatility: 73.2246 si , falta probar mas versiones y gas\n# # 📊 MAE anterior: 72.3572\n\n#14vo modelo ademas de electricity_volatility pruebo con gas\n# ✅ MAE con electricity + gas price volatility: 70.6517 se queda\n# 📊 MAE anterior: 73.2246\n\n#creo train6 de 150 dias para agregar\n#capacity_per_obs\n#production_obs_ratio\n#consumption_obs_ratio\n#capacity_per_capita\n#elect price volatility\n#gas price volatility\n\n#15vo probar gas y/o electricity de 30 dias , agrega mucho ruido no sirve\n\n# 16vo modelo Lag 24h de target por prediction_unit_id , no va\n#✅ MAE con lag 24h (90 días): 104.3003\n#📊 MAE anterior: 70.6517\n\n# 17vo modelo Lag 48h de target por prediction_unit_id , no va\n#✅ MAE con lag 48h (90 días): 111.3501\n#📊 MAE anterior: 70.6517\n\n# # Modelo #18: Cyclical encoding de hora (hour_sin, hour_cos)\n# ✅ MAE con hour_sin/cos: 122.4835 # algo esta pasado ver como corregirlo\n# 📊 MAE anterior: 70.6517\n\n# Modelo #19: Cyclical encoding de semana (week_sin, week_cos)\n#✅ MAE con week_sin/cos: 118.3710 los temporales , de la forma que los planteo no estan funcionando\n#📊 MAE anterior: 70.6517\n\n# Modelo #20: Cambios día a día (delta vs día anterior) 'temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation'\n# ✅ MAE con deltas weather 24h: 124.1003 algo mal estoy haciendo o el overfit es grande por 90dias ,capaz con mas dias menos overfit\n# 📊 MAE anterior: 70.6517\n\n# Modelo #21: Media móvil de target por prediction_unit_id\n# ✅ MAE con media móvil 7d: 56.1320 MMM data leakage por como estan los datos , veo como lo cambio\n# 📊 MAE anterior: 70.6517\n\n# Modelo #22: Media móvil de target por prediction_unit_id lag 5 dias\n# ✅ MAE con media móvil 7d (lag 5d): 61.2636\n# 📊 MAE anterior: 70.6517\n\n# Modelo #23: Media móvil de target por prediction_unit_id lag 5 dias  \n# voy a probar con una sola de las 2 y tambien probar de 5 dias para ser consistente con el lag\n#✅ MAE con ambas media móvil 7d y 3d (lag 5d): 64.0943\n#📊 MAE anterior: 61.2636\n\n \n# Modelo #24: Media móvil de target por prediction_unit_id lag 5 dias\n#✅ MAE con media móvil 3d (lag 5d): 63.5897 # sigue siendo mejor solo la de 7 dias\n#📊 MAE anterior: 61.2636\n\n# Modelo #25: Media móvil de target por prediction_unit_id lag 5 dias\n#✅ MAE con media móvil 5d (lag 5d): 71.2301 # sigue siendo mejor solo la de 7 dias\n#📊 MAE anterior: 61.2636\n\n#creo train7 que contiene la media movil de 7d con lag de 5d\n\n# Modelo #26 (deberia haber sido antes) forecast Weather vs historical Weather (historical no esta en test)\n# teniendo en cuenta varias cosas esto deberia haber sido hecho antes , tanto por memoria/optimizacion/forma prolijas de trabajar y etc\n# mismo el pipeline deberia haber sido hecho solo con info de los ultimos 6 meses para entrenar mas rapido y no cortarlo despues\n# pese a los varios e importantes errores que tuve sigo probando unas cosas mas en este dataset\n# agrego esto a cuando creo train7 y sigo ese como base\n#✅ MAE sin weather histórico: 62.9796\n#📊 MAE con weather histórico: 61.2636\n\n# Modelo #27: Weather lags (variables solares) - 24h lag\n# ✅ MAE con weather lags 24h: 64.9544 #prueba de 5 a la vez , no se si alguna reduce y las otras son ruido , paso a probar de a 1\n# 📊 MAE anterior: 62.9796\n\n# Modelo #28: Weather lag 24h - solo ftemperature\n# ✅ MAE con ftemperature lag 24h: 63.2238 # nop , pruebo 48h y sigo con la siguiente\n# 📊 MAE anterior: 62.9796\n\n# # Modelo #29: Weather lag 48h - solo ftemperature\n# # ✅ MAE con ftemperature lag 48h: 62.3245 #nop , pruebo los demas , igual tendria que hacer una funcion que haga esto con los otras variables \n# #  y en 24h y 48h\n# # 📊 MAE anterior: 62.9796\n\n# Modelo #30\n# #🎉 Variables que MEJORAN (ordenadas por MAE):\n#    fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n#     ftemperature_lag48h y: 62.3245 (-0.6551)\n#    fcloudcover_total_lag48h: 62.3520 (-0.6276)\n\n#la verdad es que deberia automatizar esto como #mlxtend.feature_selection.SequentialFeatureSelector\n#cuando vaya a probar las otras variables que no sean solares lo pruebo\n\n# Modelo #31 probando las 3 de arriba juntas ,empeoran el modelo\n# ✅ MAE con weather lags 24 y 48h: 67.3127\n# 📊 MAE anterior: 62.9796\n\n# Modelo #32 agrego fdirect_solar_radiation_lag24h\n# ✅ MAE con weather lags 24h : 61.5811\n# 📊 MAE anterior: 62.9796\n\n# Modelo #33 pruebo ftemperature_lag48h y fcloudcover_total_lag48h\n# ✅ MAE con weather lags 24 y 48h: 62.3582\n# 📊 MAE anterior: 62.9796\n\n# Modelo #34 pruebo fdirect_solar_radiation_lag24h y ftemperature_lag48h\n# ✅ MAE con weather lags 24 y 48h: 64.1737\n# 📊 MAE anterior: 62.9796\n\n# Modelo #35 pruebo fdirect_solar_radiation_lag24h y fcloudcover_total_lag48h me quedo conn el modelo #32\n# ✅ MAE con weather lags 24 y 48h: 62.3281\n# 📊 MAE anterior: 62.9796\n \n# agrego fdirect_solar_radiation_lag24h a train7 pasa a ser train8 \n\n\n# Modelo #36 \n# fdewpoint_lag24h: 64.4357 (+2.8546) ❌ EMPEORA\n# fdewpoint_lag48h: 67.4056 (+5.8245) ❌ EMPEORA\n# fcloudcover_high_lag24h: 66.1002 (+4.5191) ❌ EMPEORA\n# fcloudcover_high_lag48h: 66.3328 (+4.7517) ❌ EMPEORA\n# fcloudcover_low_lag24h: 62.4589 (+0.8778) ❌ EMPEORA\n# fcloudcover_low_lag48h: 66.0350 (+4.4539) ❌ EMPEORA\n# fcloudcover_mid_lag24h: 64.7723 (+3.1912) ❌ EMPEORA\n# fcloudcover_mid_lag48h: 67.0185 (+5.4374) ❌ EMPEORA\n# f10_metre_u_wind_component_lag24h: 63.2744 (+1.6933) ❌ EMPEORA\n# f10_metre_u_wind_component_lag48h: 64.4923 (+2.9112) ❌ EMPEORA\n# f10_metre_v_wind_component_lag24h: 68.0118 (+6.4307) ❌ EMPEORA\n# f10_metre_v_wind_component_lag48h: 64.3495 (+2.7684) ❌ EMPEORA\n# ftotal_precipitation_lag24h: 64.6852 (+3.1041) ❌ EMPEORA\n# ftotal_precipitation_lag48h: 67.5245 (+5.9434) ❌ EMPEORA\n\n# Modelo #37\n# # temp_solar_interaction: 63.5858 (+2.0047) ❌ EMPEORA\n# # cloud_solar_ratio: 64.5474 (+2.9663) ❌ EMPEORA\n# # wind_magnitude: 66.2764 (+4.6953) ❌ EMPEORA\n# # temp_dewpoint_diff: 65.7131 (+4.1320) ❌ EMPEORA\n# # surface_direct_ratio: 65.2653 (+3.6842) ❌ EMPEORA\n# # ftemperature_roll3h: 64.2013 (+2.6202) ❌ EMPEORA\n# # ftemperature_roll6h: 64.3444 (+2.7633) ❌ EMPEORA\n# # fdirect_solar_radiation_roll3h: 64.2987 (+2.7176) ❌ EMPEORA\n# # fdirect_solar_radiation_roll6h: 66.8305 (+5.2494) ❌ EMPEORA\n# # fcloudcover_total_roll3h: 64.4380 (+2.8569) ❌ EMPEORA\n# # fcloudcover_total_roll6h: 65.9477 (+4.3666) ❌ EMPEORA\n\n# # Modelo #38 la forma de calculo tiene filtracion de datos y aun asi no funciona\n# # Clustering de prediction_units por comportamiento similar\n# # cluster: 64.9886 (+3.4075) ❌ EMPEORA\n# # cluster_hour_avg_target: 61.8005 (+0.2194) ❌ EMPEORA\n# # 🔥 Probando ambas cluster features juntas...\n# # cluster + cluster_hour_avg_target: 62.4489 (+0.8678) ❌ EMPEORA\n\n# Modelo #39 long_lag_features = ['fdirect_solar_radiation_lag168h', 'fdirect_solar_radiation_lag336h', 'ftemperature_lag168h']\n# fdirect_solar_radiation_lag168h: 65.1855 (+3.6044) ❌ EMPEORA\n# fdirect_solar_radiation_lag336h: 65.7386 (+4.1575) ❌ EMPEORA\n# ftemperature_lag168h: 63.7261 (+2.1450) ❌ EMPEORA\n\n\n# Modelo #40 fdirect_solar_radiation la problematica , train8 tiene 150dias y luego corta a 90dias train8 tiene incorporada la variable\n#                                                      train7 corta primero a 90dias y luego calcula , incorpora aca la variable\n#✅ MAE: 64.9886 con train8\n# MAE: 62.9796 con train7\n\n\n# Modelo #41 pruebas con distintios dias ,parace haber concept drift\n\n# Días\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7 #modelo 31\n# 100\t63.9248\t\t63.4369\t\t-0.4878\t\tTRAIN8\n# 110\t64.4976\t\t65.3186\t\t+0.8210\t\tTRAIN7\n# 120\t65.7664\t\t66.9320\t\t+1.1656\t\tTRAIN7\n\n# Modelo 42 no ayuda y puede ser un cambio en los patrones subyacentes\n# Días\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 60\t62.6199\t\t63.9901\t\t+1.3702\t\tTRAIN7\n# 70\t66.6343\t\t62.8047\t\t-3.8296\t\tTRAIN8\n# 80\t62.7194\t\t63.1605\t\t+0.4411\t\tTRAIN7\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7 #modelo 31","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.295270Z","iopub.execute_input":"2025-09-17T20:15:33.295656Z","iopub.status.idle":"2025-09-17T20:15:33.309513Z","shell.execute_reply.started":"2025-09-17T20:15:33.295628Z","shell.execute_reply":"2025-09-17T20:15:33.307981Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Ratio production/consumption por prediction_unit_id (histórico) #no se pudo probar\n# Capacidad instalada per capita (installed_capacity / eic_count) # si mejora el modelo\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False) # empeora el modelo\n# Volatilidad de precios (rolling std de electricity/gas prices) # mejor poco 1 cosa falta probar mas\n# Volatilidad individual + combinada\n# electricity_price_volatility_7d = rolling(7).std() # si funciona bien\n# gas_price_volatility_7d = rolling(7).std() # si funciona bien\n\n\n\n##Temporales:\n\n# Lag de consumo/producción del mismo prediction_unit_id (24h, 48h, 168h)# empeoran el modelo\n# hour_sin y hour_cos # no va\n# week_sin, week_cos # no va\n# Media móvil de target por prediction_unit_id # \n# Cambios día a día (delta vs día anterior) ['temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation']# no funciono\n# Media móvil de target por prediction_unit_id lag 5 dias si y muy bien\n\n##Weather engineering:\n\n# weather lag #['ftemperature', 'fdirect_solar_radiation', 'fsurface_solar_radiation_downwards', \n              #'fcloudcover_total', 'fsnowfall'] pruebo estas primero con 24h y 48h\n# Índice de confort térmico (combinando temp + humidity) # nop\n# Potencial solar real (direct + diffuse radiation ajustado por cloudcover) #nop\n# Diferencia forecast vs historical weather (para medir accuracy del forecast) # lo elimine porque el historico no esta en el test set\n\n##Segmentación:\n\n# Clustering de prediction_units por comportamiento similar\n# Ratios county-level (agregaciones por county vs individual)\n\n\n\n\n#para la red neuronal probar lo siguiente\n#MLP\n#DeepAR\n#TFT (temporal fusion transformer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.311292Z","iopub.execute_input":"2025-09-17T20:15:33.311729Z","iopub.status.idle":"2025-09-17T20:15:33.359739Z","shell.execute_reply.started":"2025-09-17T20:15:33.311698Z","shell.execute_reply":"2025-09-17T20:15:33.357652Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"#test5 = run_test_pipeline(\n#    test, gas_prices_t, electricity_prices_t, client_t, \n#    forecast_weather_t, weather_station\n#)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.361023Z","iopub.execute_input":"2025-09-17T20:15:33.361454Z","iopub.status.idle":"2025-09-17T20:15:33.391137Z","shell.execute_reply.started":"2025-09-17T20:15:33.361416Z","shell.execute_reply":"2025-09-17T20:15:33.389814Z"}},"outputs":[],"execution_count":60}]}