{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:12.797510Z","iopub.execute_input":"2025-09-17T20:06:12.797982Z","iopub.status.idle":"2025-09-17T20:06:14.175727Z","shell.execute_reply.started":"2025-09-17T20:06:12.797954Z","shell.execute_reply":"2025-09-17T20:06:14.174409Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/public_timeseries_testing_util.py\n/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/county_id_to_name_map.json\n/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/sample_submission.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CORE DATA MANIPULATION\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import pearsonr, spearmanr\nimport math\n\n# MACHINE LEARNING - SCIKIT-LEARN\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, roc_curve, auc\nfrom sklearn.pipeline import Pipeline\n\n# ADVANCED ML LIBRARIES\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# VISUALIZATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\n\n# CONFIGURATION FOR PLOTS\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n\n# DEEP LEARNING \ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n    print(\"TensorFlow disponible\")\nexcept ImportError:\n    print(\"TensorFlow no disponible en este entorno\")\n\n\n# UTILITIES\nimport os\nimport sys\nimport warnings\nimport itertools\nfrom datetime import datetime, timedelta\nimport time\nfrom collections import Counter\nimport pickle\nimport joblib\n\n# JUPYTER SPECIFIC\nfrom IPython.display import display, HTML\nfrom tqdm.notebook import tqdm\n\n# SUPPRESS WARNINGS\nwarnings.filterwarnings('ignore')\n\n# PANDAS CONFIGURATION\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n# NUMPY CONFIGURATION\nnp.random.seed(42)\n\n# PLOTLY CONFIGURATION\npyo.init_notebook_mode(connected=True)\n\nprint(\"‚úÖ Todas las librer√≠as importadas correctamente!\")\nprint(f\"üìä Pandas version: {pd.__version__}\")\nprint(f\"üî¢ NumPy version: {np.__version__}\")\nprint(f\"ü§ñ Scikit-learn version: {__import__('sklearn').__version__}\")\nprint(f\"üìà Matplotlib version: {__import__('matplotlib').__version__}\")\nprint(f\"üé® Seaborn version: {sns.__version__}\")\n\n# ============================================================================\n# FUNCIONES √öTILES ADICIONALES\n# ============================================================================\n\ndef quick_info(df):\n    \"\"\"Informaci√≥n r√°pida del dataset\"\"\"\n    print(f\"üìä Dataset Shape: {df.shape}\")\n    print(f\"üî¢ Columnas num√©ricas: {df.select_dtypes(include=[np.number]).columns.tolist()}\")\n    print(f\"üìù Columnas categ√≥ricas: {df.select_dtypes(include=['object']).columns.tolist()}\")\n    print(f\"‚ùå Valores nulos por columna:\")\n    print(df.isnull().sum()[df.isnull().sum() > 0])\n\ndef plot_missing_values(df):\n    \"\"\"Visualizar valores faltantes\"\"\"\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    if len(missing) > 0:\n        plt.figure(figsize=(10, 6))\n        missing.plot(kind='bar')\n        plt.title('Valores Faltantes por Columna')\n        plt.ylabel('Cantidad')\n        plt.xticks(rotation=45)\n        plt.show()\n    else:\n        print(\"‚úÖ No hay valores faltantes en el dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:14.177435Z","iopub.execute_input":"2025-09-17T20:06:14.177901Z","iopub.status.idle":"2025-09-17T20:06:34.680626Z","shell.execute_reply.started":"2025-09-17T20:06:14.177873Z","shell.execute_reply":"2025-09-17T20:06:34.679388Z"}},"outputs":[{"name":"stderr","text":"2025-09-17 20:06:24.781159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758139585.080430     227 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758139585.165584     227 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow disponible\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"name":"stdout","text":"‚úÖ Todas las librer√≠as importadas correctamente!\nüìä Pandas version: 2.2.3\nüî¢ NumPy version: 1.26.4\nü§ñ Scikit-learn version: 1.2.2\nüìà Matplotlib version: 3.7.2\nüé® Seaborn version: 0.12.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#competencia https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/overview\n\n# Files\n# train.csv\n\n# county - An ID code for the county.\n# is_business - Boolean for whether or not the prosumer is a business.\n# product_type - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n# target - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n# is_consumption - Boolean for whether or not this row's target is consumption or production.\n# datetime - The Estonian time in EET (UTC+2) / EEST (UTC+3). It describes the start of the 1-hour period on which target is given.\n# data_block_id - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n# row_id - A unique identifier for the row.\n# prediction_unit_id - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n# gas_prices.csv\n\n# origin_date - The date when the day-ahead prices became available.\n# forecast_date - The date when the forecast prices should be relevant.\n# [lowest/highest]_price_per_mwh - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n# data_block_id\n# client.csv\n\n# product_type\n# county - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n# eic_count - The aggregated number of consumption points (EICs - European Identifier Code).\n# installed_capacity - Installed photovoltaic solar panel capacity in kilowatts.\n# is_business - Boolean for whether or not the prosumer is a business.\n# date\n# data_block_id\n# electricity_prices.csv\n\n# origin_date\n# forecast_date - Represents the start of the 1-hour period when the price is valid\n# euros_per_mwh - The price of electricity on the day ahead markets in euros per megawatt hour.\n# data_block_id\n# forecast_weather.csv Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n\n# [latitude/longitude] - The coordinates of the weather forecast.\n# origin_datetime - The timestamp of when the forecast was generated.\n# hours_ahead - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n# temperature - The air temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# dewpoint - The dew point temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# cloudcover_[low/mid/high/total] - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total. Estimated for the end of the 1-hour period.\n# 10_metre_[u/v]_wind_component - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second. Estimated for the end of the 1-hour period.\n# data_block_id\n# forecast_datetime - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead. This represents the start of the 1-hour period for which weather data are forecasted.\n# direct_solar_radiation - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the hour, in watt-hours per square meter.\n# surface_solar_radiation_downwards - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, accumulated during the hour, in watt-hours per square meter.\n# snowfall - Snowfall over hour in units of meters of water equivalent.\n# total_precipitation - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the described hour, in units of meters.\n# historical_weather.csv Historic weather data.\n\n# datetime - This represents the start of the 1-hour period for which weather data are measured.\n# temperature - Measured at the end of the 1-hour period.\n# dewpoint - Measured at the end of the 1-hour period.\n# rain - Different from the forecast conventions. The rain from large scale weather systems of the hour in millimeters.\n# snowfall - Different from the forecast conventions. Snowfall over the hour in centimeters.\n# surface_pressure - The air pressure at surface in hectopascals.\n# cloudcover_[low/mid/high/total] - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n# windspeed_10m - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n# winddirection_10m - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n# shortwave_radiation - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n# direct_solar_radiation\n# diffuse_radiation - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n# [latitude/longitude] - The coordinates of the weather station.\n# data_block_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.681730Z","iopub.execute_input":"2025-09-17T20:06:34.682041Z","iopub.status.idle":"2025-09-17T20:06:34.689688Z","shell.execute_reply.started":"2025-09-17T20:06:34.682017Z","shell.execute_reply":"2025-09-17T20:06:34.688390Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#modelos a correr: NN por ser forecasting y algun xgb o lgb\n#son 4 dias a predecir el comportamiento de los prosumers\n#predecir con rolling de a 1 dia\n\n\n#analizar y preparar\n# como siempre primero ver los faltantes y los NaN y en cada caso decidir que hacer\n# atento con valores expresados para el dia actual o la hora actual / la hora anterior / el dia de ma√±ana , ver de no malinterpretar\n# ver la correlacion de precios sea gas y/o electricidad para ver como se comparta con business y no business\n# lo mismo con el clima para business y no business\n# ver si aplica el supuesto de que los business si operan bajo esos valores (costos de electricidad/gas) y los no business no es tan relevante\n# clima historical vs forecast , adaptar columnas que usan distintas proporciones , medir la certeza del forecast\n# analizar por fuera como funcionan los paneles y como miden lo que miden ( para mejor entendimiento del df)\n\n#columnas a agregar\n#Datetime dias lu-1/ma-2/mi-3/ju-4/vi-5/sa-6/do-7 \n#Date time horas y capaz minutos\n#horario laboral ejemplo 7 a 17hs (analizarlo) sea en el dataset y por fuera\n#horario business , para los que son business ver un pseudo horario de apertura y cierre como afecta y si se puede\n#Dia 6hs a 18hs / noche de 18hs a 6hs ver si lo adapto con 4 columnas por las estaciones\n#hora de dormir ejemplo 22hs a 6hs\n#dias festivos y vacaciones , columnas binarias en ambos\n#estaciones del a√±o 4 columnas binarias\n#periodogram para ver los lags y sea mensuales/quincenales/diarios/hora\n#ver si es relevante agregar alguna columna con la poblacion mensual y usar un ratio (info a buscar afuera)\n\n#sugerencias de features\n\n##Energ√©ticas:\n\n# Ratio production/consumption por prediction_unit_id (hist√≥rico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n##Temporales:\n\n# Lag de consumo/producci√≥n del mismo prediction_unit_id (24h, 48h, 168h)\n# Media m√≥vil de target por prediction_unit_id\n# Cambios d√≠a a d√≠a (delta vs d√≠a anterior)\n\n##Weather engineering:\n\n# √çndice de confort t√©rmico (combinando temp + humidity)\n# Potencial solar real (direct + diffuse radiation ajustado por cloudcover)\n# Diferencia forecast vs historical weather (para medir accuracy del forecast)\n\n##Segmentaci√≥n:\n\n# Clustering de prediction_units por comportamiento similar\n# Ratios county-level (agregaciones por county vs individual)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.692284Z","iopub.execute_input":"2025-09-17T20:06:34.692903Z","iopub.status.idle":"2025-09-17T20:06:34.721398Z","shell.execute_reply.started":"2025-09-17T20:06:34.692873Z","shell.execute_reply":"2025-09-17T20:06:34.720174Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# hacer scatterplot con \n# train3.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) el alpha para ver con mas claridad las zonas mas poblada\n# podria agregar colores para diferencia temperatura o exposicion a rayos solares o nubes y poder encarar distintos el dataset\n# sino funciona probar %matplotlib inline\n# train3.hist(bins=50, figsize=(20,15)) ver tmb los histograma de los datos cuando este todo junto# cambiar los bins a valores mas chicos\n# plt.show() agregar si hace falta\n# corr_matrix = train3.corr()\n# corr_matrix[\"target\"].sort_values(ascending=False) ver si aplica pero creo que no , recordad probar en columnas numericas si hay otra relacion\n# ejemplo crear columna con raiz/potencia/log y ver como apartan esos valores en correlacion\n# o usar scatter_matrix() de pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.722476Z","iopub.execute_input":"2025-09-17T20:06:34.722751Z","iopub.status.idle":"2025-09-17T20:06:34.750908Z","shell.execute_reply.started":"2025-09-17T20:06:34.722730Z","shell.execute_reply":"2025-09-17T20:06:34.749460Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train                 = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\nclient                = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\nelectricity_prices    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\nforecast_weather      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\ngas_prices            = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\nhistorical_weather    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\nweather_station       = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv')\n\ntest                  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv')\nclient_t              = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv')\nelectricity_prices_t  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv')\nforecast_weather_t    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv')\ngas_prices_t          = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv')\nrevealed_targets      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:06:34.752316Z","iopub.execute_input":"2025-09-17T20:06:34.753236Z","iopub.status.idle":"2025-09-17T20:07:04.830796Z","shell.execute_reply.started":"2025-09-17T20:06:34.753203Z","shell.execute_reply":"2025-09-17T20:07:04.829720Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# pruebo pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:07:04.831948Z","iopub.execute_input":"2025-09-17T20:07:04.832320Z","iopub.status.idle":"2025-09-17T20:07:04.837343Z","shell.execute_reply.started":"2025-09-17T20:07:04.832284Z","shell.execute_reply":"2025-09-17T20:07:04.836397Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import unicodedata\nfrom sklearn.neighbors import NearestNeighbors\nimport gc\nfrom collections import defaultdict\nfrom pathlib import Path\n\nclass DataPreprocessingPipeline:\n    \"\"\"Pipeline modular para preprocessing de datos de train y test\"\"\"\n    \n    def __init__(self):\n        self.county_id_to_name_map = {\n            0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"J√ÑRVAMAA\",\n            4: \"J√ïGEVAMAA\", 5: \"L√Ñ√ÑNE-VIRUMAA\", 6: \"L√Ñ√ÑNEMAA\", 7: \"P√ÑRNUMAA\",\n            8: \"P√ïLVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n            12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"V√ïRUMAA\"\n        }\n        self.county_name_to_id_map = {v: k for k, v in self.county_id_to_name_map.items()}\n        \n        # Diccionario de sin√≥nimos para normalizaci√≥n\n        self.synonyms_to_canonical = {\n            \"HARJU\": \"HARJUMAA\", \"HIIU\": \"HIIUMAA\", \"IDA-VIRU\": \"IDA-VIRUMAA\",\n            \"JARVA\": \"J√ÑRVAMAA\", \"J√ÑRVA\": \"J√ÑRVAMAA\", \"JOGEVA\": \"J√ïGEVAMAA\", \n            \"J√ïGEVA\": \"J√ïGEVAMAA\", \"LAANE-VIRU\": \"L√Ñ√ÑNE-VIRUMAA\", \n            \"L√Ñ√ÑNE-VIRU\": \"L√Ñ√ÑNE-VIRUMAA\", \"LAANE\": \"L√Ñ√ÑNEMAA\", \"L√Ñ√ÑNE\": \"L√Ñ√ÑNEMAA\",\n            \"PARNU\": \"P√ÑRNUMAA\", \"P√ÑRNU\": \"P√ÑRNUMAA\", \"POLVA\": \"P√ïLVAMAA\", \n            \"P√ïLVA\": \"P√ïLVAMAA\", \"RAPLA\": \"RAPLAMAA\", \"SAARE\": \"SAAREMAA\",\n            \"TARTU\": \"TARTUMAA\", \"VALGA\": \"VALGAMAA\", \"VILJANDI\": \"VILJANDIMAA\",\n            \"VORU\": \"V√ïRUMAA\", \"V√ïRU\": \"V√ïRUMAA\",\n            # versiones ya can√≥nicas\n            **{name: name for name in self.county_id_to_name_map.values()}\n        }\n    \n    def strip_accents(self, s: str) -> str:\n        \"\"\"Eliminar acentos de string\"\"\"\n        return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n    \n    def normalize_county_name(self, x):\n        \"\"\"Normalizar nombre de county\"\"\"\n        if pd.isna(x):\n            return np.nan\n        s = str(x).upper().strip()\n        # eliminar sufijos frecuentes\n        for suf in [\" COUNTY\", \" MAAKOND\"]:\n            if s.endswith(suf):\n                s = s[: -len(suf)]\n        s = s.replace(\"  \", \" \").replace(\"‚Äì\", \"-\")\n        s_noacc = self.strip_accents(s)\n        s_noacc = s_noacc.replace(\"  \", \" \")\n        \n        token = s_noacc.split()[0]\n        if \"-\" in s_noacc:\n            token = s_noacc\n        \n        # intentos de match\n        if s_noacc in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s_noacc]\n        if token in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[token]\n        if s in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s]\n        \n        return np.nan\n    \n    def process_weather_stations(self, weather_station):\n        \"\"\"Procesar estaciones meteorol√≥gicas para asignar counties\"\"\"\n        weather_station = weather_station.copy()\n        \n        # Normalizar nombres de county\n        weather_station['county_name_norm'] = weather_station['county_name'].apply(self.normalize_county_name)\n        \n        # Si tengo 'county' num√©rico pero falta nombre, lo inferimos del ID\n        mask = weather_station['county_name_norm'].isna() & weather_station['county'].notna()\n        weather_station.loc[mask, 'county_name_norm'] = (\n            weather_station.loc[mask, 'county'].astype(int).map(self.county_id_to_name_map)\n        )\n        \n        # Completar faltantes por k-NN\n        fcols = ['latitude', 'longitude']\n        known = weather_station.dropna(subset=['county_name_norm']).copy()\n        unknown = weather_station[weather_station['county_name_norm'].isna()].copy()\n        \n        if not unknown.empty and not known.empty:\n            known_rad = np.radians(known[fcols].values)\n            unknown_rad = np.radians(unknown[fcols].values)\n            \n            nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n            nbrs.fit(known_rad)\n            distances, idxs = nbrs.kneighbors(unknown_rad)\n            \n            nn_names = known['county_name_norm'].values\n            filled = []\n            for neigh_idx in idxs:\n                cands = nn_names[neigh_idx]\n                vals, counts = np.unique(cands, return_counts=True)\n                filled.append(vals[np.argmax(counts)])\n            \n            weather_station.loc[unknown.index, 'county_name_norm'] = filled\n        \n        # Sincronizar columnas finales\n        weather_station['county_name'] = weather_station['county_name_norm']\n        weather_station['county'] = weather_station['county_name'].map(self.county_name_to_id_map).astype('Int64')\n        \n        # Cualquier remanente a UNKNOWN (12)\n        weather_station.loc[weather_station['county_name'].isna(), 'county'] = 12\n        weather_station['county'] = weather_station['county'].astype('Int64')\n        \n        weather_station.drop(columns=['county_name_norm'], inplace=True)\n        \n        return weather_station\n    \n    def optimize_dtypes(self, df, is_train=True):\n        \"\"\"Optimizar tipos de datos para reducir memoria\"\"\"\n        df = df.copy()\n        \n        # Tipos comunes\n        if 'county' in df.columns:\n            df['county'] = df['county'].astype('uint8')\n        if 'is_business' in df.columns:\n            df['is_business'] = df['is_business'].astype('uint8')\n        if 'product_type' in df.columns:\n            df['product_type'] = df['product_type'].astype('uint8')\n        if 'is_consumption' in df.columns:\n            df['is_consumption'] = df['is_consumption'].astype('uint8')\n        if 'hour' in df.columns:\n            df['hour'] = df['hour'].astype('uint8')\n        \n        # IDs\n        if 'data_block_id' in df.columns:\n            df['data_block_id'] = df['data_block_id'].astype('uint16')\n        if 'row_id' in df.columns:\n            df['row_id'] = df['row_id'].astype('uint32')\n        if 'prediction_unit_id' in df.columns:\n            df['prediction_unit_id'] = df['prediction_unit_id'].astype('uint8')\n        \n        # Coordenadas y variables meteorol√≥gicas - float32\n        float_cols = ['latitude', 'longitude', 'target', 'lowest_price_per_mwh', \n                     'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', \n                     'installed_capacity']\n        \n        # Variables meteorol√≥gicas\n        weather_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                       'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                       'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                       'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                       'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                       'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                       'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                       'direct_solar_radiation', 'diffuse_radiation']\n        \n        # Aplicar float32 a columnas que existen\n        for col in float_cols + weather_cols:\n            if col in df.columns:\n                df[col] = df[col].astype('float32')\n        \n        # Weather forecast hour\n        if 'weather_forecast_hour' in df.columns:\n            df['weather_forecast_hour'] = df['weather_forecast_hour'].astype('uint8')\n        \n        return df\n\n\nclass TrainPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline espec√≠fico para datos de entrenamiento\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.temp_dir = Path(\"temp_chunks\")\n        self.temp_dir.mkdir(exist_ok=True)\n    \n    def part1_prepare_base_merges(self, train, gas_prices, electricity_prices, client):\n        \"\"\"Parte 1: Preparar datos base y hacer primeros merges\"\"\"\n        print(\"=== PARTE 1: Preparaci√≥n y merges base ===\")\n        \n        # Preparar copias\n        train1 = train.dropna().copy()\n        gas_prices1 = gas_prices.copy()\n        electricity_prices1 = electricity_prices.copy()\n        client1 = client.copy()\n        \n        # Procesar train\n        train1['datetime'] = pd.to_datetime(train1['datetime'])\n        train1['hour'] = train1['datetime'].dt.hour\n        train1['forecast_date'] = train1['datetime']\n        \n        # Merge 1: Gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        gas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n        \n        # Expandir gas a nivel horario\n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices1_hourly = pd.DataFrame(gas_hourly)\n        train2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')\n        \n        # Forward fill gas prices\n        train2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\n        train2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge 2: Electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        elec_columns = ['forecast_date', 'euros_per_mwh']\n        train3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')\n        train3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge 3: Client data\n        print(\"Procesando client data...\")\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        train4 = train3.merge(client1.drop('data_block_id', axis=1),\n                             left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                             right_on=['product_type', 'county', 'is_business', 'date'],\n                             how='left')\n        \n        train4 = train4.drop('date', axis=1)\n        train4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        print(\"Completando datos de clientes...\")\n        train4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        train4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        train4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        train4['eic_count'] = train4['eic_count'].fillna(train4.groupby('county')['eic_count'].transform('mean'))\n        train4['installed_capacity'] = train4['installed_capacity'].fillna(train4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Optimizar tipos\n        train4 = self.optimize_dtypes(train4, is_train=True)\n        \n        print(f\"Parte 1 completada. Dataset: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")\n        return train4\n    \n    def part2_prepare_weather_merge(self, train4, forecast_weather, historical_weather, weather_station):\n        \"\"\"Parte 2: Preparar datos meteorol√≥gicos y hacer merge\"\"\"\n        print(\"=== PARTE 2: Preparaci√≥n datos meteorol√≥gicos ===\")\n        \n        # Procesar weather stations\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        forecast_weather1 = forecast_weather.dropna().copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Merge con historical weather\n        print(\"Mergeando forecast con historical weather...\")\n        historical_weather1 = historical_weather.copy()\n        historical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n        \n        forecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n        \n        merged_weather = pd.merge(\n            forecast_weather2,\n            historical_weather1,\n            left_on=['latitude', 'longitude', 'forecast_datetime'],\n            right_on=['latitude', 'longitude', 'datetime'],\n            how='inner'\n        )\n        \n        # Limpiar y optimizar\n        merged_weather = merged_weather.drop(['f_data_block_id', 'forecast_datetime'], axis=1)\n        merged_weather = self.optimize_dtypes(merged_weather, is_train=True)\n        \n        print(f\"Weather data preparado: {merged_weather.shape[0]:,} filas, {merged_weather.shape[1]} columnas\")\n        return train4, merged_weather\n    \n    def part3_final_merge_and_cleanup(self, train4, merged_weather, chunk_size=1_000_000):\n        \"\"\"Parte 3: Merge final y limpieza\"\"\"\n        print(\"=== PARTE 3: Merge final y limpieza ===\")\n        \n        # Definir per√≠odos para procesar por chunks\n        periods = [\n            ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), \n            ('2021-12-01', '2022-01-01'), ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), \n            ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'), ('2022-05-01', '2022-06-01'), \n            ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n            ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), \n            ('2022-12-01', '2023-01-01'), ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), \n            ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'), ('2023-05-01', '2023-05-31')\n        ]\n        \n        # Procesar por chunks temporales\n        chunk_files = []\n        for i, (start_date, end_date) in enumerate(periods):\n            print(f\"Procesando per√≠odo {i+1}/{len(periods)}: {start_date} a {end_date}\")\n            \n            # Filtrar por per√≠odo\n            mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n            train4_chunk = train4[mask_train].copy()\n            \n            mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n            weather_chunk = merged_weather[mask_weather].copy()\n            \n            if len(train4_chunk) > 0 and len(weather_chunk) > 0:\n                # Merge chunk\n                chunk_result = pd.merge(train4_chunk, weather_chunk, \n                               on=['county', 'datetime'], \n                               how='left')\n                \n                # Guardar chunk temporal\n                chunk_file = self.temp_dir / f'train5_chunk_{i+1}.parquet'\n                chunk_result.to_parquet(chunk_file, index=False)\n                chunk_files.append(chunk_file)\n                \n                print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n                \n                # Liberar memoria\n                del train4_chunk, weather_chunk, chunk_result\n                gc.collect()\n        \n        # Concatenar chunks\n        print(\"Concatenando chunks...\")\n        final_chunks = []\n        for chunk_file in chunk_files:\n            chunk_data = pd.read_parquet(chunk_file)\n            final_chunks.append(chunk_data)\n        \n        train5 = pd.concat(final_chunks, ignore_index=True)\n        \n        # Limpieza final\n        print(\"Aplicando limpieza final...\")\n        train5 = self._cleanup_merged_data(train5, chunk_size)\n        \n        # Limpiar archivos temporales\n        for chunk_file in chunk_files:\n            chunk_file.unlink()\n        \n        return train5\n    \n    def _cleanup_merged_data(self, train5, chunk_size=1_000_000):\n        \"\"\"Limpiar datos despu√©s del merge\"\"\"\n        # Identificar columnas a eliminar y renombrar\n        sample = train5.head(1000)\n        cols_to_drop = [col for col in sample.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\n        \n        print(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n        \n        # Procesar por chunks\n        chunks_processed = []\n        total_rows = len(train5)\n        n_chunks = (total_rows // chunk_size) + 1\n        \n        for i in range(n_chunks):\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, total_rows)\n            \n            print(f\"Procesando chunk limpieza {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n            \n            chunk = train5.iloc[start_idx:end_idx].copy()\n            \n            # Filtrar county != 12\n            chunk = chunk[chunk['county'] != 12]\n            \n            if len(chunk) > 0:\n                # Eliminar columnas _y y renombrar _x\n                chunk = chunk.drop(columns=cols_to_drop)\n                chunk.rename(columns=rename_dict, inplace=True)\n                chunks_processed.append(chunk)\n            \n            del chunk\n            gc.collect()\n        \n        # Concatenar final\n        train5_clean = pd.concat(chunks_processed, ignore_index=True)\n        del chunks_processed\n        gc.collect()\n        \n        # Eliminar datos de clima faltantes\n        weather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n        initial_rows = len(train5_clean)\n        train5_clean = train5_clean.dropna(subset=weather_key_cols)\n        print(f\"Filas eliminadas por clima faltante: {initial_rows - len(train5_clean):,}\")\n        \n        # Reordenar columnas\n        train5_clean = self._reorder_columns(train5_clean)\n        \n        return train5_clean\n    \n    def _reorder_columns(self, df):\n        \"\"\"Reordenar columnas en el orden especificado\"\"\"\n        main_cols = [\n            'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = main_cols.copy()\n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n    \n    def run_full_pipeline(self, train, gas_prices, electricity_prices, client, \n                         forecast_weather, historical_weather, weather_station):\n        \"\"\"Ejecutar pipeline completo de entrenamiento\"\"\"\n        print(\"Iniciando pipeline completo de entrenamiento...\")\n        \n        # Parte 1: Merges base\n        train4 = self.part1_prepare_base_merges(train, gas_prices, electricity_prices, client)\n        \n        # Parte 2: Preparar datos meteorol√≥gicos\n        train4, merged_weather = self.part2_prepare_weather_merge(\n            train4, forecast_weather, historical_weather, weather_station\n        )\n        \n        # Parte 3: Merge final y limpieza\n        train5 = self.part3_final_merge_and_cleanup(train4, merged_weather)\n        \n        print(f\"Pipeline completado! Dataset final: {train5.shape[0]:,} filas, {train5.shape[1]} columnas\")\n        return train5\n\n\nclass TestPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline simplificado para datos de test\"\"\"\n    \n    def run_test_pipeline(self, test, gas_prices_t, electricity_prices_t, client_t, \n                         forecast_weather_t, weather_station):\n        \"\"\"Pipeline completo para test set (m√°s simple, sin historical weather)\"\"\"\n        print(\"=== PIPELINE TEST ===\")\n        \n        # Preparar datos base\n        test1 = test.copy()\n        test1['datetime'] = pd.to_datetime(test1['prediction_datetime'])\n        test1['hour'] = test1['datetime'].dt.hour\n        test1['forecast_date'] = test1['datetime']\n        \n        # Merge gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1 = gas_prices_t.copy()\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        \n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices_hourly = pd.DataFrame(gas_hourly)\n        test2 = pd.merge(test1, gas_prices_hourly, on='forecast_date', how='left')\n        test2['lowest_price_per_mwh'] = test2['lowest_price_per_mwh'].fillna(method='ffill')\n        test2['highest_price_per_mwh'] = test2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1 = electricity_prices_t.copy()\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        test3 = pd.merge(test2, electricity_prices1[['forecast_date', 'euros_per_mwh']], \n                        on='forecast_date', how='left')\n        test3['euros_per_mwh'] = test3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge client data\n        print(\"Procesando client data...\")\n        client1 = client_t.copy()\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        test4 = test3.merge(client1.drop('data_block_id', axis=1),\n                           left_on=['product_type', 'county', 'is_business', test3['datetime'].dt.date],\n                           right_on=['product_type', 'county', 'is_business', 'date'],\n                           how='left')\n        \n        test4 = test4.drop('date', axis=1)\n        test4 = test4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        test4 = test4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        test4['eic_count'] = test4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        test4['installed_capacity'] = test4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        test4['eic_count'] = test4['eic_count'].fillna(test4.groupby('county')['eic_count'].transform('mean'))\n        test4['installed_capacity'] = test4['installed_capacity'].fillna(test4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        forecast_weather1 = forecast_weather_t.copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Para test, no tenemos historical weather, as√≠ que creamos columnas dummy o usamos solo forecast\n        # Opci√≥n 1: Solo usar forecast weather (renombrar las f columns)\n        # Opci√≥n 2: Crear columnas hist√≥ricas como NaN y llenar despu√©s\n        \n        # Vamos con opci√≥n 1: usar forecast como hist√≥rico tambi√©n (aproximaci√≥n)\n        historical_cols_mapping = {\n            'ftemperature': 'temperature',\n            'fdewpoint': 'dewpoint', \n            'fcloudcover_high': 'cloudcover_high',\n            'fcloudcover_low': 'cloudcover_low',\n            'fcloudcover_mid': 'cloudcover_mid', \n            'fcloudcover_total': 'cloudcover_total',\n            'fdirect_solar_radiation': 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards': 'shortwave_radiation',\n            'fsnowfall': 'snowfall',\n            'ftotal_precipitation': 'rain'\n        }\n        \n        # Crear versiones hist√≥ricas basadas en forecast\n        for fcol, hcol in historical_cols_mapping.items():\n            if fcol in forecast_weather2.columns:\n                forecast_weather2[hcol] = forecast_weather2[fcol]\n        \n        # Agregar columnas que solo existen en historical\n        forecast_weather2['surface_pressure'] = 1013.25  # valor t√≠pico\n        forecast_weather2['windspeed_10m'] = np.sqrt(\n            forecast_weather2['f10_metre_u_wind_component']**2 + \n            forecast_weather2['f10_metre_v_wind_component']**2\n        ) if 'f10_metre_u_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['winddirection_10m'] = np.arctan2(\n            forecast_weather2['f10_metre_v_wind_component'], \n            forecast_weather2['f10_metre_u_wind_component']\n        ) * 180 / np.pi if 'f10_metre_v_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['diffuse_radiation'] = forecast_weather2.get('fsurface_solar_radiation_downwards', 0) - forecast_weather2.get('fdirect_solar_radiation', 0)\n        forecast_weather2['diffuse_radiation'] = forecast_weather2['diffuse_radiation'].clip(lower=0)\n        \n        # Limpiar y optimizar\n        forecast_weather2 = forecast_weather2.drop(columns=['data_block_id'], errors='ignore')\n        forecast_weather2 = self.optimize_dtypes(forecast_weather2, is_train=False)\n        \n        # Merge final con weather\n        print(\"Merge final con datos meteorol√≥gicos...\")\n        test5 = pd.merge(test4, forecast_weather2, \n                        left_on=['datetime', 'county'], \n                        right_on=['forecast_datetime', 'county'], \n                        how='left')\n\n\n\n        # Limpiar columnas duplicadas y renombrar\n        cols_to_drop = [col for col in test5.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in test5.columns if col.endswith('_x')}\n        \n        test5 = test5.drop(columns=cols_to_drop + ['forecast_datetime'], errors='ignore')\n        test5.rename(columns=rename_dict, inplace=True)\n        \n        # Optimizar tipos finales\n        test5 = self.optimize_dtypes(test5, is_train=False)\n        \n        # Reordenar columnas igual que train\n        test5 = self._reorder_columns_test(test5)\n        \n        print(f\"Pipeline test completado! Dataset: {test5.shape[0]:,} filas, {test5.shape[1]} columnas\")\n        return test5\n    \n    def _reorder_columns_test(self, df):\n        \"\"\"Reordenar columnas para test (mismo orden que train)\"\"\"\n        main_cols = [\n            'row_id', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = []\n        # Solo agregar columnas que existen\n        for col in main_cols:\n            if col in df.columns:\n                final_order.append(col)\n        \n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n\n\n# Funciones de utilidad para usar el pipeline\ndef run_train_pipeline(train, gas_prices, electricity_prices, client, \n                      forecast_weather, historical_weather, weather_station):\n    \"\"\"\n    Funci√≥n principal para ejecutar pipeline de entrenamiento\n    \n    Returns:\n        pd.DataFrame: Dataset procesado train5\n    \"\"\"\n    pipeline = TrainPipeline()\n    return pipeline.run_full_pipeline(\n        train, gas_prices, electricity_prices, client, \n        forecast_weather, historical_weather, weather_station\n    )\n\ndef run_test_pipeline(test, gas_prices_t, electricity_prices_t, client_t, \n                     forecast_weather_t, weather_station):\n    \"\"\"\n    Funci√≥n principal para ejecutar pipeline de test\n    \n    Returns:\n        pd.DataFrame: Dataset procesado test5\n    \"\"\"\n    pipeline = TestPipeline()\n    return pipeline.run_test_pipeline(\n        test, gas_prices_t, electricity_prices_t, client_t, \n        forecast_weather_t, weather_station\n    )\n\n# Ejemplo de uso:\n\"\"\"\n# Para train:\ntrain5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)\n\n# Para test:\ntest5 = run_test_pipeline(\n    test, gas_prices_t, electricity_prices_t, client_t, \n    forecast_weather_t, weather_station\n)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:07:04.838724Z","iopub.execute_input":"2025-09-17T20:07:04.839033Z","iopub.status.idle":"2025-09-17T20:07:04.946212Z","shell.execute_reply.started":"2025-09-17T20:07:04.839009Z","shell.execute_reply":"2025-09-17T20:07:04.944928Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n# Para train:\\ntrain5 = run_train_pipeline(\\n    train, gas_prices, electricity_prices, client, \\n    forecast_weather, historical_weather, weather_station\\n)\\n\\n# Para test:\\ntest5 = run_test_pipeline(\\n    test, gas_prices_t, electricity_prices_t, client_t, \\n    forecast_weather_t, weather_station\\n)\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:07:04.947519Z","iopub.execute_input":"2025-09-17T20:07:04.947956Z","iopub.status.idle":"2025-09-17T20:09:25.437993Z","shell.execute_reply.started":"2025-09-17T20:07:04.947928Z","shell.execute_reply":"2025-09-17T20:09:25.436662Z"}},"outputs":[{"name":"stdout","text":"Iniciando pipeline completo de entrenamiento...\n=== PARTE 1: Preparaci√≥n y merges base ===\nProcesando gas prices...\nProcesando electricity prices...\nProcesando client data...\nCompletando datos de clientes...\nParte 1 completada. Dataset: 2,017,824 filas, 16 columnas\n=== PARTE 2: Preparaci√≥n datos meteorol√≥gicos ===\nProcesando forecast weather...\nCompletando 2140318 counties faltantes con k-NN...\nMergeando forecast con historical weather...\nWeather data preparado: 3,418,242 filas, 32 columnas\n=== PARTE 3: Merge final y limpieza ===\nProcesando per√≠odo 1/21: 2021-09-01 a 2021-10-01\nChunk 1 guardado: (1423080, 46)\nProcesando per√≠odo 2/21: 2021-10-01 a 2021-11-01\nChunk 2 guardado: (1532066, 46)\nProcesando per√≠odo 3/21: 2021-11-01 a 2021-12-01\nChunk 3 guardado: (1484640, 46)\nProcesando per√≠odo 4/21: 2021-12-01 a 2022-01-01\nChunk 4 guardado: (1543056, 46)\nProcesando per√≠odo 5/21: 2022-01-01 a 2022-02-01\nChunk 5 guardado: (1590672, 46)\nProcesando per√≠odo 6/21: 2022-02-01 a 2022-03-01\nChunk 6 guardado: (1441632, 46)\nProcesando per√≠odo 7/21: 2022-03-01 a 2022-04-01\nChunk 7 guardado: (1597678, 46)\nProcesando per√≠odo 8/21: 2022-04-01 a 2022-05-01\nChunk 8 guardado: (1630080, 46)\nProcesando per√≠odo 9/21: 2022-05-01 a 2022-06-01\nChunk 9 guardado: (1644240, 46)\nProcesando per√≠odo 10/21: 2022-06-01 a 2022-07-01\nChunk 10 guardado: (1573920, 46)\nProcesando per√≠odo 11/21: 2022-07-01 a 2022-08-01\nChunk 11 guardado: (1581744, 46)\nProcesando per√≠odo 12/21: 2022-08-01 a 2022-09-01\nChunk 12 guardado: (1581716, 46)\nProcesando per√≠odo 13/21: 2022-09-01 a 2022-10-01\nChunk 13 guardado: (1573920, 46)\nProcesando per√≠odo 14/21: 2022-10-01 a 2022-11-01\nChunk 14 guardado: (1686610, 46)\nProcesando per√≠odo 15/21: 2022-11-01 a 2022-12-01\nChunk 15 guardado: (1643084, 46)\nProcesando per√≠odo 16/21: 2022-12-01 a 2023-01-01\nChunk 16 guardado: (1669008, 46)\nProcesando per√≠odo 17/21: 2023-01-01 a 2023-02-01\nChunk 17 guardado: (1654608, 46)\nProcesando per√≠odo 18/21: 2023-02-01 a 2023-03-01\nChunk 18 guardado: (1494048, 46)\nProcesando per√≠odo 19/21: 2023-03-01 a 2023-04-01\nChunk 19 guardado: (1604926, 46)\nProcesando per√≠odo 20/21: 2023-04-01 a 2023-05-01\nChunk 20 guardado: (1514016, 46)\nProcesando per√≠odo 21/21: 2023-05-01 a 2023-05-31\nChunk 21 guardado: (1530872, 46)\nConcatenando chunks...\nAplicando limpieza final...\nEliminando 1 columnas, renombrando 1\nProcesando chunk limpieza 1/33: filas 0 a 1,000,000\nProcesando chunk limpieza 2/33: filas 1,000,000 a 2,000,000\nProcesando chunk limpieza 3/33: filas 2,000,000 a 3,000,000\nProcesando chunk limpieza 4/33: filas 3,000,000 a 4,000,000\nProcesando chunk limpieza 5/33: filas 4,000,000 a 5,000,000\nProcesando chunk limpieza 6/33: filas 5,000,000 a 6,000,000\nProcesando chunk limpieza 7/33: filas 6,000,000 a 7,000,000\nProcesando chunk limpieza 8/33: filas 7,000,000 a 8,000,000\nProcesando chunk limpieza 9/33: filas 8,000,000 a 9,000,000\nProcesando chunk limpieza 10/33: filas 9,000,000 a 10,000,000\nProcesando chunk limpieza 11/33: filas 10,000,000 a 11,000,000\nProcesando chunk limpieza 12/33: filas 11,000,000 a 12,000,000\nProcesando chunk limpieza 13/33: filas 12,000,000 a 13,000,000\nProcesando chunk limpieza 14/33: filas 13,000,000 a 14,000,000\nProcesando chunk limpieza 15/33: filas 14,000,000 a 15,000,000\nProcesando chunk limpieza 16/33: filas 15,000,000 a 16,000,000\nProcesando chunk limpieza 17/33: filas 16,000,000 a 17,000,000\nProcesando chunk limpieza 18/33: filas 17,000,000 a 18,000,000\nProcesando chunk limpieza 19/33: filas 18,000,000 a 19,000,000\nProcesando chunk limpieza 20/33: filas 19,000,000 a 20,000,000\nProcesando chunk limpieza 21/33: filas 20,000,000 a 21,000,000\nProcesando chunk limpieza 22/33: filas 21,000,000 a 22,000,000\nProcesando chunk limpieza 23/33: filas 22,000,000 a 23,000,000\nProcesando chunk limpieza 24/33: filas 23,000,000 a 24,000,000\nProcesando chunk limpieza 25/33: filas 24,000,000 a 25,000,000\nProcesando chunk limpieza 26/33: filas 25,000,000 a 26,000,000\nProcesando chunk limpieza 27/33: filas 26,000,000 a 27,000,000\nProcesando chunk limpieza 28/33: filas 27,000,000 a 28,000,000\nProcesando chunk limpieza 29/33: filas 28,000,000 a 29,000,000\nProcesando chunk limpieza 30/33: filas 29,000,000 a 30,000,000\nProcesando chunk limpieza 31/33: filas 30,000,000 a 31,000,000\nProcesando chunk limpieza 32/33: filas 31,000,000 a 32,000,000\nProcesando chunk limpieza 33/33: filas 32,000,000 a 32,995,616\nFilas eliminadas por clima faltante: 2,024\nPipeline completado! Dataset final: 32,963,024 filas, 45 columnas\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train5.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:25.442594Z","iopub.execute_input":"2025-09-17T20:09:25.443145Z","iopub.status.idle":"2025-09-17T20:09:25.508602Z","shell.execute_reply.started":"2025-09-17T20:09:25.443107Z","shell.execute_reply":"2025-09-17T20:09:25.507097Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"           row_id      target            datetime       forecast_date  hour  \\\n32965017  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965018  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965019  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965020  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965021  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n\n          data_block_id  prediction_unit_id  is_business  product_type  \\\n32965017            636                  60            1             3   \n32965018            636                  60            1             3   \n32965019            636                  60            1             3   \n32965020            636                  60            1             3   \n32965021            636                  60            1             3   \n\n          county   latitude  longitude  lowest_price_per_mwh  \\\n32965017      15  57.599998  28.200001                  29.0   \n32965018      15  57.900002  26.700001                  29.0   \n32965019      15  57.900002  27.200001                  29.0   \n32965020      15  57.900002  27.700001                  29.0   \n32965021      15  57.900002  28.200001                  29.0   \n\n          highest_price_per_mwh  euros_per_mwh  eic_count  installed_capacity  \\\n32965017                   34.0      82.370003       55.0         2188.199951   \n32965018                   34.0      82.370003       55.0         2188.199951   \n32965019                   34.0      82.370003       55.0         2188.199951   \n32965020                   34.0      82.370003       55.0         2188.199951   \n32965021                   34.0      82.370003       55.0         2188.199951   \n\n          ftemperature  temperature  fdewpoint  dewpoint  fcloudcover_high  \\\n32965017     12.136377         13.8   2.330225       3.9               0.0   \n32965018     14.534815         13.9   2.355859       3.3               0.0   \n32965019     13.503565         13.9   3.533350       2.9               0.0   \n32965020     13.150782         13.6   2.600976       3.2               0.0   \n32965021     12.648584         13.6   1.409814       3.6               0.0   \n\n          cloudcover_high  fcloudcover_low  cloudcover_low  fcloudcover_mid  \\\n32965017              0.0         0.000000             7.0         0.966461   \n32965018              0.0         0.270050            15.0         0.544281   \n32965019              0.0         0.024902            18.0         0.181091   \n32965020              0.0         0.000153            18.0         0.866364   \n32965021              0.0         0.000000             9.0         0.997070   \n\n          cloudcover_mid  fcloudcover_total  cloudcover_total  \\\n32965017            71.0           0.966461              49.0   \n32965018            73.0           0.652069              57.0   \n32965019            66.0           0.184845              56.0   \n32965020            66.0           0.866455              56.0   \n32965021            78.0           0.997070              55.0   \n\n          f10_metre_u_wind_component  windspeed_10m  \\\n32965017                   -1.009770       2.055556   \n32965018                    2.205562       2.916667   \n32965019                    0.538569       3.194444   \n32965020                   -0.223149       3.527778   \n32965021                   -0.198003       3.194444   \n\n          f10_metre_v_wind_component  winddirection_10m  \\\n32965017                   -0.927298              346.0   \n32965018                   -1.008352              297.0   \n32965019                   -1.362844              309.0   \n32965020                   -1.600393              317.0   \n32965021                   -2.190237              319.0   \n\n          fdirect_solar_radiation  direct_solar_radiation  \\\n32965017                 1.858164                   155.0   \n32965018               832.791504                   249.0   \n32965019               486.835938                   257.0   \n32965020                26.035942                   240.0   \n32965021                63.440388                   186.0   \n\n          fsurface_solar_radiation_downwards  shortwave_radiation  \\\n32965017                          191.511948                410.0   \n32965018                          685.467529                490.0   \n32965019                          500.649719                498.0   \n32965020                          268.223053                482.0   \n32965021                          304.685272                446.0   \n\n          diffuse_radiation  fsnowfall  snowfall  ftotal_precipitation  rain  \\\n32965017              255.0        0.0       0.0              0.000000   0.1   \n32965018              241.0        0.0       0.0              0.000056   0.1   \n32965019              241.0        0.0       0.0              0.000000   0.0   \n32965020              242.0        0.0       0.0              0.000000   0.0   \n32965021              260.0        0.0       0.0              0.000000   0.0   \n\n          surface_pressure  weather_forecast_hour  is_consumption  \n32965017       1012.799988                    8.0               1  \n32965018       1005.200012                    8.0               1  \n32965019       1007.200012                    8.0               1  \n32965020       1011.099976                    8.0               1  \n32965021       1013.000000                    8.0               1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>32965017</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.599998</td>\n      <td>28.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>12.136377</td>\n      <td>13.8</td>\n      <td>2.330225</td>\n      <td>3.9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>7.0</td>\n      <td>0.966461</td>\n      <td>71.0</td>\n      <td>0.966461</td>\n      <td>49.0</td>\n      <td>-1.009770</td>\n      <td>2.055556</td>\n      <td>-0.927298</td>\n      <td>346.0</td>\n      <td>1.858164</td>\n      <td>155.0</td>\n      <td>191.511948</td>\n      <td>410.0</td>\n      <td>255.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>1012.799988</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965018</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>26.700001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>14.534815</td>\n      <td>13.9</td>\n      <td>2.355859</td>\n      <td>3.3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.270050</td>\n      <td>15.0</td>\n      <td>0.544281</td>\n      <td>73.0</td>\n      <td>0.652069</td>\n      <td>57.0</td>\n      <td>2.205562</td>\n      <td>2.916667</td>\n      <td>-1.008352</td>\n      <td>297.0</td>\n      <td>832.791504</td>\n      <td>249.0</td>\n      <td>685.467529</td>\n      <td>490.0</td>\n      <td>241.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000056</td>\n      <td>0.1</td>\n      <td>1005.200012</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965019</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>27.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>13.503565</td>\n      <td>13.9</td>\n      <td>3.533350</td>\n      <td>2.9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.024902</td>\n      <td>18.0</td>\n      <td>0.181091</td>\n      <td>66.0</td>\n      <td>0.184845</td>\n      <td>56.0</td>\n      <td>0.538569</td>\n      <td>3.194444</td>\n      <td>-1.362844</td>\n      <td>309.0</td>\n      <td>486.835938</td>\n      <td>257.0</td>\n      <td>500.649719</td>\n      <td>498.0</td>\n      <td>241.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1007.200012</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965020</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>27.700001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>13.150782</td>\n      <td>13.6</td>\n      <td>2.600976</td>\n      <td>3.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000153</td>\n      <td>18.0</td>\n      <td>0.866364</td>\n      <td>66.0</td>\n      <td>0.866455</td>\n      <td>56.0</td>\n      <td>-0.223149</td>\n      <td>3.527778</td>\n      <td>-1.600393</td>\n      <td>317.0</td>\n      <td>26.035942</td>\n      <td>240.0</td>\n      <td>268.223053</td>\n      <td>482.0</td>\n      <td>242.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1011.099976</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965021</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>28.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>12.648584</td>\n      <td>13.6</td>\n      <td>1.409814</td>\n      <td>3.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.997070</td>\n      <td>78.0</td>\n      <td>0.997070</td>\n      <td>55.0</td>\n      <td>-0.198003</td>\n      <td>3.194444</td>\n      <td>-2.190237</td>\n      <td>319.0</td>\n      <td>63.440388</td>\n      <td>186.0</td>\n      <td>304.685272</td>\n      <td>446.0</td>\n      <td>260.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1013.000000</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Features temporales\ntrain5['day_of_week'] = train5['datetime'].dt.dayofweek\ntrain5['month'] = train5['datetime'].dt.month\n# Crear is_daylight (1 si hay radiaci√≥n solar, 0 si es noche)\ntrain5['is_daylight'] = (train5['direct_solar_radiation'] > 0).astype(int)\n# Crear weekend (1 para s√°bado/domingo, 0 para lunes-viernes)\ntrain5['weekend'] = (train5['day_of_week'] >= 5).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:25.509590Z","iopub.execute_input":"2025-09-17T20:09:25.509862Z","iopub.status.idle":"2025-09-17T20:09:27.957092Z","shell.execute_reply.started":"2025-09-17T20:09:25.509843Z","shell.execute_reply":"2025-09-17T20:09:27.954851Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Mae puesto n1 kaggle 52.3090\n#1er primer modelo xgb sobre train5 haciendo split 80/20 de train y test \n#Mae = 107.9896\n#sin lag sin feature, sin nada , solo haciendo el merge\n\n\n#agrego day of week y month para todos los siguientes\n\n#2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# estas variables de lags las quito\n\n#agrego is_daylight binaria\n\n#4to modelo lgb 14.2% y sobre esto split 80/20 de train y test  \n#de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses 14.2% aprox\n# se que no deberia ser comparable al usar de base distinto , pero parte de aca ahora\n#Mae = 86.6747\n\n#agrego weekend binaria\n\n#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n\n#para todos los demas modelos ya tienen incluido\n#day of week\n#month\n#is_daylight\n#weekend\n\n#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo la saco por ahora a is_active_hours\n\n#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que  la saco a is_sleeping_hours\n\n#empiezo a probar con variables\n##Energ√©ticas:\n\n# Ratio production/consumption por prediction_unit_id (hist√≥rico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n\n#9no modelo lgb mantengo las 3 anteriores y prueba una nueva Ratio production/cn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:27.958969Z","iopub.execute_input":"2025-09-17T20:09:27.959545Z","iopub.status.idle":"2025-09-17T20:09:27.969183Z","shell.execute_reply.started":"2025-09-17T20:09:27.959505Z","shell.execute_reply":"2025-09-17T20:09:27.966863Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n# # Sample 10% manteniendo orden temporal\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# sample_df = train5.sample(frac=0.1, random_state=42).sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"Sample 10%: {len(sample_df):,}\")\n\n# # Convertir datetime\n# sample_df['datetime'] = pd.to_datetime(sample_df['datetime'])\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# # LightGBM (m√°s eficiente en memoria)\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE: {mae:.4f}\")\n# print(f\"Features usadas: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# Dataset original: 32,963,024\n# Sample 10%: 3,296,302\n# MAE: 89.3026\n# Features usadas: 43\n# Train: 2,637,041 | Test: 659,261","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:27.971612Z","iopub.execute_input":"2025-09-17T20:09:27.973347Z","iopub.status.idle":"2025-09-17T20:09:28.023368Z","shell.execute_reply.started":"2025-09-17T20:09:27.973281Z","shell.execute_reply":"2025-09-17T20:09:28.021084Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # Feature importance\n# importance = model.feature_importances_\n# feature_names = X_train.columns\n\n# # Crear DataFrame y ordenar\n# importance_df = pd.DataFrame({\n#     'feature': feature_names,\n#     'importance': importance\n# }).sort_values('importance', ascending=False)\n\n# # Ver top 15\n# print(\"Top 15 features m√°s importantes:\")\n# print(importance_df.head(15))\n\n# # Plot opcional (si quieres visualizar)\n# plt.figure(figsize=(10, 8))\n# plt.barh(importance_df.head(15)['feature'], importance_df.head(15)['importance'])\n# plt.title('Top 15 Feature Importance')\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.026021Z","iopub.execute_input":"2025-09-17T20:09:28.026528Z","iopub.status.idle":"2025-09-17T20:09:28.073240Z","shell.execute_reply.started":"2025-09-17T20:09:28.026476Z","shell.execute_reply":"2025-09-17T20:09:28.071474Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# importance_df['importance_pct'] = (importance_df['importance'] / importance_df['importance'].sum()) * 100\n# print(importance_df.head(15)[['feature', 'importance', 'importance_pct']])\n\n#                               feature  importance  importance_pct\n# 40                      is_consumption         469       15.633333\n# 12                  installed_capacity         436       14.533333\n# 11                           eic_count         292        9.733333\n# 2                   prediction_unit_id         228        7.600000\n# 31  fsurface_solar_radiation_downwards         227        7.566667\n# 0                                 hour         180        6.000000\n# 1                        data_block_id         164        5.466667\n# 41                         day_of_week         161        5.366667\n# 32                 shortwave_radiation         111        3.700000\n# 3                          is_business         105        3.500000\n# 10                       euros_per_mwh          83        2.766667\n# 14                         temperature          73        2.433333\n# 42                               month          72        2.400000\n# 30              direct_solar_radiation          60        2.000000\n# 29             fdirect_solar_radiation          48        1.600000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.074818Z","iopub.execute_input":"2025-09-17T20:09:28.075247Z","iopub.status.idle":"2025-09-17T20:09:28.122844Z","shell.execute_reply.started":"2025-09-17T20:09:28.075206Z","shell.execute_reply":"2025-09-17T20:09:28.120859Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n# Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# Sample 10%\n\n\n\n\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# sample_df = train5.sample(frac=0.1, random_state=42).sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Sample 10%: {len(sample_df):,}\")\n\n# # Convertir datetime\n# sample_df['datetime'] = pd.to_datetime(sample_df['datetime'])\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Variables clim√°ticas importantes para lags\n# climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                 'temperature', 'direct_solar_radiation']\n\n# # Crear lags por prediction_unit_id\n# print(\"üîÑ Creando lags clim√°ticos...\")\n# lag_dfs = []\n\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id].copy()\n    \n#     # Lags de 1h, 4h, 12h para cada variable clim√°tica\n#     for var in climate_vars:\n#         if var in unit_data.columns:\n#             unit_data[f'{var}_lag1h'] = unit_data[var].shift(1)\n#             unit_data[f'{var}_lag4h'] = unit_data[var].shift(4)\n#             unit_data[f'{var}_lag12h'] = unit_data[var].shift(12)\n    \n#     lag_dfs.append(unit_data)\n\n# sample_with_lags = pd.concat(lag_dfs, ignore_index=True)\n# sample_with_lags = sample_with_lags.sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Datos con lags: {len(sample_with_lags):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_with_lags.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal\n# split_idx = int(len(sample_with_lags) * 0.8)\n# train_data = sample_with_lags.iloc[:split_idx]\n# test_data = sample_with_lags.iloc[split_idx:]\n\n# # Dropear NaN (por los lags)\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train final: {len(X_train):,}\")\n# print(f\"Test final: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con lags: {mae:.4f}\")\n# print(f\"Mejora vs 89.3: {89.3 - mae:.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.124109Z","iopub.execute_input":"2025-09-17T20:09:28.124912Z","iopub.status.idle":"2025-09-17T20:09:28.168652Z","shell.execute_reply.started":"2025-09-17T20:09:28.124874Z","shell.execute_reply":"2025-09-17T20:09:28.167055Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.171211Z","iopub.execute_input":"2025-09-17T20:09:28.171666Z","iopub.status.idle":"2025-09-17T20:09:28.219560Z","shell.execute_reply.started":"2025-09-17T20:09:28.171634Z","shell.execute_reply":"2025-09-17T20:09:28.218147Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#4to modelo lgb de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses por memoria \n#Mae = 86.6747\n\n\n# Preparar datos y tomar √∫ltimos 3 meses\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# train5['datetime'] = pd.to_datetime(train5['datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# # Features temporales\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Crear is_daylight (1 si hay radiaci√≥n solar, 0 si es noche)\n# sample_df['is_daylight'] = (sample_df['direct_solar_radiation'] > 0).astype(int)\n\n# print(f\"Daylight distribution:\")\n# print(sample_df['is_daylight'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_daylight (3 meses): {mae:.4f}\")\n# print(f\"Baseline original era: 89.3\")\n\n# # #Dataset original: 32,963,024\n# # √öltimos 3 meses: 4,674,024\n# # Porcentaje: 14.2%\n# # Daylight distribution:\n# # is_daylight\n# # 1    2592660\n# # 0    2081364\n# # Name: count, dtype: int64\n# # Features totales: 44\n# # Train: 3,739,219 | Test: 934,805\n# # MAE con is_daylight (3 meses): 86.6747\n# # Baseline original era: 89.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.220786Z","iopub.execute_input":"2025-09-17T20:09:28.221072Z","iopub.status.idle":"2025-09-17T20:09:28.261512Z","shell.execute_reply.started":"2025-09-17T20:09:28.221052Z","shell.execute_reply":"2025-09-17T20:09:28.260070Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n# Preparar datos y tomar √∫ltimos 3 meses\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# train5['datetime'] = pd.to_datetime(train5['datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# # Features temporales\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Crear is_daylight (1 si hay radiaci√≥n solar, 0 si es noche)\n# sample_df['is_daylight'] = (sample_df['direct_solar_radiation'] > 0).astype(int)\n\n# # Crear weekend (1 para s√°bado/domingo, 0 para lunes-viernes)\n# sample_df['weekend'] = (sample_df['day_of_week'] >= 5).astype(int)\n\n# print(f\"Daylight distribution:\")\n# print(sample_df['is_daylight'].value_counts())\n# print(f\"Weekend distribution:\")\n# print(sample_df['weekend'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_daylight + weekend (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior era: 86.67\")\n\n# # Dataset original: 32,963,024\n# # √öltimos 3 meses: 4,674,024\n# # Porcentaje: 14.2%\n# # Daylight distribution:\n# # is_daylight\n# # 1    2592660\n# # 0    2081364\n# # Name: count, dtype: int64\n# # Weekend distribution:\n# # weekend\n# # 0    3341592\n# # 1    1332432\n# # Name: count, dtype: int64\n# # Features totales: 45\n# # Train: 3,739,219 | Test: 934,805\n# # MAE con is_daylight + weekend (3 meses): 86.6747\n# # MAE anterior era: 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.262862Z","iopub.execute_input":"2025-09-17T20:09:28.263267Z","iopub.status.idle":"2025-09-17T20:09:28.317573Z","shell.execute_reply.started":"2025-09-17T20:09:28.263240Z","shell.execute_reply":"2025-09-17T20:09:28.315469Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo\n\n# # Usar train5 con las features ya agregadas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Agregar is_active_hours (horario laboral/escolar lunes-viernes 8-17h)\n# train5['is_active_hours'] = (\n#     (train5['day_of_week'] < 5) &  # Lunes-viernes\n#     (train5['hour'] >= 8) & \n#     (train5['hour'] <= 17)\n# ).astype(int)\n\n# # √öltimos 3 meses para evitar problemas de memoria\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_active_hours (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior (is_daylight): 86.67\")\n\n# MAE con is_active_hours (3 meses): 90.4776\n# MAE anterior (is_daylight): 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.319424Z","iopub.execute_input":"2025-09-17T20:09:28.320930Z","iopub.status.idle":"2025-09-17T20:09:28.390262Z","shell.execute_reply.started":"2025-09-17T20:09:28.320877Z","shell.execute_reply":"2025-09-17T20:09:28.388232Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que no la saco\n\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Agregar is_sleeping_hours (22h-6h)\n# train5['is_sleeping_hours'] = (\n#     (train5['hour'] >= 22) | (train5['hour'] <= 6)\n# ).astype(int)\n\n# # √öltimos 3 meses para evitar problemas de memoria\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# print(f\"Sleeping hours distribution:\")\n# print(sample_df['is_sleeping_hours'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_active_hours (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior (is_daylight): 86.67\")\n\n# Dataset original: 32,963,024\n# √öltimos 3 meses: 4,674,024\n# Porcentaje: 14.2%\n# Sleeping hours distribution:\n# is_sleeping_hours\n# 0    2923632\n# 1    1750392\n# Name: count, dtype: int64\n# Features totales: 46\n# Train: 3,739,219 | Test: 934,805\n# MAE con is_active_hours (3 meses): 98.3152\n# MAE anterior (is_daylight): 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.392682Z","iopub.execute_input":"2025-09-17T20:09:28.393402Z","iopub.status.idle":"2025-09-17T20:09:28.434815Z","shell.execute_reply.started":"2025-09-17T20:09:28.393335Z","shell.execute_reply":"2025-09-17T20:09:28.432717Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n# Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# # Features energ√©ticas \n# print(\"üîÑ Calculando features energ√©ticas...\")\n\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     # Usar installed_capacity como proxy de capacidad productiva\n#     capacity = unit_data['installed_capacity'].iloc[0]\n    \n#     # Ratio de tipos: cu√°ntas observaciones son production vs consumption\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     production_ratio = production_obs / total_obs if total_obs > 0 else 0\n#     consumption_ratio = consumption_obs / total_obs if total_obs > 0 else 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_ratio,\n#         'consumption_obs_ratio': consumption_ratio\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n\n# # Merge con datos principales\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Nuevas features: capacity_per_obs, production_obs_ratio, consumption_obs_ratio\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con features energ√©ticas: {mae:.4f}\")\n# print(f\"MAE anterior: 86.67\")\n\n# # BORRAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data, energy_df, energy_stats\n# import gc\n# gc.collect()\n\n# √öltimos 3 meses: 4,674,024\n# Nuevas features: capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n# Features totales: 48\n# Train: 3,739,219 | Test: 934,805\n# MAE con features energ√©ticas: 76.2703\n# MAE anterior: 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.437123Z","iopub.execute_input":"2025-09-17T20:09:28.437508Z","iopub.status.idle":"2025-09-17T20:09:28.484372Z","shell.execute_reply.started":"2025-09-17T20:09:28.437478Z","shell.execute_reply":"2025-09-17T20:09:28.482853Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#v9 no optimizado para poder usarlo #\n\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 4 MESES para calcular ratio, luego reducir a 3\n# max_date = train5['datetime'].max()\n# cutoff_4m = max_date - pd.DateOffset(months=4)\n# cutoff_3m = max_date - pd.DateOffset(months=3)\n\n# sample_4m = train5[train5['datetime'] >= cutoff_4m].copy()\n# print(\"üîÑ Calculando ratio production/consumption hist√≥rico (30d)...\")\n\n# # M√âTODO SIMPLE Y DIRECTO - sin reindexing problem√°tico\n# sample_4m['prod_cons_ratio_30d'] = 0.0\n\n# for unit_id in sample_4m['prediction_unit_id'].unique():\n#     unit_data = sample_4m[sample_4m['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     # Para cada fila, calcular ratio con ventana de 30 d√≠as hacia atr√°s (SIMPLE)\n#     for i in range(len(unit_data)):\n#         current_time = unit_data.iloc[i]['datetime']\n#         start_time = current_time - pd.Timedelta(days=30)\n        \n#         # Datos en ventana de 30 d√≠as\n#         window_data = unit_data[\n#             (unit_data['datetime'] >= start_time) & \n#             (unit_data['datetime'] < current_time)\n#         ]\n        \n#         if len(window_data) > 0:\n#             # Promedios simples\n#             prod_mean = window_data[window_data['is_consumption'] == 0]['target'].mean()\n#             cons_mean = window_data[window_data['is_consumption'] == 1]['target'].mean()\n            \n#             # Manejar NaN y divisi√≥n por cero\n#             if pd.isna(prod_mean): prod_mean = 0\n#             if pd.isna(cons_mean): cons_mean = 1\n            \n#             ratio = prod_mean / cons_mean if cons_mean != 0 else 0\n#             sample_4m.iloc[i, sample_4m.columns.get_loc('prod_cons_ratio_30d')] = ratio\n\n# # Reducir a 3 meses finales\n# sample_df = sample_4m[sample_4m['datetime'] >= cutoff_3m].copy()\n# print(f\"Datos finales (3 meses): {len(sample_df):,}\")\n\n# # Features energ√©ticas\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# # Split y entrenamiento\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# split_idx = int(len(sample_df) * 0.8)\n# X_train = sample_df.iloc[:split_idx][feature_cols].dropna()\n# y_train = sample_df.iloc[:split_idx].loc[X_train.index, 'target']\n# X_test = sample_df.iloc[split_idx:][feature_cols].dropna()\n# y_test = sample_df.iloc[split_idx:].loc[X_test.index, 'target']\n\n# print(f\"Features: {len(feature_cols)} | Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                          random_state=42, n_jobs=-1, verbose=-1)\n# model.fit(X_train, y_train)\n\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con ratio 30d: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 76.27\")\n\n# print(f\"üìà Ratio stats: min={sample_df['prod_cons_ratio_30d'].min():.3f}, max={sample_df['prod_cons_ratio_30d'].max():.3f}, NaNs={sample_df['prod_cons_ratio_30d'].isna().sum()}\")\n\n# # Limpiar memoria\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_4m, energy_df, energy_stats\n# import gc\n# gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.485860Z","iopub.execute_input":"2025-09-17T20:09:28.486281Z","iopub.status.idle":"2025-09-17T20:09:28.542158Z","shell.execute_reply.started":"2025-09-17T20:09:28.486235Z","shell.execute_reply":"2025-09-17T20:09:28.538700Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# #10mo modelo lgb mantengo las 3 anteriores y pruebo  Capacidad instalada per capita (installed_capacity / eic_count)\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita...\")\n\n# # Features energ√©ticas existentes + NUEVA: capacity per capita\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0  # NUEVA FEATURE\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con capacity per capita: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 76.27\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats, feature_importance\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.545367Z","iopub.execute_input":"2025-09-17T20:09:28.546495Z","iopub.status.idle":"2025-09-17T20:09:28.592083Z","shell.execute_reply.started":"2025-09-17T20:09:28.546455Z","shell.execute_reply":"2025-09-17T20:09:28.589172Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.593419Z","iopub.execute_input":"2025-09-17T20:09:28.593765Z","iopub.status.idle":"2025-09-17T20:09:28.702747Z","shell.execute_reply.started":"2025-09-17T20:09:28.593736Z","shell.execute_reply":"2025-09-17T20:09:28.701023Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n6      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n7      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n8      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n9      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n10     366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n\n    data_block_id  prediction_unit_id  is_business  product_type  county  \\\n6               0                   0            0             1       0   \n7               0                   0            0             1       0   \n8               0                   0            0             1       0   \n9               0                   0            0             1       0   \n10              0                   0            0             1       0   \n\n     latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n6   59.099998  24.200001                 45.23                  46.32   \n7   59.099998  25.200001                 45.23                  46.32   \n8   59.400002  22.700001                 45.23                  46.32   \n9   59.400002  23.200001                 45.23                  46.32   \n10  59.400002  23.700001                 45.23                  46.32   \n\n    euros_per_mwh  eic_count  installed_capacity  ftemperature  temperature  \\\n6       86.879997      108.0          952.890015     12.681543         12.4   \n7       86.879997      108.0          952.890015     12.868921         12.3   \n8       86.879997      108.0          952.890015     15.041773         15.2   \n9       86.879997      108.0          952.890015     14.632105         14.9   \n10      86.879997      108.0          952.890015     14.480005         12.8   \n\n    fdewpoint  dewpoint  fcloudcover_high  cloudcover_high  fcloudcover_low  \\\n6    9.783228       9.7          0.023590             11.0         0.002380   \n7    9.498316       9.6          0.431854              6.0         0.211182   \n8   11.860376      11.8          0.134674             18.0         0.202515   \n9   11.773584      11.5          0.255188             19.0         0.036774   \n10  11.581568      10.4          0.338074             12.0         0.037109   \n\n    cloudcover_low  fcloudcover_mid  cloudcover_mid  fcloudcover_total  \\\n6             10.0         0.001251             0.0           0.026398   \n7             23.0         0.006790             1.0           0.548508   \n8              7.0         0.003906             0.0           0.308930   \n9              6.0         0.026245             0.0           0.286194   \n10             4.0         0.016510             1.0           0.368134   \n\n    cloudcover_total  f10_metre_u_wind_component  windspeed_10m  \\\n6               12.0                    1.840991       4.222222   \n7               23.0                    1.505298       4.027778   \n8               12.0                    3.185351       9.055555   \n9               11.0                    3.474780       8.361111   \n10               8.0                    3.211841       5.416667   \n\n    f10_metre_v_wind_component  winddirection_10m  fdirect_solar_radiation  \\\n6                    -3.857846              338.0                      0.0   \n7                    -3.590024              337.0                      0.0   \n8                    -8.173276              338.0                      0.0   \n9                    -8.008969              335.0                      0.0   \n10                   -7.426206              337.0                      0.0   \n\n    direct_solar_radiation  fsurface_solar_radiation_downwards  \\\n6                      0.0                                 0.0   \n7                      0.0                                 0.0   \n8                      0.0                                 0.0   \n9                      0.0                                 0.0   \n10                     0.0                                 0.0   \n\n    shortwave_radiation  diffuse_radiation  fsnowfall  snowfall  \\\n6                   0.0                0.0        0.0       0.0   \n7                   0.0                0.0        0.0       0.0   \n8                   0.0                0.0        0.0       0.0   \n9                   0.0                0.0        0.0       0.0   \n10                  0.0                0.0        0.0       0.0   \n\n    ftotal_precipitation  rain  surface_pressure  weather_forecast_hour  \\\n6                    0.0   0.0       1009.200012                    1.0   \n7                    0.0   0.0       1004.200012                    1.0   \n8                    0.0   0.0       1015.000000                    1.0   \n9                    0.0   0.0       1014.500000                    1.0   \n10                   0.0   0.0       1014.000000                    1.0   \n\n    is_consumption  day_of_week  month  is_daylight  weekend  \n6                0            2      9            0        0  \n7                0            2      9            0        0  \n8                0            2      9            0        0  \n9                0            2      9            0        0  \n10               0            2      9            0        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.681543</td>\n      <td>12.4</td>\n      <td>9.783228</td>\n      <td>9.7</td>\n      <td>0.023590</td>\n      <td>11.0</td>\n      <td>0.002380</td>\n      <td>10.0</td>\n      <td>0.001251</td>\n      <td>0.0</td>\n      <td>0.026398</td>\n      <td>12.0</td>\n      <td>1.840991</td>\n      <td>4.222222</td>\n      <td>-3.857846</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1009.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.868921</td>\n      <td>12.3</td>\n      <td>9.498316</td>\n      <td>9.6</td>\n      <td>0.431854</td>\n      <td>6.0</td>\n      <td>0.211182</td>\n      <td>23.0</td>\n      <td>0.006790</td>\n      <td>1.0</td>\n      <td>0.548508</td>\n      <td>23.0</td>\n      <td>1.505298</td>\n      <td>4.027778</td>\n      <td>-3.590024</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1004.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>15.041773</td>\n      <td>15.2</td>\n      <td>11.860376</td>\n      <td>11.8</td>\n      <td>0.134674</td>\n      <td>18.0</td>\n      <td>0.202515</td>\n      <td>7.0</td>\n      <td>0.003906</td>\n      <td>0.0</td>\n      <td>0.308930</td>\n      <td>12.0</td>\n      <td>3.185351</td>\n      <td>9.055555</td>\n      <td>-8.173276</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1015.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.632105</td>\n      <td>14.9</td>\n      <td>11.773584</td>\n      <td>11.5</td>\n      <td>0.255188</td>\n      <td>19.0</td>\n      <td>0.036774</td>\n      <td>6.0</td>\n      <td>0.026245</td>\n      <td>0.0</td>\n      <td>0.286194</td>\n      <td>11.0</td>\n      <td>3.474780</td>\n      <td>8.361111</td>\n      <td>-8.008969</td>\n      <td>335.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.500000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.480005</td>\n      <td>12.8</td>\n      <td>11.581568</td>\n      <td>10.4</td>\n      <td>0.338074</td>\n      <td>12.0</td>\n      <td>0.037109</td>\n      <td>4.0</td>\n      <td>0.016510</td>\n      <td>1.0</td>\n      <td>0.368134</td>\n      <td>8.0</td>\n      <td>3.211841</td>\n      <td>5.416667</td>\n      <td>-7.426206</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# train5[\"fsurface_solar_radiation_downwards\"].describe()\n\n# count    3.296302e+07\n# mean     1.103279e+02\n# std      1.714488e+02\n# min     -3.258333e-01\n# 25%      0.000000e+00\n# 50%      5.706424e-01\n# 75%      1.429561e+02\n# max      8.487144e+02\n# Name: fsurface_solar_radiation_downwards, dtype: float64\n\ntrain5[\"fsurface_solar_radiation_downwards\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:28.703941Z","iopub.execute_input":"2025-09-17T20:09:28.704309Z","iopub.status.idle":"2025-09-17T20:09:29.817604Z","shell.execute_reply.started":"2025-09-17T20:09:28.704282Z","shell.execute_reply":"2025-09-17T20:09:29.816440Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"fsurface_solar_radiation_downwards\n0.000000      15297128\n0.284444         39564\n0.142222         35932\n0.568889         28742\n0.071111         21062\n                ...   \n169.200272           2\n320.213043           2\n48.876667            2\n42.334446            2\n268.698120           2\nName: count, Length: 1291576, dtype: int64"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# #11vo modelo lgb 4 feature electrica y pruebo panel_efficiency\n# # ‚úÖ MAE con panel efficiency: 85.0309\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita + panel efficiency...\")\n\n# # Features energ√©ticas existentes + NUEVAS: capacity per capita + panel efficiency\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     # Calcular eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n#     production_data = unit_data[unit_data['is_consumption'] == 0]\n#     if len(production_data) > 0 and 'fsurface_solar_radiation_downwards' in production_data.columns:\n#         solar_rad = production_data['fsurface_solar_radiation_downwards']\n#         production = production_data['target']\n#         # Filtrar valores v√°lidos (solar > 0 para evitar divisi√≥n por 0)\n#         valid_mask = (solar_rad > 0) & (production >= 0)\n#         if valid_mask.sum() > 0:\n#             panel_efficiency = (production[valid_mask] / solar_rad[valid_mask]).mean()\n#         else:\n#             panel_efficiency = 0\n#     else:\n#         panel_efficiency = 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0,\n#         'panel_efficiency': panel_efficiency  # NUEVA FEATURE\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con panel efficiency: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"üìà Panel efficiency stats:\")\n# print(f\"   - Min: {sample_df['panel_efficiency'].min():.4f}\")\n# print(f\"   - Max: {sample_df['panel_efficiency'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['panel_efficiency'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['panel_efficiency'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.818786Z","iopub.execute_input":"2025-09-17T20:09:29.819152Z","iopub.status.idle":"2025-09-17T20:09:29.826123Z","shell.execute_reply.started":"2025-09-17T20:09:29.819126Z","shell.execute_reply":"2025-09-17T20:09:29.825039Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# #12vo modelo lgb , lo anterior pero ahora efficiency panel es log\n# # ‚úÖ MAE con panel efficiency LOG: 85.3200\n# # üìä MAE anterior: 72.3572\n\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita + panel efficiency LOG...\")\n\n# # Features energ√©ticas existentes + NUEVAS: capacity per capita + panel efficiency LOG\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     # Calcular eficiencia de paneles LOG (production vs solar_radiation cuando is_consumption=False)\n#     production_data = unit_data[unit_data['is_consumption'] == 0]\n#     if len(production_data) > 0 and 'fsurface_solar_radiation_downwards' in production_data.columns:\n#         solar_rad = production_data['fsurface_solar_radiation_downwards']\n#         production = production_data['target']\n#         # Filtrar valores v√°lidos (solar > 0 para evitar divisi√≥n por 0)\n#         valid_mask = (solar_rad > 0) & (production >= 0)\n#         if valid_mask.sum() > 0:\n#             efficiency_ratio = (production[valid_mask] / solar_rad[valid_mask]).mean()\n#             # Aplicar log pero manejando valores <= 1\n#             if efficiency_ratio > 1:\n#                 panel_efficiency_log = np.log(efficiency_ratio)\n#             elif efficiency_ratio > 0:\n#                 panel_efficiency_log = -np.log(1/efficiency_ratio)  # log inverso para valores < 1\n#             else:\n#                 panel_efficiency_log = 0\n#         else:\n#             panel_efficiency_log = 0\n#     else:\n#         panel_efficiency_log = 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0,\n#         'panel_efficiency_log': panel_efficiency_log  # NUEVA FEATURE CON LOG\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con panel efficiency LOG: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"üìà Capacity per capita stats:\")\n# print(f\"   - Min: {sample_df['capacity_per_capita'].min():.4f}\")\n# print(f\"   - Max: {sample_df['capacity_per_capita'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['capacity_per_capita'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['capacity_per_capita'].isna().sum()}\")\n\n# print(f\"üìà Panel efficiency LOG stats:\")\n# print(f\"   - Min: {sample_df['panel_efficiency_log'].min():.4f}\")\n# print(f\"   - Max: {sample_df['panel_efficiency_log'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['panel_efficiency_log'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['panel_efficiency_log'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.831600Z","iopub.execute_input":"2025-09-17T20:09:29.832025Z","iopub.status.idle":"2025-09-17T20:09:29.852545Z","shell.execute_reply.started":"2025-09-17T20:09:29.831997Z","shell.execute_reply":"2025-09-17T20:09:29.851420Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# #13vo modelo probando volatilidad de precio de electricidad 7 dias\n# # ‚úÖ MAE con electricity price volatility: 73.2246\n# # üìä MAE anterior: 72.3572\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 120 d√≠as para calcular volatilidad, luego reducir a 90\n# max_date = train5['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular volatilidad\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular volatilidad\n# sample_120d = train5[train5['datetime'] >= cutoff_120d].copy()\n\n# print(\"üîÑ Calculando volatilidad de precios (7d rolling) con 120 d√≠as...\")\n\n# # Calcular volatilidad de electricity prices (rolling std 7 d√≠as)\n# sample_120d['electricity_price_volatility_7d'] = 0.0\n\n# for unit_id in sample_120d['prediction_unit_id'].unique():\n#     unit_data = sample_120d[sample_120d['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     if 'euros_per_mwh' in unit_data.columns and len(unit_data) > 0:\n#         # Rolling std de 7 d√≠as para electricity prices\n#         unit_data = unit_data.set_index('datetime')\n#         price_vol = unit_data['euros_per_mwh'].rolling('7D', min_periods=1).std()\n        \n#         # Manejar NaN y resetear index\n#         price_vol = price_vol.fillna(0)\n#         unit_data = unit_data.reset_index()\n        \n#         # Asignar volatilidad al dataframe principal\n#         mask = sample_120d['prediction_unit_id'] == unit_id\n#         sample_120d.loc[mask, 'electricity_price_volatility_7d'] = price_vol.values\n\n# # AHORA reducir a √∫ltimos 90 d√≠as (con volatilidad ya calculada)\n# sample_df = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n# print(f\"Datos finales (90 d√≠as con volatilidad): {len(sample_df):,}\")\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita...\")\n\n# # Features energ√©ticas existentes + capacity per capita (SIN panel_efficiency)\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas + volatilidad: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con electricity price volatility: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"üìà Electricity price volatility 7d stats:\")\n# print(f\"   - Min: {sample_df['electricity_price_volatility_7d'].min():.4f}\")\n# print(f\"   - Max: {sample_df['electricity_price_volatility_7d'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['electricity_price_volatility_7d'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['electricity_price_volatility_7d'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_120d, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.853579Z","iopub.execute_input":"2025-09-17T20:09:29.854080Z","iopub.status.idle":"2025-09-17T20:09:29.885563Z","shell.execute_reply.started":"2025-09-17T20:09:29.854002Z","shell.execute_reply":"2025-09-17T20:09:29.884031Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# #14vo modelo ademas de electricity_volatility pruebo con gas\n# # ‚úÖ MAE con electricity + gas price volatility: 70.6517\n# # üìä MAE anterior: 73.2246\n\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 120 d√≠as para calcular volatilidad, luego reducir a 90\n# max_date = train5['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular volatilidad\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular volatilidad\n# sample_120d = train5[train5['datetime'] >= cutoff_120d].copy()\n\n# print(\"üîÑ Calculando volatilidad de electricity y gas prices (7d rolling) con 120 d√≠as...\")\n\n# # Calcular volatilidad de electricity y gas prices (rolling std 7 d√≠as)\n# sample_120d['electricity_price_volatility_7d'] = 0.0\n# sample_120d['gas_price_volatility_7d'] = 0.0\n\n# for unit_id in sample_120d['prediction_unit_id'].unique():\n#     unit_data = sample_120d[sample_120d['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     if len(unit_data) > 0:\n#         unit_data_indexed = unit_data.set_index('datetime')\n        \n#         # Volatilidad electricity prices\n#         if 'euros_per_mwh' in unit_data.columns:\n#             elec_vol = unit_data_indexed['euros_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n#         else:\n#             elec_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n#         # Volatilidad gas prices\n#         if 'lowest_price_per_mwh' in unit_data.columns:\n#             gas_vol = unit_data_indexed['lowest_price_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n#         else:\n#             gas_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n#         # Asignar volatilidades al dataframe principal usando iloc\n#         mask = sample_120d['prediction_unit_id'] == unit_id\n#         indices = sample_120d[mask].index\n        \n#         sample_120d.loc[indices, 'electricity_price_volatility_7d'] = elec_vol.values\n#         sample_120d.loc[indices, 'gas_price_volatility_7d'] = gas_vol.values\n\n# # AHORA reducir a √∫ltimos 90 d√≠as (conservando TODAS las columnas)\n# sample_df = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n# print(f\"Datos finales (90 d√≠as con volatilidades): {len(sample_df):,}\")\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita...\")\n\n# # Features energ√©ticas existentes + capacity per capita\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas + volatilidades: {len(sample_df):,}\")\n\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con electricity + gas price volatility: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 73.2246\")\n\n# # Verificar nuevas features\n\n# if 'gas_price_volatility_7d' in sample_df.columns:\n#     print(f\"üìà Gas price volatility 7d stats:\")\n#     print(f\"   - Min: {sample_df['gas_price_volatility_7d'].min():.4f}\")\n#     print(f\"   - Max: {sample_df['gas_price_volatility_7d'].max():.4f}\")\n#     print(f\"   - Mean: {sample_df['gas_price_volatility_7d'].mean():.4f}\")\n#     print(f\"   - NaN count: {sample_df['gas_price_volatility_7d'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_120d, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.887381Z","iopub.execute_input":"2025-09-17T20:09:29.887769Z","iopub.status.idle":"2025-09-17T20:09:29.918729Z","shell.execute_reply.started":"2025-09-17T20:09:29.887740Z","shell.execute_reply":"2025-09-17T20:09:29.917484Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# CREAR TRAIN6 con 150 d√≠as y todas las features consolidadas\nprint(\"üîÑ Creando train6 con 150 d√≠as...\")\n\n# Usar train5 como base\ntrain5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# √öltimos 150 d√≠as para train6\nmax_date = train5['datetime'].max()\ncutoff_150d = max_date - pd.DateOffset(days=150)\ntrain6 = train5[train5['datetime'] >= cutoff_150d].copy()\n\nprint(f\"üìä train6 creado: {len(train6):,} filas ({len(train6['prediction_unit_id'].unique())} units)\")\n\nprint(\"üîÑ Calculando features energ√©ticas...\")\n\n# Features energ√©ticas\nenergy_stats = []\nfor unit_id in train6['prediction_unit_id'].unique():\n    unit_data = train6[train6['prediction_unit_id'] == unit_id]\n    \n    capacity = unit_data['installed_capacity'].iloc[0]\n    eic_count = unit_data['eic_count'].iloc[0]\n    total_obs = len(unit_data)\n    production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n    consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n    energy_stats.append({\n        'prediction_unit_id': unit_id,\n        'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n        'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n        'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n        'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n    })\n\nenergy_df = pd.DataFrame(energy_stats)\ntrain6 = train6.merge(energy_df, on='prediction_unit_id', how='left')\n\nprint(\"üîÑ Calculando volatilidades de precios (7d rolling)...\")\n\n# Volatilidades de precios (7d rolling)\ntrain6['electricity_price_volatility_7d'] = 0.0\ntrain6['gas_price_volatility_7d'] = 0.0\n\nfor unit_id in train6['prediction_unit_id'].unique():\n    unit_data = train6[train6['prediction_unit_id'] == unit_id].copy()\n    unit_data = unit_data.sort_values('datetime')\n    \n    if len(unit_data) > 0:\n        unit_data_indexed = unit_data.set_index('datetime')\n        \n        # Volatilidad electricity prices\n        if 'euros_per_mwh' in unit_data.columns:\n            elec_vol = unit_data_indexed['euros_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n        else:\n            elec_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n        # Volatilidad gas prices\n        if 'lowest_price_per_mwh' in unit_data.columns:\n            gas_vol = unit_data_indexed['lowest_price_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n        else:\n            gas_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n        # Asignar volatilidades\n        mask = train6['prediction_unit_id'] == unit_id\n        indices = train6[mask].index\n        \n        train6.loc[indices, 'electricity_price_volatility_7d'] = elec_vol.values\n        train6.loc[indices, 'gas_price_volatility_7d'] = gas_vol.values\n\nprint(\"‚úÖ train6 creado con todas las features:\")\nprint(f\"   üìä Datos: {len(train6):,} filas\")\nprint(f\"   üìä Per√≠odo: {train6['datetime'].min()} a {train6['datetime'].max()}\")\nprint(f\"   üîß Features energ√©ticas: 4 (capacity_per_obs, production_obs_ratio, consumption_obs_ratio, capacity_per_capita)\")\nprint(f\"   üìà Volatilidades: 2 (electricity_price_volatility_7d, gas_price_volatility_7d)\")\n\n# Verificar features creadas\nfeature_check = [\n    'capacity_per_obs', 'production_obs_ratio', 'consumption_obs_ratio', \n    'capacity_per_capita', 'electricity_price_volatility_7d', 'gas_price_volatility_7d'\n]\n\nprint(\"\\nüîç Verificaci√≥n de features:\")\nfor feature in feature_check:\n    if feature in train6.columns:\n        print(f\"   ‚úÖ {feature}: OK\")\n    else:\n        print(f\"   ‚ùå {feature}: FALTA\")\n\n# Estad√≠sticas b√°sicas de las nuevas features\nprint(f\"\\nüìà Estad√≠sticas r√°pidas:\")\nfor feature in feature_check:\n    if feature in train6.columns:\n        print(f\"   {feature}: min={train6[feature].min():.3f}, max={train6[feature].max():.3f}, mean={train6[feature].mean():.3f}\")\n\nprint(f\"\\n‚ú® train6 listo para usar con {len(train6.columns)} columnas totales\")\n\n# Limpiar memoria temporal\ndel energy_df, energy_stats\nimport gc\ngc.collect()\nprint(\"üßπ Memoria temporal liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:29.919970Z","iopub.execute_input":"2025-09-17T20:09:29.920381Z","iopub.status.idle":"2025-09-17T20:09:51.859530Z","shell.execute_reply.started":"2025-09-17T20:09:29.920323Z","shell.execute_reply":"2025-09-17T20:09:51.858313Z"}},"outputs":[{"name":"stdout","text":"üîÑ Creando train6 con 150 d√≠as...\nüìä train6 creado: 7,821,024 filas (68 units)\nüîÑ Calculando features energ√©ticas...\nüîÑ Calculando volatilidades de precios (7d rolling)...\n‚úÖ train6 creado con todas las features:\n   üìä Datos: 7,821,024 filas\n   üìä Per√≠odo: 2022-12-31 10:00:00 a 2023-05-30 10:00:00\n   üîß Features energ√©ticas: 4 (capacity_per_obs, production_obs_ratio, consumption_obs_ratio, capacity_per_capita)\n   üìà Volatilidades: 2 (electricity_price_volatility_7d, gas_price_volatility_7d)\n\nüîç Verificaci√≥n de features:\n   ‚úÖ capacity_per_obs: OK\n   ‚úÖ production_obs_ratio: OK\n   ‚úÖ consumption_obs_ratio: OK\n   ‚úÖ capacity_per_capita: OK\n   ‚úÖ electricity_price_volatility_7d: OK\n   ‚úÖ gas_price_volatility_7d: OK\n\nüìà Estad√≠sticas r√°pidas:\n   capacity_per_obs: min=0.000, max=0.091, mean=0.014\n   production_obs_ratio: min=0.500, max=0.500, mean=0.500\n   consumption_obs_ratio: min=0.500, max=0.500, mean=0.500\n   capacity_per_capita: min=0.438, max=213.750, mean=26.458\n   electricity_price_volatility_7d: min=0.000, max=66.608, mean=39.864\n   gas_price_volatility_7d: min=0.000, max=12.594, mean=2.619\n\n‚ú® train6 listo para usar con 55 columnas totales\nüßπ Memoria temporal liberada\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Modelo #16: Lag 24h de target por prediction_unit_id\n# #  √∫ltimos 90 d√≠as \n\n# print(\"üîÑ Modelo #16: Lag 24h con √∫ltimos 90 d√≠as...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h por prediction_unit_id\n# sample_90d['target_lag_24h'] = sample_90d.groupby('prediction_unit_id')['target'].shift(24)\n\n# # Stats del lag\n# valid_lags = sample_90d['target_lag_24h'].notna().sum()\n# print(f\"üìà Lag 24h v√°lidos: {valid_lags:,} ({valid_lags/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en lag\n# train_clean = sample_90d.dropna(subset=['target_lag_24h'])\n# print(f\"üìä Datos finales: {len(train_clean):,}\")\n\n# # Features (incluye nueva lag)\n# feature_cols = [col for col in train_clean.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(train_clean) * 0.8)\n# train_data = train_clean.iloc[:split_idx]\n# test_data = train_clean.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con lag 24h (90 d√≠as): {mae:.4f}\")\n# print(f\"üìä MAE anterior: 70.6517\")\n# print(f\"üìà Cambio: {mae - 70.6517:+.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.860898Z","iopub.execute_input":"2025-09-17T20:09:51.861329Z","iopub.status.idle":"2025-09-17T20:09:51.868378Z","shell.execute_reply.started":"2025-09-17T20:09:51.861297Z","shell.execute_reply":"2025-09-17T20:09:51.867321Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# # 17vo modelo Lag 48h de target por prediction_unit_id , no va\n# #‚úÖ MAE con lag 48h (90 d√≠as): 104.3003\n# #üìä MAE anterior: 70.6517\n# #  √∫ltimos 90 d√≠as \n\n# print(\"üîÑ Modelo #17: Lag 48h con √∫ltimos 90 d√≠as...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h por prediction_unit_id\n# sample_90d['target_lag_48h'] = sample_90d.groupby('prediction_unit_id')['target'].shift(24)\n\n# # Stats del lag\n# valid_lags = sample_90d['target_lag_48h'].notna().sum()\n# print(f\"üìà Lag 24h v√°lidos: {valid_lags:,} ({valid_lags/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en lag\n# train_clean = sample_90d.dropna(subset=['target_lag_48h'])\n# print(f\"üìä Datos finales: {len(train_clean):,}\")\n\n# # Features (incluye nueva lag)\n# feature_cols = [col for col in train_clean.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(train_clean) * 0.8)\n# train_data = train_clean.iloc[:split_idx]\n# test_data = train_clean.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con lag 48h (90 d√≠as): {mae:.4f}\")\n# print(f\"üìä MAE anterior: 70.6517\")\n# print(f\"üìà Cambio: {mae - 70.6517:+.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.869615Z","iopub.execute_input":"2025-09-17T20:09:51.869948Z","iopub.status.idle":"2025-09-17T20:09:51.898746Z","shell.execute_reply.started":"2025-09-17T20:09:51.869924Z","shell.execute_reply":"2025-09-17T20:09:51.897763Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# # Modelo #18: Cyclical encoding de hora (hour_sin, hour_cos)\n# ‚úÖ MAE con hour_sin/cos: 122.4835 # algo esta pasado ver como corregirlo\n# üìä MAE anterior: 70.6517\n\n# # Usando √∫ltimos 90 d√≠as\n\n# print(\"üîÑ Modelo #18: Agregando hour_sin y hour_cos...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Crear cyclical encoding de hora\n# sample_90d['hour_sin'] = np.sin(2 * np.pi * sample_90d['hour'] / 24)\n# sample_90d['hour_cos'] = np.cos(2 * np.pi * sample_90d['hour'] / 24)\n\n# print(f\"üìà Hour_sin range: [{sample_90d['hour_sin'].min():.3f}, {sample_90d['hour_sin'].max():.3f}]\")\n# print(f\"üìà Hour_cos range: [{sample_90d['hour_cos'].min():.3f}, {sample_90d['hour_cos'].max():.3f}]\")\n\n# # Features (incluye nuevas cyclical)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con hour_sin/cos: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 70.6517\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.899887Z","iopub.execute_input":"2025-09-17T20:09:51.900252Z","iopub.status.idle":"2025-09-17T20:09:51.926287Z","shell.execute_reply.started":"2025-09-17T20:09:51.900218Z","shell.execute_reply":"2025-09-17T20:09:51.925096Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# # Modelo #19: Cyclical encoding de semana (week_sin, week_cos)\n# #‚úÖ MAE con week_sin/cos: 118.3710 los temporales , de la forma que los planteo no estan funcionando\n# #üìä MAE anterior: 70.6517\n# # Usando √∫ltimos 90 d√≠as\n\n# print(\"üîÑ Modelo #19: Agregando week_sin y week_cos...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Crear cyclical encoding de semana del a√±o\n# sample_90d['week_of_year'] = sample_90d['datetime'].dt.isocalendar().week\n# sample_90d['week_sin'] = np.sin(2 * np.pi * sample_90d['week_of_year'] / 52)\n# sample_90d['week_cos'] = np.cos(2 * np.pi * sample_90d['week_of_year'] / 52)\n\n# print(f\"üìà Week range: [{sample_90d['week_of_year'].min()}, {sample_90d['week_of_year'].max()}]\")\n# print(f\"üìà Week_sin range: [{sample_90d['week_sin'].min():.3f}, {sample_90d['week_sin'].max():.3f}]\")\n# print(f\"üìà Week_cos range: [{sample_90d['week_cos'].min():.3f}, {sample_90d['week_cos'].max():.3f}]\")\n\n# # Features (incluye nuevas cyclical de semana)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con week_sin/cos: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 70.6517\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.927450Z","iopub.execute_input":"2025-09-17T20:09:51.927893Z","iopub.status.idle":"2025-09-17T20:09:51.960720Z","shell.execute_reply.started":"2025-09-17T20:09:51.927858Z","shell.execute_reply":"2025-09-17T20:09:51.959416Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# # Modelo #20: Cambios d√≠a a d√≠a (delta vs d√≠a anterior) 'temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation'\n# # ‚úÖ MAE con deltas weather 24h: 124.1003 algo mal estoy haciendo o el overfit es grande por 90dias ,capaz con mas dias menos overfit\n# # üìä MAE anterior: 70.6517\n# # Usando √∫ltimos 90 d√≠as\n\n# print(\"üîÑ Modelo #20: Agregando deltas d√≠a a d√≠a...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear deltas d√≠a a d√≠a de weather features \n# weather_cols = ['temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation']\n\n# for col in weather_cols:\n#     if col in sample_90d.columns:\n#         # Delta de 24h hacia atr√°s por prediction_unit_id\n#         sample_90d[f'{col}_delta_24h'] = (sample_90d[col] - \n#                                          sample_90d.groupby('prediction_unit_id')[col].shift(24))\n        \n#         # Stats del delta\n#         valid_deltas = sample_90d[f'{col}_delta_24h'].notna().sum()\n#         print(f\"üìà {col}_delta_24h v√°lidos: {valid_deltas:,}\")\n\n# # Limpiar NaN en deltas (al menos uno debe ser v√°lido)\n# delta_cols = [f'{col}_delta_24h' for col in weather_cols if col in sample_90d.columns]\n# sample_90d = sample_90d.dropna(subset=delta_cols, how='all')\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos deltas)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con deltas weather 24h: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 70.6517\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.962292Z","iopub.execute_input":"2025-09-17T20:09:51.963095Z","iopub.status.idle":"2025-09-17T20:09:51.995702Z","shell.execute_reply.started":"2025-09-17T20:09:51.963047Z","shell.execute_reply":"2025-09-17T20:09:51.994369Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# # Modelo #21: Media m√≥vil de target por prediction_unit_id\n# # ‚úÖ MAE con media m√≥vil 7d: 56.1320 MMM data leakage por como estan los datos , veo como lo cambio\n# # üìä MAE anterior: 70.6517\n# # Usando √∫ltimos 90 d√≠as\n\n# print(\"üîÑ Modelo #21: Agregando media m√≥vil de target...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train6\n# max_date = train6['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train6[train6['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media m√≥vil de target por prediction_unit_id (ventana 7 d√≠as)\n# sample_90d['target_ma_7d'] = (sample_90d.groupby('prediction_unit_id')['target']\n#                               .shift(1)  # Evitar data leakage\n#                               .rolling(window=168, min_periods=24)  # 7 d√≠as = 168 horas\n#                               .mean())\n\n# # Stats de la media m√≥vil\n# valid_ma = sample_90d['target_ma_7d'].notna().sum()\n# print(f\"üìà Target MA 7d v√°lidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media m√≥vil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d'])\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media m√≥vil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con media m√≥vil 7d: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 70.6517\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:51.996940Z","iopub.execute_input":"2025-09-17T20:09:51.997280Z","iopub.status.idle":"2025-09-17T20:09:52.026142Z","shell.execute_reply.started":"2025-09-17T20:09:51.997244Z","shell.execute_reply":"2025-09-17T20:09:52.024899Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# # Modelo #22: Media m√≥vil de target por prediction_unit_id lag 5 dias\n# # ‚úÖ MAE con media m√≥vil 7d (lag 5d): 61.2636\n# # üìä MAE anterior: 70.6517\n# # Usando 120 d√≠as para calcular, reducir a 90 d√≠as\n\n# print(\"üîÑ Modelo #22: Media m√≥vil con datos suficientes...\")\n\n# # USAR 120 d√≠as para calcular media m√≥vil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular media m√≥vil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"üìä Dataset 120 d√≠as para c√°lculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media m√≥vil de target por prediction_unit_id (ventana 7 d√≠as, retraso 5 d√≠as)\n# sample_120d['target_ma_7d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 d√≠as = 120 horas\n#                                    .rolling(window=168, min_periods=168)  # 7 d√≠as = 168 horas\n#                                    .mean())\n\n# # AHORA reducir a √∫ltimos 90 d√≠as\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media m√≥vil\n# valid_ma = sample_90d['target_ma_7d_lag5d'].notna().sum()\n# print(f\"üìä Datos finales (90 d√≠as): {len(sample_90d):,} registros\")\n# print(f\"üìà Target MA 7d (lag 5d) v√°lidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media m√≥vil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos despu√©s de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media m√≥vil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con media m√≥vil 7d (lag 5d): {mae:.4f}\")\n# print(f\"üìä MAE anterior: 70.6517\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.027482Z","iopub.execute_input":"2025-09-17T20:09:52.027962Z","iopub.status.idle":"2025-09-17T20:09:52.058001Z","shell.execute_reply.started":"2025-09-17T20:09:52.027928Z","shell.execute_reply":"2025-09-17T20:09:52.056916Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# # Modelo #23: Media m√≥vil de target por prediction_unit_id lag 5 dias\n# #‚úÖ MAE con ambas media m√≥vil 7d y 3d (lag 5d): 64.0943\n# #üìä MAE anterior: 61.2636\n# # Usando 120 d√≠as para calcular, reducir a 90 d√≠as\n\n# print(\"üîÑ Modelo #23: Media m√≥vil con datos suficientes...\")\n\n# # USAR 120 d√≠as para calcular media m√≥vil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular media m√≥vil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"üìä Dataset 120 d√≠as para c√°lculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media m√≥vil de target por prediction_unit_id (ventana 7 d√≠as, retraso 5 d√≠as)\n# sample_120d['target_ma_7d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 d√≠as = 120 horas\n#                                    .rolling(window=168, min_periods=168)  # 7 d√≠as = 168 horas\n#                                    .mean())\n\n# sample_120d['target_ma_3d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 d√≠as = 120 horas\n#                                    .rolling(window=72, min_periods=72)  # 3 d√≠as = 72 horas\n#                                    .mean())\n\n\n# # AHORA reducir a √∫ltimos 90 d√≠as\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media m√≥vil\n# valid_ma = sample_90d['target_ma_7d_lag5d'].notna().sum()\n# print(f\"üìä Datos finales (90 d√≠as): {len(sample_90d):,} registros\")\n# print(f\"üìà Target MA 7d (lag 5d) v√°lidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Stats de la media m√≥vil\n# valid_ma1 = sample_90d['target_ma_3d_lag5d'].notna().sum()\n# print(f\"üìä Datos finales (90 d√≠as): {len(sample_90d):,} registros\")\n# print(f\"üìà Target MA 3d (lag 5d) v√°lidos: {valid_ma1:,} ({valid_ma1/len(sample_90d)*100:.1f}%)\")\n\n\n# # Limpiar NaN en media m√≥vil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos despu√©s de limpiar NaN: {len(sample_90d):,}\")\n\n# sample_90d = sample_90d.dropna(subset=['target_ma_3d_lag5d'])\n# print(f\"üìä Datos despu√©s de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media m√≥vil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con media m√≥vil 3d (lag 5d): {mae:.4f}\")\n# print(f\"üìä MAE anterior: 61.2636\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.059289Z","iopub.execute_input":"2025-09-17T20:09:52.059673Z","iopub.status.idle":"2025-09-17T20:09:52.088535Z","shell.execute_reply.started":"2025-09-17T20:09:52.059627Z","shell.execute_reply":"2025-09-17T20:09:52.087318Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# # Modelo #24: Media m√≥vil de target por prediction_unit_id lag 5 dias\n# #‚úÖ MAE con media m√≥vil 3d (lag 5d): 63.5897\n# #üìä MAE anterior: 61.2636\n# # Usando 120 d√≠as para calcular, reducir a 90 d√≠as\n\n# print(\"üîÑ Modelo #24: Media m√≥vil 3d...\")\n\n# # USAR 120 d√≠as para calcular media m√≥vil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular media m√≥vil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"üìä Dataset 120 d√≠as para c√°lculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media m√≥vil de target por prediction_unit_id (ventana 7 d√≠as, retraso 5 d√≠as)\n# sample_120d['target_ma_3d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 d√≠as = 120 horas\n#                                    .rolling(window=72, min_periods=72)  # 3 d√≠as = 72 horas\n#                                    .mean())\n\n# # AHORA reducir a √∫ltimos 90 d√≠as\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media m√≥vil\n# valid_ma = sample_90d['target_ma_3d_lag5d'].notna().sum()\n# print(f\"üìä Datos finales (90 d√≠as): {len(sample_90d):,} registros\")\n# print(f\"üìà Target MA 3d (lag 5d) v√°lidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media m√≥vil\n# sample_90d = sample_90d.dropna(subset=['target_ma_3d_lag5d'])\n# print(f\"üìä Datos despu√©s de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media m√≥vil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con media m√≥vil 3d (lag 5d): {mae:.4f}\")\n# print(f\"üìä MAE anterior: 61.2636\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.089993Z","iopub.execute_input":"2025-09-17T20:09:52.090402Z","iopub.status.idle":"2025-09-17T20:09:52.118719Z","shell.execute_reply.started":"2025-09-17T20:09:52.090373Z","shell.execute_reply":"2025-09-17T20:09:52.117535Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# # Modelo #25: Media m√≥vil de target por prediction_unit_id lag 5 dias\n# #‚úÖ MAE con media m√≥vil 5d (lag 5d): 71.2301\n# #üìä MAE anterior: 61.2636\n# # Usando 120 d√≠as para calcular, reducir a 90 d√≠as\n\n# print(\"üîÑ Modelo #25: Media m√≥vil 5d...\")\n\n# # USAR 120 d√≠as para calcular media m√≥vil, luego reducir a 90\n# max_date = train6['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular media m√≥vil\n# sample_120d = train6[train6['datetime'] >= cutoff_120d].copy()\n# print(f\"üìä Dataset 120 d√≠as para c√°lculo: {len(sample_120d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_120d = sample_120d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear media m√≥vil de target por prediction_unit_id (ventana 7 d√≠as, retraso 5 d√≠as)\n# sample_120d['target_ma_5d_lag5d'] = (sample_120d.groupby('prediction_unit_id')['target']\n#                                    .shift(120)  # 5 d√≠as = 120 horas\n#                                    .rolling(window=120, min_periods=120)  # 3 d√≠as = 72 horas\n#                                    .mean())\n\n# # AHORA reducir a √∫ltimos 90 d√≠as\n# sample_90d = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n\n# # Stats de la media m√≥vil\n# valid_ma = sample_90d['target_ma_5d_lag5d'].notna().sum()\n# print(f\"üìä Datos finales (90 d√≠as): {len(sample_90d):,} registros\")\n# print(f\"üìà Target MA 5d (lag 5d) v√°lidos: {valid_ma:,} ({valid_ma/len(sample_90d)*100:.1f}%)\")\n\n# # Limpiar NaN en media m√≥vil\n# sample_90d = sample_90d.dropna(subset=['target_ma_5d_lag5d'])\n# print(f\"üìä Datos despu√©s de limpiar NaN: {len(sample_90d):,}\")\n\n# # Features (incluye nueva media m√≥vil)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con media m√≥vil 5d (lag 5d): {mae:.4f}\")\n# print(f\"üìä MAE anterior: 61.2636\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.120037Z","iopub.execute_input":"2025-09-17T20:09:52.120439Z","iopub.status.idle":"2025-09-17T20:09:52.150302Z","shell.execute_reply.started":"2025-09-17T20:09:52.120403Z","shell.execute_reply":"2025-09-17T20:09:52.149201Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Crear train7: train6 + media m√≥vil 7d lag 5d + sin weather hist√≥rico\n# train6 tiene 150 d√≠as, primeros d√≠as tendr√°n NaN en MA\n\nprint(\"üîÑ Creando train7: train6 + target_ma_7d_lag5d + sin weather hist√≥rico...\")\n\n# Copiar train6 y ordenar\ntrain7 = train6.copy()\ntrain7 = train7.sort_values(['prediction_unit_id', 'datetime'])\n\nprint(f\"üìä train6 original: {len(train7):,} registros\")\n\n# Crear media m√≥vil de target por prediction_unit_id (ventana 7 d√≠as, retraso 5 d√≠as)\ntrain7['target_ma_7d_lag5d'] = (train7.groupby('prediction_unit_id')['target']\n                               .shift(120)  # 5 d√≠as = 120 horas\n                               .rolling(window=168, min_periods=168)  # 7 d√≠as = 168 horas\n                               .mean())\n\n# Stats de la nueva feature\nvalid_ma = train7['target_ma_7d_lag5d'].notna().sum()\nprint(f\"üìà target_ma_7d_lag5d v√°lidos: {valid_ma:,} ({valid_ma/len(train7)*100:.1f}%)\")\n\n# Eliminar columnas de historical_weather (no est√°n en test set)\nhistorical_weather_cols = [\n    'temperature',      # hist√≥rico\n    'dewpoint',         # hist√≥rico  \n    'rain',\n    'snowfall', \n    'surface_pressure',\n    'cloudcover_low',\n    'cloudcover_mid', \n    'cloudcover_high',\n    'cloudcover_total',\n    'windspeed_10m',\n    'winddirection_10m', \n    'shortwave_radiation',\n    'direct_solar_radiation',\n    'diffuse_radiation'\n]\n\n# Verificar qu√© columnas existen y eliminarlas\ncols_to_drop = [col for col in historical_weather_cols if col in train7.columns]\nprint(f\"üóëÔ∏è Eliminando {len(cols_to_drop)} columnas weather hist√≥ricas:\")\nfor col in cols_to_drop:\n    print(f\"   - {col}\")\n\ntrain7 = train7.drop(columns=cols_to_drop)\n\nprint(f\"‚úÖ train7 creado: {len(train7):,} registros, {len(train7.columns)} columnas\")\n\n# Borrar train6 para liberar memoria\ndel train6\ngc.collect()\nprint(\"üßπ train6 eliminado de memoria\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:52.151544Z","iopub.execute_input":"2025-09-17T20:09:52.151922Z","iopub.status.idle":"2025-09-17T20:09:56.995276Z","shell.execute_reply.started":"2025-09-17T20:09:52.151890Z","shell.execute_reply":"2025-09-17T20:09:56.994072Z"}},"outputs":[{"name":"stdout","text":"üîÑ Creando train7: train6 + target_ma_7d_lag5d + sin weather hist√≥rico...\nüìä train6 original: 7,821,024 registros\nüìà target_ma_7d_lag5d v√°lidos: 7,801,508 (99.8%)\nüóëÔ∏è Eliminando 14 columnas weather hist√≥ricas:\n   - temperature\n   - dewpoint\n   - rain\n   - snowfall\n   - surface_pressure\n   - cloudcover_low\n   - cloudcover_mid\n   - cloudcover_high\n   - cloudcover_total\n   - windspeed_10m\n   - winddirection_10m\n   - shortwave_radiation\n   - direct_solar_radiation\n   - diffuse_radiation\n‚úÖ train7 creado: 7,821,024 registros, 42 columnas\nüßπ train6 eliminado de memoria\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# # Modelo #26: Sin hist√≥rico weather (solo forecast weather)\n# #‚úÖ MAE sin weather hist√≥rico: 62.9796\n# #üìä MAE con weather hist√≥rico: 61.2636\n# # Eliminar columnas de historical_weather para ver impacto\n\n# print(\"üîÑ Modelo #26: Sin hist√≥rico weather...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Eliminar columnas de historical_weather (excepto datetime, latitude, longitude, data_block_id)\n# historical_weather_cols = [\n#     'temperature',      # hist√≥rico\n#     'dewpoint',         # hist√≥rico  \n#     'rain',\n#     'snowfall', \n#     'surface_pressure',\n#     'cloudcover_low',\n#     'cloudcover_mid', \n#     'cloudcover_high',\n#     'cloudcover_total',\n#     'windspeed_10m',\n#     'winddirection_10m', \n#     'shortwave_radiation',\n#     'direct_solar_radiation',\n#     'diffuse_radiation'\n# ]\n\n# # Verificar qu√© columnas existen y eliminarlas\n# cols_to_drop = [col for col in historical_weather_cols if col in sample_90d.columns]\n# print(f\"üóëÔ∏è Eliminando {len(cols_to_drop)} columnas weather hist√≥ricas:\")\n# for col in cols_to_drop:\n#     print(f\"   - {col}\")\n\n# sample_90d = sample_90d.drop(columns=cols_to_drop)\n\n# # Limpiar NaN en media m√≥vil\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (excluye target, datetime, forecast_date, row_id)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"üéØ Features totales: {len(feature_cols)} (sin weather hist√≥rico)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols]\n# y_train = train_data['target']\n# X_test = test_data[feature_cols]\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE sin weather hist√≥rico: {mae:.4f}\")\n# print(f\"üìä MAE con weather hist√≥rico: 61.2636\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:56.996467Z","iopub.execute_input":"2025-09-17T20:09:56.996983Z","iopub.status.idle":"2025-09-17T20:09:57.004342Z","shell.execute_reply.started":"2025-09-17T20:09:56.996956Z","shell.execute_reply":"2025-09-17T20:09:57.003012Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# # Modelo #27: Weather lags (variables solares) - 24h lag\n# # ‚úÖ MAE con weather lags 24h: 64.9544\n# # üìä MAE anterior: 62.9796\n\n\n# print(\"üîÑ Modelo #27: Weather lags 24h (variables solares)...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Variables solares para lag 24h\n# solar_vars = ['ftemperature', 'fdirect_solar_radiation', 'fsurface_solar_radiation_downwards', \n#               'fcloudcover_total', 'fsnowfall']\n\n# # Crear lags 24h de variables solares por prediction_unit_id\n# for var in solar_vars:\n#     if var in sample_90d.columns:\n#         sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n        \n#         # Stats del lag\n#         valid_lags = sample_90d[f'{var}_lag24h'].notna().sum()\n#         print(f\"üìà {var}_lag24h v√°lidos: {valid_lags:,}\")\n\n# # Limpiar NaN en media m√≥vil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos weather lags)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"üéØ Features totales: {len(feature_cols)} (incluye weather lags)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con weather lags 24h: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 62.9796\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.005556Z","iopub.execute_input":"2025-09-17T20:09:57.005988Z","iopub.status.idle":"2025-09-17T20:09:57.033203Z","shell.execute_reply.started":"2025-09-17T20:09:57.005962Z","shell.execute_reply":"2025-09-17T20:09:57.032034Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# # Modelo #28: Weather lag 24h - solo ftemperature\n# # ‚úÖ MAE con ftemperature lag 24h: 63.2238\n# # üìä MAE anterior: 62.9796\n\n\n# print(\"üîÑ Modelo #28: Weather lag 24h - solo ftemperature...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h solo de ftemperature\n# if 'ftemperature' in sample_90d.columns:\n#     sample_90d['ftemperature_lag24h'] = sample_90d.groupby('prediction_unit_id')['ftemperature'].shift(24)\n    \n#     # Stats del lag\n#     valid_lags = sample_90d['ftemperature_lag24h'].notna().sum()\n#     print(f\"üìà ftemperature_lag24h v√°lidos: {valid_lags:,}\")\n# else:\n#     print(\"‚ö†Ô∏è ftemperature no encontrada\")\n\n# # Limpiar NaN en media m√≥vil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevo weather lag)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"üéØ Features totales: {len(feature_cols)} (incluye ftemperature lag)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con ftemperature lag 24h: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 62.9796\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.034406Z","iopub.execute_input":"2025-09-17T20:09:57.034772Z","iopub.status.idle":"2025-09-17T20:09:57.066705Z","shell.execute_reply.started":"2025-09-17T20:09:57.034740Z","shell.execute_reply":"2025-09-17T20:09:57.065691Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# # Modelo #29: Weather lag 48h - solo ftemperature\n# # ‚úÖ MAE con ftemperature lag 48h: 62.3245\n# # üìä MAE anterior: 62.9796\n\n\n# print(\"üîÑ Modelo #29: Weather lag 48h - solo ftemperature...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear lag 24h solo de ftemperature\n# if 'ftemperature' in sample_90d.columns:\n#     sample_90d['ftemperature_lag48h'] = sample_90d.groupby('prediction_unit_id')['ftemperature'].shift(48)\n    \n#     # Stats del lag\n#     valid_lags = sample_90d['ftemperature_lag48h'].notna().sum()\n#     print(f\"üìà ftemperature_lag48h v√°lidos: {valid_lags:,}\")\n# else:\n#     print(\"‚ö†Ô∏è ftemperature no encontrada\")\n\n# # Limpiar NaN en media m√≥vil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevo weather lag)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"üéØ Features totales: {len(feature_cols)} (incluye ftemperature lag)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con ftemperature lag 48h: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 62.9796\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.067839Z","iopub.execute_input":"2025-09-17T20:09:57.068138Z","iopub.status.idle":"2025-09-17T20:09:57.096001Z","shell.execute_reply.started":"2025-09-17T20:09:57.068110Z","shell.execute_reply":"2025-09-17T20:09:57.094623Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# # Modelo 30\n# # #üéâ Variables que MEJORAN (ordenadas por MAE):\n# #    fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n# #    ftemperature_lag48h: 62.3245 (-0.6551)\n# #    fcloudcover_total_lag48h: 62.3520 (-0.6276)\n# def test_weather_lags():\n#     \"\"\"\n#     Prueba cada variable weather con lag 24h y 48h individualmente\n#     \"\"\"\n#     print(\"üîÑ Creando todos los weather lags...\")\n    \n#     # Variables solares a probar\n#     solar_vars = ['ftemperature', 'fdirect_solar_radiation', 'fsurface_solar_radiation_downwards', \n#                   'fcloudcover_total', 'fsnowfall']\n    \n#     # Filtrar √∫ltimos 90 d√≠as de train7\n#     max_date = train7['datetime'].max()\n#     cutoff_90d = max_date - pd.DateOffset(days=90)\n#     sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n    \n#     # Ordenar por prediction_unit_id y datetime\n#     sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n    \n#     # Crear TODOS los lags de una vez\n#     print(\"üìä Creando lags...\")\n#     for var in solar_vars:\n#         if var in sample_90d.columns:\n#             # Lag 24h\n#             sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n#             # Lag 48h  \n#             sample_90d[f'{var}_lag48h'] = sample_90d.groupby('prediction_unit_id')[var].shift(48)\n#             print(f\"   ‚úÖ {var} lags creados\")\n#         else:\n#             print(f\"   ‚ö†Ô∏è {var} no encontrada\")\n    \n#     # Limpiar NaN en media m√≥vil\n#     sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n    \n#     # Features base (sin los nuevos lags)\n#     base_features = [col for col in sample_90d.columns \n#                      if col not in ['target', 'datetime', 'forecast_date', 'row_id'] \n#                      and not col.endswith('_lag24h') and not col.endswith('_lag48h')]\n    \n#     # Split 80/20 una sola vez\n#     split_idx = int(len(sample_90d) * 0.8)\n#     train_data = sample_90d.iloc[:split_idx]\n#     test_data = sample_90d.iloc[split_idx:]\n    \n#     baseline_mae = 62.9796\n#     results = []\n    \n#     print(f\"\\nüß™ Probando cada lag individualmente contra baseline {baseline_mae}...\")\n    \n#     # Probar cada lag individualmente\n#     for var in solar_vars:\n#         if var in sample_90d.columns:\n#             for lag_hours in [24, 48]:\n#                 lag_col = f'{var}_lag{lag_hours}h'\n                \n#                 # Features = base + este lag espec√≠fico\n#                 test_features = base_features + [lag_col]\n                \n#                 X_train = train_data[test_features].fillna(0)\n#                 y_train = train_data['target']\n#                 X_test = test_data[test_features].fillna(0)\n#                 y_test = test_data['target']\n                \n#                 # Modelo\n#                 model = lgb.LGBMRegressor(\n#                     n_estimators=100,\n#                     max_depth=6,\n#                     learning_rate=0.1,\n#                     random_state=42,\n#                     n_jobs=-1,\n#                     verbose=-1\n#                 )\n                \n#                 model.fit(X_train, y_train)\n#                 pred = model.predict(X_test)\n#                 mae = mean_absolute_error(y_test, pred)\n                \n#                 # A1: Resultado individual\n#                 change = mae - baseline_mae\n#                 status = \"üéâ MEJORA\" if mae < baseline_mae else \"‚ùå EMPEORA\"\n                \n#                 print(f\"\\n‚úÖ MAE con {var} lag {lag_hours}h: {mae:.4f}\")\n#                 print(f\"üìä Cambio: {change:+.4f} - {status}\")\n                \n#                 # Guardar resultado\n#                 results.append({\n#                     'variable': var,\n#                     'lag_hours': lag_hours,\n#                     'mae': mae,\n#                     'change': change,\n#                     'improves': mae < baseline_mae\n#                 })\n    \n#     # Resumen final\n#     print(\"\\n\" + \"=\"*60)\n#     print(\"üìä RESUMEN DE RESULTADOS:\")\n#     print(\"=\"*60)\n    \n#     # Mejores resultados\n#     improving = [r for r in results if r['improves']]\n#     if improving:\n#         improving.sort(key=lambda x: x['mae'])\n#         print(\"\\nüéâ Variables que MEJORAN (ordenadas por MAE):\")\n#         for r in improving:\n#             print(f\"   {r['variable']}_lag{r['lag_hours']}h: {r['mae']:.4f} ({r['change']:+.4f})\")\n    \n#     # Peores resultados  \n#     worsening = [r for r in results if not r['improves']]\n#     if worsening:\n#         worsening.sort(key=lambda x: x['change'], reverse=True)  # Menos empeoramiento primero\n#         print(f\"\\n‚ùå Variables que EMPEORAN:\")\n#         for r in worsening:\n#             print(f\"   {r['variable']}_lag{r['lag_hours']}h: {r['mae']:.4f} ({r['change']:+.4f})\")\n    \n#     return results\n\n# # Ejecutar la funci√≥n\n# results = test_weather_lags()\n\n\n# # #üéâ Variables que MEJORAN (ordenadas por MAE):\n# #    fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n# #    ftemperature_lag48h: 62.3245 (-0.6551)\n# #    fcloudcover_total_lag48h: 62.3520 (-0.6276)\n\n# # ‚ùå Variables que EMPEORAN:\n# #    fdirect_solar_radiation_lag48h: 67.6610 (+4.6814)\n# #    fsurface_solar_radiation_downwards_lag48h: 65.9643 (+2.9847)\n# #    fsnowfall_lag48h: 65.7171 (+2.7375)\n# #    fsurface_solar_radiation_downwards_lag24h: 65.3518 (+2.3722)\n# #    fcloudcover_total_lag24h: 64.2290 (+1.2494)\n# #    fsnowfall_lag24h: 63.7094 (+0.7298)\n# #    ftemperature_lag24h: 63.2238 (+0.2442)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.097535Z","iopub.execute_input":"2025-09-17T20:09:57.098463Z","iopub.status.idle":"2025-09-17T20:09:57.131611Z","shell.execute_reply.started":"2025-09-17T20:09:57.098420Z","shell.execute_reply":"2025-09-17T20:09:57.130444Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# # Modelo #31: Weather lags (variables solares) \n# # fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n# # ftemperature_lag48h: 62.3245 (-0.6551)\n# # fcloudcover_total_lag48h: 62.3520 (-0.6276)\n\n\n# print(\"üîÑ Modelo #31: Weather lags 24h (variables solares)...\")\n\n# # Filtrar √∫ltimos 90 d√≠as de train7\n# max_date = train7['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train7[train7['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Variables solares para lag 24h\n# solar_vars24 = ['fdirect_solar_radiation']\n\n# #Variables solares para lag 48h\n# #solar_vars48 = ['ftemperature', 'fcloudcover_total']\n# #solar_vars48 = ['ftemperature']\n# #solar_vars48 = ['fcloudcover_total']\n\n# #Crear lags 24h de variables solares por prediction_unit_id\n# for var in solar_vars24:\n#     if var in sample_90d.columns:\n#         sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n        \n#         # Stats del lag\n#         valid_lags = sample_90d[f'{var}_lag24h'].notna().sum()\n#         print(f\"üìà {var}_lag24h v√°lidos: {valid_lags:,}\")\n\n# # for var in solar_vars48:\n# #     if var in sample_90d.columns:\n# #         sample_90d[f'{var}_lag48h'] = sample_90d.groupby('prediction_unit_id')[var].shift(48)\n        \n# #         # Stats del lag\n# #         valid_lags1 = sample_90d[f'{var}_lag48h'].notna().sum()\n# #         print(f\"üìà {var}_lag48h v√°lidos: {valid_lags1:,}\")\n\n# # Limpiar NaN en media m√≥vil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos weather lags)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# print(f\"üéØ Features totales: {len(feature_cols)} (incluye weather lags)\")\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE con weather lags 24 y 48h: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 62.9796\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.132736Z","iopub.execute_input":"2025-09-17T20:09:57.133061Z","iopub.status.idle":"2025-09-17T20:09:57.162905Z","shell.execute_reply.started":"2025-09-17T20:09:57.133033Z","shell.execute_reply":"2025-09-17T20:09:57.161462Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# incorporo fdirect_solar_radiation_lag24h\ntrain8 = train7.sort_values(['prediction_unit_id', 'datetime'])\ntrain8['fdirect_solar_radiation_lag24h'] = train8.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(24)\nprint(\"‚úÖ fdirect_solar_radiation_lag24h agregado a train8\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:57.164225Z","iopub.execute_input":"2025-09-17T20:09:57.165641Z","iopub.status.idle":"2025-09-17T20:09:58.680949Z","shell.execute_reply.started":"2025-09-17T20:09:57.165595Z","shell.execute_reply":"2025-09-17T20:09:58.678762Z"}},"outputs":[{"name":"stdout","text":"‚úÖ fdirect_solar_radiation_lag24h agregado a train8\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# # Modelo 36\n# # \n# # fdewpoint_lag24h: 64.4357 (+2.8546) ‚ùå EMPEORA\n# # fdewpoint_lag48h: 67.4056 (+5.8245) ‚ùå EMPEORA\n# # fcloudcover_high_lag24h: 66.1002 (+4.5191) ‚ùå EMPEORA\n# # fcloudcover_high_lag48h: 66.3328 (+4.7517) ‚ùå EMPEORA\n# # fcloudcover_low_lag24h: 62.4589 (+0.8778) ‚ùå EMPEORA\n# # fcloudcover_low_lag48h: 66.0350 (+4.4539) ‚ùå EMPEORA\n# # fcloudcover_mid_lag24h: 64.7723 (+3.1912) ‚ùå EMPEORA\n# # fcloudcover_mid_lag48h: 67.0185 (+5.4374) ‚ùå EMPEORA\n# # f10_metre_u_wind_component_lag24h: 63.2744 (+1.6933) ‚ùå EMPEORA\n# # f10_metre_u_wind_component_lag48h: 64.4923 (+2.9112) ‚ùå EMPEORA\n# # f10_metre_v_wind_component_lag24h: 68.0118 (+6.4307) ‚ùå EMPEORA\n# # f10_metre_v_wind_component_lag48h: 64.3495 (+2.7684) ‚ùå EMPEORA\n# # ftotal_precipitation_lag24h: 64.6852 (+3.1041) ‚ùå EMPEORA\n# # ftotal_precipitation_lag48h: 67.5245 (+5.9434) ‚ùå EMPEORA\n\n# climate_vars = ['fdewpoint', 'fcloudcover_high', 'fcloudcover_low', 'fcloudcover_mid',\n#                 'f10_metre_u_wind_component', 'f10_metre_v_wind_component', 'ftotal_precipitation']\n\n# # Preparar datos\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Crear todos los lags clim√°ticos\n# for var in climate_vars:\n#     sample_90d[f'{var}_lag24h'] = sample_90d.groupby('prediction_unit_id')[var].shift(24)\n#     sample_90d[f'{var}_lag48h'] = sample_90d.groupby('prediction_unit_id')[var].shift(48)\n\n# # Limpiar y split\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# # Features base\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id'] \n#                  and not any(f'{var}_lag' in col for var in climate_vars)]\n\n# baseline_mae = 61.5811\n# results = []\n\n# # Probar cada lag individual\n# for var in climate_vars:\n#     for lag_hours in [24, 48]:\n#         lag_col = f'{var}_lag{lag_hours}h'\n#         test_features = base_features + [lag_col]\n        \n#         X_train = train_data[test_features].fillna(0)\n#         X_test = test_data[test_features].fillna(0)\n        \n#         model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                                  random_state=42, n_jobs=-1, verbose=-1)\n#         model.fit(X_train, train_data['target'])\n#         mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n        \n#         change = mae - baseline_mae\n#         status = \"üéâ MEJORA\" if mae < baseline_mae else \"‚ùå EMPEORA\"\n#         print(f\"{var}_lag{lag_hours}h: {mae:.4f} ({change:+.4f}) {status}\")\n        \n#         results.append({'variable': var, 'lag_hours': lag_hours, 'mae': mae, 'change': change})\n\n# # Mejores resultados\n# improving = [r for r in results if r['mae'] < baseline_mae]\n# if improving:\n#     improving.sort(key=lambda x: x['mae'])\n#     print(\"\\nüéâ MEJORAN:\")\n#     for r in improving:\n#         print(f\"   {r['variable']}_lag{r['lag_hours']}h: {r['mae']:.4f} ({r['change']:+.4f})\")\n# else:\n#     print(\"üòî Ninguna variable clim√°tica mejora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.682834Z","iopub.execute_input":"2025-09-17T20:09:58.684043Z","iopub.status.idle":"2025-09-17T20:09:58.691946Z","shell.execute_reply.started":"2025-09-17T20:09:58.683990Z","shell.execute_reply":"2025-09-17T20:09:58.690314Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# # Modelo #37\n# # temp_solar_interaction: 63.5858 (+2.0047) ‚ùå EMPEORA\n# # cloud_solar_ratio: 64.5474 (+2.9663) ‚ùå EMPEORA\n# # wind_magnitude: 66.2764 (+4.6953) ‚ùå EMPEORA\n# # temp_dewpoint_diff: 65.7131 (+4.1320) ‚ùå EMPEORA\n# # surface_direct_ratio: 65.2653 (+3.6842) ‚ùå EMPEORA\n# # ftemperature_roll3h: 64.2013 (+2.6202) ‚ùå EMPEORA\n# # ftemperature_roll6h: 64.3444 (+2.7633) ‚ùå EMPEORA\n# # fdirect_solar_radiation_roll3h: 64.2987 (+2.7176) ‚ùå EMPEORA\n# # fdirect_solar_radiation_roll6h: 66.8305 (+5.2494) ‚ùå EMPEORA\n# # fcloudcover_total_roll3h: 64.4380 (+2.8569) ‚ùå EMPEORA\n# # fcloudcover_total_roll6h: 65.9477 (+4.3666) ‚ùå EMPEORA\n\n\n# # Preparar datos base\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # 1. INTERACCIONES WEATHER\n# print(\"üîÑ Creando interacciones weather...\")\n# sample_90d['temp_solar_interaction'] = sample_90d['ftemperature'] * sample_90d['fdirect_solar_radiation']\n# sample_90d['cloud_solar_ratio'] = sample_90d['fdirect_solar_radiation'] / (sample_90d['fcloudcover_total'] + 0.01)\n# sample_90d['wind_magnitude'] = np.sqrt(sample_90d['f10_metre_u_wind_component']**2 + sample_90d['f10_metre_v_wind_component']**2)\n\n# # 2. DIFERENCIAS Y RATIOS\n# sample_90d['temp_dewpoint_diff'] = sample_90d['ftemperature'] - sample_90d['fdewpoint']\n# sample_90d['surface_direct_ratio'] = sample_90d['fsurface_solar_radiation_downwards'] / (sample_90d['fdirect_solar_radiation'] + 1)\n\n# # 3. ROLLING FEATURES (ventanas m√°s cortas)\n# for var in ['ftemperature', 'fdirect_solar_radiation', 'fcloudcover_total']:\n#     sample_90d[f'{var}_roll3h'] = sample_90d.groupby('prediction_unit_id')[var].rolling(3, min_periods=1).mean().values\n#     sample_90d[f'{var}_roll6h'] = sample_90d.groupby('prediction_unit_id')[var].rolling(6, min_periods=1).mean().values\n\n# # Limpiar y split\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# # Features base (con fdirect_solar_radiation_lag24h)\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Nuevas features a probar\n# new_features = [\n#     'temp_solar_interaction',\n#     'cloud_solar_ratio', \n#     'wind_magnitude',\n#     'temp_dewpoint_diff',\n#     'surface_direct_ratio',\n#     'ftemperature_roll3h',\n#     'ftemperature_roll6h',\n#     'fdirect_solar_radiation_roll3h',\n#     'fdirect_solar_radiation_roll6h',\n#     'fcloudcover_total_roll3h',\n#     'fcloudcover_total_roll6h'\n# ]\n\n# # Features base sin las nuevas\n# base_only = [col for col in base_features if col not in new_features]\n# baseline_mae = 61.5811\n\n# print(f\"üß™ Probando {len(new_features)} nuevas features...\")\n\n# # Probar cada feature nueva individualmente\n# results = []\n# for feature in new_features:\n#     if feature in sample_90d.columns:\n#         test_features = base_only + [feature]\n        \n#         X_train = train_data[test_features].fillna(0)\n#         X_test = test_data[test_features].fillna(0)\n        \n#         model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                                  random_state=42, n_jobs=-1, verbose=-1)\n#         model.fit(X_train, train_data['target'])\n#         mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n        \n#         change = mae - baseline_mae\n#         status = \"üéâ MEJORA\" if mae < baseline_mae else \"‚ùå EMPEORA\"\n#         print(f\"{feature}: {mae:.4f} ({change:+.4f}) {status}\")\n        \n#         results.append({'feature': feature, 'mae': mae, 'change': change})\n\n# # Mejores resultados\n# improving = [r for r in results if r['mae'] < baseline_mae]\n# if improving:\n#     improving.sort(key=lambda x: x['mae'])\n#     print(f\"\\nüéâ MEJORAN ({len(improving)}):\")\n#     for r in improving:\n#         print(f\"   {r['feature']}: {r['mae']:.4f} ({r['change']:+.4f})\")\n# else:\n#     # Mostrar las menos malas\n#     results.sort(key=lambda x: x['change'])\n#     print(f\"\\nüîù TOP 3 menos malas:\")\n#     for r in results[:3]:\n#         print(f\"   {r['feature']}: {r['mae']:.4f} ({r['change']:+.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.693393Z","iopub.execute_input":"2025-09-17T20:09:58.693751Z","iopub.status.idle":"2025-09-17T20:09:58.728007Z","shell.execute_reply.started":"2025-09-17T20:09:58.693717Z","shell.execute_reply":"2025-09-17T20:09:58.726741Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# # Modelo #38 la forma de calculo tiene filtracion de datos y aun asi no funciona\n# # Clustering de prediction_units por comportamiento similar\n# # cluster: 64.9886 (+3.4075) ‚ùå EMPEORA\n# # cluster_hour_avg_target: 61.8005 (+0.2194) ‚ùå EMPEORA\n# # üî• Probando ambas cluster features juntas...\n# # cluster + cluster_hour_avg_target: 62.4489 (+0.8678) ‚ùå EMPEORA\n\n# from sklearn.cluster import KMeans\n# from sklearn.preprocessing import StandardScaler\n\n# # 1. CREAR PERFIL DE CADA PREDICTION_UNIT\n# print(\"üìä Creando perfiles de prediction_units...\")\n\n# # Usar train8 completo (150 d√≠as) para clustering\n# # Agregaciones por prediction_unit_id para caracterizar comportamiento\n# unit_profiles = train8.groupby('prediction_unit_id').agg({\n#     'target': ['mean', 'std', 'min', 'max'],\n#     'hour': lambda x: x.value_counts().idxmax(),  # hora m√°s com√∫n\n#     'is_consumption': 'mean',  # % tiempo consumiendo\n#     'weekend': 'mean',  # % tiempo en weekend  \n#     'installed_capacity': 'first',\n#     'eic_count': 'first',\n#     'is_business': 'first',\n#     'county': 'first'\n# }).round(4)\n\n# # Aplanar nombres de columnas\n# unit_profiles.columns = ['_'.join(col).strip() for col in unit_profiles.columns]\n# unit_profiles = unit_profiles.reset_index()\n\n# # 2. CLUSTERING\n# print(\"üîÑ Aplicando KMeans clustering...\")\n\n# # Features para clustering (solo num√©ricas)\n# cluster_features = [\n#     'target_mean', 'target_std', 'target_min', 'target_max',\n#     'hour_<lambda>', 'is_consumption_mean', 'weekend_mean',\n#     'installed_capacity_first', 'eic_count_first'\n# ]\n\n# # Preparar datos\n# X_cluster = unit_profiles[cluster_features].fillna(0)\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X_cluster)\n\n# # Probar diferentes n√∫meros de clusters\n# silhouette_scores = []\n# for n_clusters in range(3, 8):\n#     kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n#     labels = kmeans.fit_predict(X_scaled)\n    \n#     from sklearn.metrics import silhouette_score\n#     score = silhouette_score(X_scaled, labels)\n#     silhouette_scores.append((n_clusters, score))\n#     print(f\"   K={n_clusters}: silhouette={score:.3f}\")\n\n# # Mejor n√∫mero de clusters\n# best_k = max(silhouette_scores, key=lambda x: x[1])[0]\n# print(f\"üéØ Mejor K: {best_k}\")\n\n# # Clustering final\n# kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n# unit_profiles['cluster'] = kmeans_final.fit_predict(X_scaled)\n\n# print(f\"\\nüìà Distribuci√≥n de clusters:\")\n# print(unit_profiles['cluster'].value_counts().sort_index())\n\n# # 3. AGREGAR CLUSTER AL DATASET ORIGINAL\n# print(\"\\nüîÑ Agregando clusters a muestra de 90 d√≠as...\")\n\n# # Sample 90 d√≠as para testing r√°pido\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.merge(\n#     unit_profiles[['prediction_unit_id', 'cluster']], \n#     on='prediction_unit_id', \n#     how='left'\n# )\n\n# # Crear features adicionales basadas en cluster\n# cluster_stats = sample_90d.groupby(['cluster', 'hour'])['target'].mean().reset_index()\n# cluster_stats.columns = ['cluster', 'hour', 'cluster_hour_avg_target']\n\n# sample_90d = sample_90d.merge(cluster_stats, on=['cluster', 'hour'], how='left')\n\n# # 4. PROBAR CLUSTER FEATURES\n# print(\"\\nüß™ Probando cluster features...\")\n\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id', 'cluster', 'cluster_hour_avg_target']]\n\n# baseline_mae = 61.5811\n\n# # Probar features de cluster individualmente\n# cluster_features_test = ['cluster', 'cluster_hour_avg_target']\n# results = []\n\n# for feature in cluster_features_test:\n#     test_features = base_features + [feature]\n    \n#     X_train = train_data[test_features].fillna(0)\n#     X_test = test_data[test_features].fillna(0)\n    \n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, train_data['target'])\n#     mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n    \n#     change = mae - baseline_mae\n#     status = \"üéâ MEJORA\" if mae < baseline_mae else \"‚ùå EMPEORA\"\n#     print(f\"{feature}: {mae:.4f} ({change:+.4f}) {status}\")\n    \n#     results.append({'feature': feature, 'mae': mae, 'change': change})\n\n# # Ambas features juntas\n# print(f\"\\nüî• Probando ambas cluster features juntas...\")\n# test_features = base_features + cluster_features_test\n\n# X_train = train_data[test_features].fillna(0)\n# X_test = test_data[test_features].fillna(0)\n\n# model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                          random_state=42, n_jobs=-1, verbose=-1)\n# model.fit(X_train, train_data['target'])\n# mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n\n# change = mae - baseline_mae\n# status = \"üéâ MEJORA\" if mae < baseline_mae else \"‚ùå EMPEORA\"\n# print(f\"cluster + cluster_hour_avg_target: {mae:.4f} ({change:+.4f}) {status}\")\n\n# # Mostrar caracter√≠sticas de cada cluster\n# print(f\"\\nüìä Perfil de clusters:\")\n# for cluster_id in sorted(unit_profiles['cluster'].unique()):\n#     cluster_data = unit_profiles[unit_profiles['cluster'] == cluster_id]\n#     n_units = len(cluster_data)\n#     avg_target = cluster_data['target_mean_'].mean()\n#     avg_consumption = cluster_data['is_consumption_mean'].mean()\n    \n#     print(f\"   Cluster {cluster_id}: {n_units} units, target_avg={avg_target:.1f}, consumption%={avg_consumption:.1%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.729294Z","iopub.execute_input":"2025-09-17T20:09:58.729666Z","iopub.status.idle":"2025-09-17T20:09:58.750257Z","shell.execute_reply.started":"2025-09-17T20:09:58.729636Z","shell.execute_reply":"2025-09-17T20:09:58.748755Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# #Modelo #39 long_lag_features = ['fdirect_solar_radiation_lag168h', 'fdirect_solar_radiation_lag336h', 'ftemperature_lag168h']\n# # fdirect_solar_radiation_lag168h: 65.1855 (+3.6044) ‚ùå EMPEORA\n# # fdirect_solar_radiation_lag336h: 65.7386 (+4.1575) ‚ùå EMPEORA\n# # ftemperature_lag168h: 63.7261 (+2.1450) ‚ùå EMPEORA\n\n# #LAGS M√ÅS LARGOS (evitar data leakage)\n# print(\"üïê Probando lags m√°s largos de variables exitosas...\")\n\n# max_date = train8['datetime'].max()\n# sample_90d = train8[train8['datetime'] >= max_date - pd.DateOffset(days=90)].copy()\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Lags de 7 d√≠as (168h) y 14 d√≠as (336h) de la variable que ya funciona\n# sample_90d['fdirect_solar_radiation_lag168h'] = sample_90d.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(168)\n# sample_90d['fdirect_solar_radiation_lag336h'] = sample_90d.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(336)\n\n# # Tambi√©n probar otros lags de temperatura que casi funcionaba\n# sample_90d['ftemperature_lag168h'] = sample_90d.groupby('prediction_unit_id')['ftemperature'].shift(168)\n\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data, test_data = sample_90d.iloc[:split_idx], sample_90d.iloc[split_idx:]\n\n# base_features = [col for col in sample_90d.columns \n#                  if col not in ['target', 'datetime', 'forecast_date', 'row_id'] \n#                  and not col.endswith('_lag168h') and not col.endswith('_lag336h')]\n\n# baseline_mae = 61.5811\n# long_lag_features = ['fdirect_solar_radiation_lag168h', 'fdirect_solar_radiation_lag336h', 'ftemperature_lag168h']\n\n# for feature in long_lag_features:\n#     test_features = base_features + [feature]\n    \n#     X_train = train_data[test_features].fillna(0)\n#     X_test = test_data[test_features].fillna(0)\n    \n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, train_data['target'])\n#     mae = mean_absolute_error(test_data['target'], model.predict(X_test))\n    \n#     change = mae - baseline_mae\n#     status = \"üéâ MEJORA\" if mae < baseline_mae else \"‚ùå EMPEORA\"\n#     print(f\"{feature}: {mae:.4f} ({change:+.4f}) {status}\")\n\n# print(\"\\n\" + \"=\"*50)\n\n# print(f\"\\nüí∞ RESUMEN: Actualmente tienes MAE {baseline_mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.751946Z","iopub.execute_input":"2025-09-17T20:09:58.752292Z","iopub.status.idle":"2025-09-17T20:09:58.780091Z","shell.execute_reply.started":"2025-09-17T20:09:58.752268Z","shell.execute_reply":"2025-09-17T20:09:58.778945Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# # Modelo #40 fdirect_solar_radiation la problematica , train8 tiene 150dias y luego corta a 90dias train8 tiene incorporada la variable\n# #                                                      train7 corta primero a 90dias y luego calcula , incorpora aca la variable\n# #‚úÖ MAE: 64.9886 con train8\n# # MAE: 62.9796 con train7\n\n# # Filtrar √∫ltimos 90 d√≠as de train8\n# max_date = train8['datetime'].max()\n# cutoff_90d = max_date - pd.DateOffset(days=90)\n# sample_90d = train8[train8['datetime'] >= cutoff_90d].copy()\n\n# print(f\"üìä Dataset 90 d√≠as: {len(sample_90d):,} registros\")\n\n# # Ordenar por prediction_unit_id y datetime\n# sample_90d = sample_90d.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Limpiar NaN en media m√≥vil (feature principal)\n# sample_90d = sample_90d.dropna(subset=['target_ma_7d_lag5d'])\n# print(f\"üìä Datos finales: {len(sample_90d):,}\")\n\n# # Features (incluye nuevos weather lags)\n# feature_cols = [col for col in sample_90d.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n\n# # Split 80/20\n# split_idx = int(len(sample_90d) * 0.8)\n# train_data = sample_90d.iloc[:split_idx]\n# test_data = sample_90d.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].fillna(0)  # Llenar NaN en lags con 0\n# y_train = train_data['target']\n# X_test = test_data[feature_cols].fillna(0)\n# y_test = test_data['target']\n\n# # Modelo LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultado\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"\\n‚úÖ MAE: {mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.781436Z","iopub.execute_input":"2025-09-17T20:09:58.781801Z","iopub.status.idle":"2025-09-17T20:09:58.808973Z","shell.execute_reply.started":"2025-09-17T20:09:58.781770Z","shell.execute_reply":"2025-09-17T20:09:58.807742Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Modelo #41 pruebas con distintios dias ,parace haber concept drift\n\n# D√≠as\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7 #modelo 31\n# 100\t63.9248\t\t63.4369\t\t-0.4878\t\tTRAIN8\n# 110\t64.4976\t\t65.3186\t\t+0.8210\t\tTRAIN7\n# 120\t65.7664\t\t66.9320\t\t+1.1656\t\tTRAIN7\n\n# def test_single_cutoff_train7(days):\n#     \"\"\"Probar UN cutoff con m√©todo train7\"\"\"\n#     print(f\"\\nüîÑ TRAIN7 - {days} d√≠as...\")\n    \n#     max_date = train7['datetime'].max()\n#     cutoff = max_date - pd.DateOffset(days=days)\n    \n#     # Cortar primero\n#     sample = train7[train7['datetime'] >= cutoff].copy()\n    \n#     # Agregar lag despu√©s\n#     sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n#     sample['fdirect_solar_radiation_lag24h'] = sample.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(24)\n    \n#     # Limpiar\n#     sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n#     feature_cols = [col for col in sample.columns \n#                    if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n#     # Stats\n#     valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n#     total_records = len(sample)\n    \n#     # Split y modelo\n#     split_idx = int(len(sample) * 0.8)\n#     train_data = sample.iloc[:split_idx]\n#     test_data = sample.iloc[split_idx:]\n    \n#     X_train = train_data[feature_cols].fillna(0)\n#     y_train = train_data['target']\n#     X_test = test_data[feature_cols].fillna(0)\n#     y_test = test_data['target']\n    \n#     # Limpiar muestras grandes\n#     del sample, train_data, test_data\n#     gc.collect()\n    \n#     # Modelo\n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, y_train)\n#     mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n#     # Limpiar modelo\n#     del model, X_train, X_test, y_train, y_test\n#     gc.collect()\n    \n#     print(f\"   üìà Registros: {total_records:,} | Lags v√°lidos: {valid_lags:,} | MAE: {mae:.4f}\")\n#     return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\n# def test_single_cutoff_train8(days):\n#     \"\"\"Probar UN cutoff con m√©todo train8\"\"\"\n#     print(f\"\\nüîÑ TRAIN8 - {days} d√≠as...\")\n    \n#     max_date = train8['datetime'].max()\n#     cutoff = max_date - pd.DateOffset(days=days)\n    \n#     # Cortar (lag ya existe)\n#     sample = train8[train8['datetime'] >= cutoff].copy()\n#     sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n#     sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n    \n#     feature_cols = [col for col in sample.columns \n#                    if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n#     # Stats\n#     valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n#     total_records = len(sample)\n    \n#     # Split y modelo\n#     split_idx = int(len(sample) * 0.8)\n#     train_data = sample.iloc[:split_idx]\n#     test_data = sample.iloc[split_idx:]\n    \n#     X_train = train_data[feature_cols].fillna(0)\n#     y_train = train_data['target']\n#     X_test = test_data[feature_cols].fillna(0)\n#     y_test = test_data['target']\n    \n#     # Limpiar muestras grandes\n#     del sample, train_data, test_data\n#     gc.collect()\n    \n#     # Modelo\n#     model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n#                              random_state=42, n_jobs=-1, verbose=-1)\n#     model.fit(X_train, y_train)\n#     mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n#     # Limpiar modelo\n#     del model, X_train, X_test, y_train, y_test\n#     gc.collect()\n    \n#     print(f\"   üìà Registros: {total_records:,} | Lags v√°lidos: {valid_lags:,} | MAE: {mae:.4f}\")\n#     return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\n# # EJECUTAR DE A UNO\n# print(\"üöÄ Probando cutoffs de a uno...\")\n# results = []\n\n# cutoff_days = [90, 100, 110, 120]\n\n# for days in cutoff_days:\n#     print(f\"\\n{'='*50}\")\n#     print(f\"PROBANDO {days} D√çAS\")\n#     print(f\"{'='*50}\")\n    \n#     # Train7\n#     result_t7 = test_single_cutoff_train7(days)\n    \n#     # Train8  \n#     result_t8 = test_single_cutoff_train8(days)\n    \n#     # Comparar\n#     diff = result_t8['mae'] - result_t7['mae']\n#     winner = \"TRAIN7\" if result_t7['mae'] < result_t8['mae'] else \"TRAIN8\"\n    \n#     print(f\"\\nüéØ RESUMEN {days} d√≠as:\")\n#     print(f\"   Train7: {result_t7['mae']:.4f}\")\n#     print(f\"   Train8: {result_t8['mae']:.4f}\")\n#     print(f\"   Diferencia: {diff:+.4f}\")\n#     print(f\"   Ganador: {winner}\")\n    \n#     results.append({\n#         'days': days,\n#         'train7_mae': result_t7['mae'],\n#         'train8_mae': result_t8['mae'],\n#         'difference': diff,\n#         'winner': winner\n#     })\n    \n#     # Forzar limpieza memoria\n#     gc.collect()\n\n# # RESUMEN FINAL\n# print(f\"\\n{'='*60}\")\n# print(\"üìä RESUMEN FINAL\")\n# print(f\"{'='*60}\")\n\n# print(\"\\nD√≠as\\tTrain7 MAE\\tTrain8 MAE\\tDiferencia\\tGanador\")\n# print(\"-\" * 55)\n# for r in results:\n#     print(f\"{r['days']}\\t{r['train7_mae']:.4f}\\t\\t{r['train8_mae']:.4f}\\t\\t{r['difference']:+.4f}\\t\\t{r['winner']}\")\n\n# # Mejor configuraci√≥n general\n# best_overall = min(results, key=lambda x: min(x['train7_mae'], x['train8_mae']))\n# best_mae = min(best_overall['train7_mae'], best_overall['train8_mae'])\n# best_method = \"Train7\" if best_overall['train7_mae'] < best_overall['train8_mae'] else \"Train8\"\n\n# print(f\"\\nüèÜ MEJOR CONFIGURACI√ìN GENERAL:\")\n# print(f\"   {best_method} con {best_overall['days']} d√≠as ‚Üí MAE {best_mae:.4f}\")\n\n# # Contar victorias\n# train7_wins = sum(1 for r in results if r['winner'] == 'TRAIN7')\n# train8_wins = sum(1 for r in results if r['winner'] == 'TRAIN8')\n\n# print(f\"\\nüìä ESTAD√çSTICAS:\")\n# print(f\"   Train7 gan√≥: {train7_wins}/4 veces\")\n# print(f\"   Train8 gan√≥: {train8_wins}/4 veces\")\n\n# if train7_wins > train8_wins:\n#     print(\"üéØ CONCLUSI√ìN: M√©todo Train7 (corta ‚Üí lag) es mejor\")\n# elif train8_wins > train7_wins:\n#     print(\"üéØ CONCLUSI√ìN: M√©todo Train8 (lag ‚Üí corta) es mejor\") \n# else:\n#     print(\"üéØ CONCLUSI√ìN: Ambos m√©todos son equivalentes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:09:58.810205Z","iopub.execute_input":"2025-09-17T20:09:58.810524Z","iopub.status.idle":"2025-09-17T20:15:33.208429Z","shell.execute_reply.started":"2025-09-17T20:09:58.810500Z","shell.execute_reply":"2025-09-17T20:15:33.206628Z"}},"outputs":[{"name":"stdout","text":"üöÄ Probando cutoffs de a uno...\n\n==================================================\nPROBANDO 90 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 90 d√≠as...\n   üìà Registros: 4,622,184 | Lags v√°lidos: 4,620,552 | MAE: 61.5811\n\nüîÑ TRAIN8 - 90 d√≠as...\n   üìà Registros: 4,622,184 | Lags v√°lidos: 4,622,184 | MAE: 64.9886\n\nüéØ RESUMEN 90 d√≠as:\n   Train7: 61.5811\n   Train8: 64.9886\n   Diferencia: +3.4075\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 100 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 100 d√≠as...\n   üìà Registros: 5,150,664 | Lags v√°lidos: 5,149,032 | MAE: 63.9248\n\nüîÑ TRAIN8 - 100 d√≠as...\n   üìà Registros: 5,150,664 | Lags v√°lidos: 5,150,664 | MAE: 63.4369\n\nüéØ RESUMEN 100 d√≠as:\n   Train7: 63.9248\n   Train8: 63.4369\n   Diferencia: -0.4878\n   Ganador: TRAIN8\n\n==================================================\nPROBANDO 110 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 110 d√≠as...\n   üìà Registros: 5,682,913 | Lags v√°lidos: 5,681,305 | MAE: 64.4976\n\nüîÑ TRAIN8 - 110 d√≠as...\n   üìà Registros: 5,682,913 | Lags v√°lidos: 5,682,913 | MAE: 65.3186\n\nüéØ RESUMEN 110 d√≠as:\n   Train7: 64.4976\n   Train8: 65.3186\n   Diferencia: +0.8210\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 120 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 120 d√≠as...\n   üìà Registros: 6,219,529 | Lags v√°lidos: 6,217,921 | MAE: 65.7664\n\nüîÑ TRAIN8 - 120 d√≠as...\n   üìà Registros: 6,219,529 | Lags v√°lidos: 6,219,529 | MAE: 66.9320\n\nüéØ RESUMEN 120 d√≠as:\n   Train7: 65.7664\n   Train8: 66.9320\n   Diferencia: +1.1656\n   Ganador: TRAIN7\n\n============================================================\nüìä RESUMEN FINAL\n============================================================\n\nD√≠as\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n-------------------------------------------------------\n90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7\n100\t63.9248\t\t63.4369\t\t-0.4878\t\tTRAIN8\n110\t64.4976\t\t65.3186\t\t+0.8210\t\tTRAIN7\n120\t65.7664\t\t66.9320\t\t+1.1656\t\tTRAIN7\n\nüèÜ MEJOR CONFIGURACI√ìN GENERAL:\n   Train7 con 90 d√≠as ‚Üí MAE 61.5811\n\nüìä ESTAD√çSTICAS:\n   Train7 gan√≥: 3/4 veces\n   Train8 gan√≥: 1/4 veces\nüéØ CONCLUSI√ìN: M√©todo Train7 (corta ‚Üí lag) es mejor\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Modelo 42\n# D√≠as\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 60\t62.6199\t\t63.9901\t\t+1.3702\t\tTRAIN7\n# 70\t66.6343\t\t62.8047\t\t-3.8296\t\tTRAIN8\n# 80\t62.7194\t\t63.1605\t\t+0.4411\t\tTRAIN7\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7\n\ndef test_single_cutoff_train7(days):\n    \"\"\"Probar UN cutoff con m√©todo train7\"\"\"\n    print(f\"\\nüîÑ TRAIN7 - {days} d√≠as...\")\n    \n    max_date = train7['datetime'].max()\n    cutoff = max_date - pd.DateOffset(days=days)\n    \n    # Cortar primero\n    sample = train7[train7['datetime'] >= cutoff].copy()\n    \n    # Agregar lag despu√©s\n    sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n    sample['fdirect_solar_radiation_lag24h'] = sample.groupby('prediction_unit_id')['fdirect_solar_radiation'].shift(24)\n    \n    # Limpiar\n    sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n    feature_cols = [col for col in sample.columns \n                   if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n    # Stats\n    valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n    total_records = len(sample)\n    \n    # Split y modelo\n    split_idx = int(len(sample) * 0.8)\n    train_data = sample.iloc[:split_idx]\n    test_data = sample.iloc[split_idx:]\n    \n    X_train = train_data[feature_cols].fillna(0)\n    y_train = train_data['target']\n    X_test = test_data[feature_cols].fillna(0)\n    y_test = test_data['target']\n    \n    # Limpiar muestras grandes\n    del sample, train_data, test_data\n    gc.collect()\n    \n    # Modelo\n    model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n                             random_state=42, n_jobs=-1, verbose=-1)\n    model.fit(X_train, y_train)\n    mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n    # Limpiar modelo\n    del model, X_train, X_test, y_train, y_test\n    gc.collect()\n    \n    print(f\"   üìà Registros: {total_records:,} | Lags v√°lidos: {valid_lags:,} | MAE: {mae:.4f}\")\n    return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\ndef test_single_cutoff_train8(days):\n    \"\"\"Probar UN cutoff con m√©todo train8\"\"\"\n    print(f\"\\nüîÑ TRAIN8 - {days} d√≠as...\")\n    \n    max_date = train8['datetime'].max()\n    cutoff = max_date - pd.DateOffset(days=days)\n    \n    # Cortar (lag ya existe)\n    sample = train8[train8['datetime'] >= cutoff].copy()\n    sample = sample.sort_values(['prediction_unit_id', 'datetime'])\n    sample = sample.dropna(subset=['target_ma_7d_lag5d'])\n    \n    feature_cols = [col for col in sample.columns \n                   if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n    \n    # Stats\n    valid_lags = sample['fdirect_solar_radiation_lag24h'].notna().sum()\n    total_records = len(sample)\n    \n    # Split y modelo\n    split_idx = int(len(sample) * 0.8)\n    train_data = sample.iloc[:split_idx]\n    test_data = sample.iloc[split_idx:]\n    \n    X_train = train_data[feature_cols].fillna(0)\n    y_train = train_data['target']\n    X_test = test_data[feature_cols].fillna(0)\n    y_test = test_data['target']\n    \n    # Limpiar muestras grandes\n    del sample, train_data, test_data\n    gc.collect()\n    \n    # Modelo\n    model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n                             random_state=42, n_jobs=-1, verbose=-1)\n    model.fit(X_train, y_train)\n    mae = mean_absolute_error(y_test, model.predict(X_test))\n    \n    # Limpiar modelo\n    del model, X_train, X_test, y_train, y_test\n    gc.collect()\n    \n    print(f\"   üìà Registros: {total_records:,} | Lags v√°lidos: {valid_lags:,} | MAE: {mae:.4f}\")\n    return {'days': days, 'mae': mae, 'records': total_records, 'valid_lags': valid_lags}\n\n# EJECUTAR DE A UNO\nprint(\"üöÄ Probando cutoffs de a uno...\")\nresults = []\n\ncutoff_days = [60, 70, 80, 90]\n\nfor days in cutoff_days:\n    print(f\"\\n{'='*50}\")\n    print(f\"PROBANDO {days} D√çAS\")\n    print(f\"{'='*50}\")\n    \n    # Train7\n    result_t7 = test_single_cutoff_train7(days)\n    \n    # Train8  \n    result_t8 = test_single_cutoff_train8(days)\n    \n    # Comparar\n    diff = result_t8['mae'] - result_t7['mae']\n    winner = \"TRAIN7\" if result_t7['mae'] < result_t8['mae'] else \"TRAIN8\"\n    \n    print(f\"\\nüéØ RESUMEN {days} d√≠as:\")\n    print(f\"   Train7: {result_t7['mae']:.4f}\")\n    print(f\"   Train8: {result_t8['mae']:.4f}\")\n    print(f\"   Diferencia: {diff:+.4f}\")\n    print(f\"   Ganador: {winner}\")\n    \n    results.append({\n        'days': days,\n        'train7_mae': result_t7['mae'],\n        'train8_mae': result_t8['mae'],\n        'difference': diff,\n        'winner': winner\n    })\n    \n    # Forzar limpieza memoria\n    gc.collect()\n\n# RESUMEN FINAL\nprint(f\"\\n{'='*60}\")\nprint(\"üìä RESUMEN FINAL\")\nprint(f\"{'='*60}\")\n\nprint(\"\\nD√≠as\\tTrain7 MAE\\tTrain8 MAE\\tDiferencia\\tGanador\")\nprint(\"-\" * 55)\nfor r in results:\n    print(f\"{r['days']}\\t{r['train7_mae']:.4f}\\t\\t{r['train8_mae']:.4f}\\t\\t{r['difference']:+.4f}\\t\\t{r['winner']}\")\n\n# Mejor configuraci√≥n general\nbest_overall = min(results, key=lambda x: min(x['train7_mae'], x['train8_mae']))\nbest_mae = min(best_overall['train7_mae'], best_overall['train8_mae'])\nbest_method = \"Train7\" if best_overall['train7_mae'] < best_overall['train8_mae'] else \"Train8\"\n\nprint(f\"\\nüèÜ MEJOR CONFIGURACI√ìN GENERAL:\")\nprint(f\"   {best_method} con {best_overall['days']} d√≠as ‚Üí MAE {best_mae:.4f}\")\n\n# Contar victorias\ntrain7_wins = sum(1 for r in results if r['winner'] == 'TRAIN7')\ntrain8_wins = sum(1 for r in results if r['winner'] == 'TRAIN8')\n\nprint(f\"\\nüìä ESTAD√çSTICAS:\")\nprint(f\"   Train7 gan√≥: {train7_wins}/4 veces\")\nprint(f\"   Train8 gan√≥: {train8_wins}/4 veces\")\n\nif train7_wins > train8_wins:\n    print(\"üéØ CONCLUSI√ìN: M√©todo Train7 (corta ‚Üí lag) es mejor\")\nelif train8_wins > train7_wins:\n    print(\"üéØ CONCLUSI√ìN: M√©todo Train8 (lag ‚Üí corta) es mejor\") \nelse:\n    print(\"üéØ CONCLUSI√ìN: Ambos m√©todos son equivalentes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:32:54.403715Z","iopub.execute_input":"2025-09-17T20:32:54.404311Z","iopub.status.idle":"2025-09-17T20:36:45.948509Z","shell.execute_reply.started":"2025-09-17T20:32:54.404280Z","shell.execute_reply":"2025-09-17T20:36:45.947556Z"}},"outputs":[{"name":"stdout","text":"üöÄ Probando cutoffs de a uno...\n\n==================================================\nPROBANDO 60 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 60 d√≠as...\n   üìà Registros: 3,069,912 | Lags v√°lidos: 3,068,304 | MAE: 62.6199\n\nüîÑ TRAIN8 - 60 d√≠as...\n   üìà Registros: 3,069,912 | Lags v√°lidos: 3,069,912 | MAE: 63.9901\n\nüéØ RESUMEN 60 d√≠as:\n   Train7: 62.6199\n   Train8: 63.9901\n   Diferencia: +1.3702\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 70 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 70 d√≠as...\n   üìà Registros: 3,585,384 | Lags v√°lidos: 3,583,752 | MAE: 66.6343\n\nüîÑ TRAIN8 - 70 d√≠as...\n   üìà Registros: 3,585,384 | Lags v√°lidos: 3,585,384 | MAE: 62.8047\n\nüéØ RESUMEN 70 d√≠as:\n   Train7: 66.6343\n   Train8: 62.8047\n   Diferencia: -3.8296\n   Ganador: TRAIN8\n\n==================================================\nPROBANDO 80 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 80 d√≠as...\n   üìà Registros: 4,103,784 | Lags v√°lidos: 4,102,152 | MAE: 62.7194\n\nüîÑ TRAIN8 - 80 d√≠as...\n   üìà Registros: 4,103,784 | Lags v√°lidos: 4,103,784 | MAE: 63.1605\n\nüéØ RESUMEN 80 d√≠as:\n   Train7: 62.7194\n   Train8: 63.1605\n   Diferencia: +0.4411\n   Ganador: TRAIN7\n\n==================================================\nPROBANDO 90 D√çAS\n==================================================\n\nüîÑ TRAIN7 - 90 d√≠as...\n   üìà Registros: 4,622,184 | Lags v√°lidos: 4,620,552 | MAE: 61.5811\n\nüîÑ TRAIN8 - 90 d√≠as...\n   üìà Registros: 4,622,184 | Lags v√°lidos: 4,622,184 | MAE: 64.9886\n\nüéØ RESUMEN 90 d√≠as:\n   Train7: 61.5811\n   Train8: 64.9886\n   Diferencia: +3.4075\n   Ganador: TRAIN7\n\n============================================================\nüìä RESUMEN FINAL\n============================================================\n\nD√≠as\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n-------------------------------------------------------\n60\t62.6199\t\t63.9901\t\t+1.3702\t\tTRAIN7\n70\t66.6343\t\t62.8047\t\t-3.8296\t\tTRAIN8\n80\t62.7194\t\t63.1605\t\t+0.4411\t\tTRAIN7\n90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7\n\nüèÜ MEJOR CONFIGURACI√ìN GENERAL:\n   Train7 con 90 d√≠as ‚Üí MAE 61.5811\n\nüìä ESTAD√çSTICAS:\n   Train7 gan√≥: 3/4 veces\n   Train8 gan√≥: 1/4 veces\nüéØ CONCLUSI√ìN: M√©todo Train7 (corta ‚Üí lag) es mejor\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"train7.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.210237Z","iopub.execute_input":"2025-09-17T20:15:33.210586Z","iopub.status.idle":"2025-09-17T20:15:33.247030Z","shell.execute_reply.started":"2025-09-17T20:15:33.210560Z","shell.execute_reply":"2025-09-17T20:15:33.246069Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n0  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n1  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n2  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n3  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n4  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n\n   data_block_id  prediction_unit_id  is_business  product_type  county  \\\n0            486                   0            0             1       0   \n1            486                   0            0             1       0   \n2            486                   0            0             1       0   \n3            486                   0            0             1       0   \n4            486                   0            0             1       0   \n\n    latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n0  59.099998  24.200001                  74.0              86.199997   \n1  59.099998  25.200001                  74.0              86.199997   \n2  59.400002  22.700001                  74.0              86.199997   \n3  59.400002  23.200001                  74.0              86.199997   \n4  59.400002  23.700001                  74.0              86.199997   \n\n   euros_per_mwh  eic_count  installed_capacity  ftemperature  fdewpoint  \\\n0           3.29      417.0         4001.080078      2.458154   1.860742   \n1           3.29      417.0         4001.080078      1.464502   0.857690   \n2           3.29      417.0         4001.080078      5.330591   3.928491   \n3           3.29      417.0         4001.080078      4.218042   2.536279   \n4           3.29      417.0         4001.080078      3.586084   2.155054   \n\n   fcloudcover_high  fcloudcover_low  fcloudcover_mid  fcloudcover_total  \\\n0          0.989075              1.0         1.000000           0.999996   \n1          0.897797              1.0         1.000000           0.999996   \n2          0.064026              1.0         0.054596           0.999996   \n3          0.132874              1.0         0.371002           0.999996   \n4          0.618713              1.0         1.000000           0.999996   \n\n   f10_metre_u_wind_component  f10_metre_v_wind_component  \\\n0                    0.320052                    8.630277   \n1                    0.415511                    7.309232   \n2                    4.160628                   12.238919   \n3                    1.823714                   13.567777   \n4                    0.730208                   12.648099   \n\n   fdirect_solar_radiation  fsurface_solar_radiation_downwards  fsnowfall  \\\n0                -0.008889                            5.534878   0.000023   \n1                 0.008889                           15.983768   0.000063   \n2                 0.008889                            5.841545   0.000000   \n3                 0.000000                            5.081545   0.000000   \n4                 0.000000                            3.925990   0.000000   \n\n   ftotal_precipitation  weather_forecast_hour  is_consumption  day_of_week  \\\n0              0.000508                   33.0               0            5   \n1              0.000489                   33.0               0            5   \n2              0.000222                   33.0               0            5   \n3              0.000197                   33.0               0            5   \n4              0.000252                   33.0               0            5   \n\n   month  is_daylight  weekend  capacity_per_obs  production_obs_ratio  \\\n0     12            0        1          0.016344                   0.5   \n1     12            0        1          0.016344                   0.5   \n2     12            0        1          0.016344                   0.5   \n3     12            0        1          0.016344                   0.5   \n4     12            0        1          0.016344                   0.5   \n\n   consumption_obs_ratio  capacity_per_capita  \\\n0                    0.5             9.594916   \n1                    0.5             9.594916   \n2                    0.5             9.594916   \n3                    0.5             9.594916   \n4                    0.5             9.594916   \n\n   electricity_price_volatility_7d  gas_price_volatility_7d  \\\n0                              0.0                      0.0   \n1                              0.0                      0.0   \n2                              0.0                      0.0   \n3                              0.0                      0.0   \n4                              0.0                      0.0   \n\n   target_ma_7d_lag5d  \n0                 NaN  \n1                 NaN  \n2                 NaN  \n3                 NaN  \n4                 NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>fdewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>fdirect_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>fsnowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n      <th>capacity_per_obs</th>\n      <th>production_obs_ratio</th>\n      <th>consumption_obs_ratio</th>\n      <th>capacity_per_capita</th>\n      <th>electricity_price_volatility_7d</th>\n      <th>gas_price_volatility_7d</th>\n      <th>target_ma_7d_lag5d</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>2.458154</td>\n      <td>1.860742</td>\n      <td>0.989075</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.320052</td>\n      <td>8.630277</td>\n      <td>-0.008889</td>\n      <td>5.534878</td>\n      <td>0.000023</td>\n      <td>0.000508</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>1.464502</td>\n      <td>0.857690</td>\n      <td>0.897797</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.415511</td>\n      <td>7.309232</td>\n      <td>0.008889</td>\n      <td>15.983768</td>\n      <td>0.000063</td>\n      <td>0.000489</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>5.330591</td>\n      <td>3.928491</td>\n      <td>0.064026</td>\n      <td>1.0</td>\n      <td>0.054596</td>\n      <td>0.999996</td>\n      <td>4.160628</td>\n      <td>12.238919</td>\n      <td>0.008889</td>\n      <td>5.841545</td>\n      <td>0.000000</td>\n      <td>0.000222</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>4.218042</td>\n      <td>2.536279</td>\n      <td>0.132874</td>\n      <td>1.0</td>\n      <td>0.371002</td>\n      <td>0.999996</td>\n      <td>1.823714</td>\n      <td>13.567777</td>\n      <td>0.000000</td>\n      <td>5.081545</td>\n      <td>0.000000</td>\n      <td>0.000197</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>3.586084</td>\n      <td>2.155054</td>\n      <td>0.618713</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.730208</td>\n      <td>12.648099</td>\n      <td>0.000000</td>\n      <td>3.925990</td>\n      <td>0.000000</td>\n      <td>0.000252</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"train8.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.248221Z","iopub.execute_input":"2025-09-17T20:15:33.248568Z","iopub.status.idle":"2025-09-17T20:15:33.294053Z","shell.execute_reply.started":"2025-09-17T20:15:33.248533Z","shell.execute_reply":"2025-09-17T20:15:33.292447Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n0  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n1  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n2  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n3  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n4  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n\n   data_block_id  prediction_unit_id  is_business  product_type  county  \\\n0            486                   0            0             1       0   \n1            486                   0            0             1       0   \n2            486                   0            0             1       0   \n3            486                   0            0             1       0   \n4            486                   0            0             1       0   \n\n    latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n0  59.099998  24.200001                  74.0              86.199997   \n1  59.099998  25.200001                  74.0              86.199997   \n2  59.400002  22.700001                  74.0              86.199997   \n3  59.400002  23.200001                  74.0              86.199997   \n4  59.400002  23.700001                  74.0              86.199997   \n\n   euros_per_mwh  eic_count  installed_capacity  ftemperature  fdewpoint  \\\n0           3.29      417.0         4001.080078      2.458154   1.860742   \n1           3.29      417.0         4001.080078      1.464502   0.857690   \n2           3.29      417.0         4001.080078      5.330591   3.928491   \n3           3.29      417.0         4001.080078      4.218042   2.536279   \n4           3.29      417.0         4001.080078      3.586084   2.155054   \n\n   fcloudcover_high  fcloudcover_low  fcloudcover_mid  fcloudcover_total  \\\n0          0.989075              1.0         1.000000           0.999996   \n1          0.897797              1.0         1.000000           0.999996   \n2          0.064026              1.0         0.054596           0.999996   \n3          0.132874              1.0         0.371002           0.999996   \n4          0.618713              1.0         1.000000           0.999996   \n\n   f10_metre_u_wind_component  f10_metre_v_wind_component  \\\n0                    0.320052                    8.630277   \n1                    0.415511                    7.309232   \n2                    4.160628                   12.238919   \n3                    1.823714                   13.567777   \n4                    0.730208                   12.648099   \n\n   fdirect_solar_radiation  fsurface_solar_radiation_downwards  fsnowfall  \\\n0                -0.008889                            5.534878   0.000023   \n1                 0.008889                           15.983768   0.000063   \n2                 0.008889                            5.841545   0.000000   \n3                 0.000000                            5.081545   0.000000   \n4                 0.000000                            3.925990   0.000000   \n\n   ftotal_precipitation  weather_forecast_hour  is_consumption  day_of_week  \\\n0              0.000508                   33.0               0            5   \n1              0.000489                   33.0               0            5   \n2              0.000222                   33.0               0            5   \n3              0.000197                   33.0               0            5   \n4              0.000252                   33.0               0            5   \n\n   month  is_daylight  weekend  capacity_per_obs  production_obs_ratio  \\\n0     12            0        1          0.016344                   0.5   \n1     12            0        1          0.016344                   0.5   \n2     12            0        1          0.016344                   0.5   \n3     12            0        1          0.016344                   0.5   \n4     12            0        1          0.016344                   0.5   \n\n   consumption_obs_ratio  capacity_per_capita  \\\n0                    0.5             9.594916   \n1                    0.5             9.594916   \n2                    0.5             9.594916   \n3                    0.5             9.594916   \n4                    0.5             9.594916   \n\n   electricity_price_volatility_7d  gas_price_volatility_7d  \\\n0                              0.0                      0.0   \n1                              0.0                      0.0   \n2                              0.0                      0.0   \n3                              0.0                      0.0   \n4                              0.0                      0.0   \n\n   target_ma_7d_lag5d  fdirect_solar_radiation_lag24h  \n0                 NaN                             NaN  \n1                 NaN                             NaN  \n2                 NaN                             NaN  \n3                 NaN                             NaN  \n4                 NaN                             NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>fdewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>fdirect_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>fsnowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n      <th>capacity_per_obs</th>\n      <th>production_obs_ratio</th>\n      <th>consumption_obs_ratio</th>\n      <th>capacity_per_capita</th>\n      <th>electricity_price_volatility_7d</th>\n      <th>gas_price_volatility_7d</th>\n      <th>target_ma_7d_lag5d</th>\n      <th>fdirect_solar_radiation_lag24h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>2.458154</td>\n      <td>1.860742</td>\n      <td>0.989075</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.320052</td>\n      <td>8.630277</td>\n      <td>-0.008889</td>\n      <td>5.534878</td>\n      <td>0.000023</td>\n      <td>0.000508</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>1.464502</td>\n      <td>0.857690</td>\n      <td>0.897797</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.415511</td>\n      <td>7.309232</td>\n      <td>0.008889</td>\n      <td>15.983768</td>\n      <td>0.000063</td>\n      <td>0.000489</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>5.330591</td>\n      <td>3.928491</td>\n      <td>0.064026</td>\n      <td>1.0</td>\n      <td>0.054596</td>\n      <td>0.999996</td>\n      <td>4.160628</td>\n      <td>12.238919</td>\n      <td>0.008889</td>\n      <td>5.841545</td>\n      <td>0.000000</td>\n      <td>0.000222</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>4.218042</td>\n      <td>2.536279</td>\n      <td>0.132874</td>\n      <td>1.0</td>\n      <td>0.371002</td>\n      <td>0.999996</td>\n      <td>1.823714</td>\n      <td>13.567777</td>\n      <td>0.000000</td>\n      <td>5.081545</td>\n      <td>0.000000</td>\n      <td>0.000197</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>3.586084</td>\n      <td>2.155054</td>\n      <td>0.618713</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.999996</td>\n      <td>0.730208</td>\n      <td>12.648099</td>\n      <td>0.000000</td>\n      <td>3.925990</td>\n      <td>0.000000</td>\n      <td>0.000252</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"#Mae puesto n1 kaggle 52.3090\n#1er primer modelo xgb sobre train5 haciendo split 80/20 de train y test \n#Mae = 107.9896\n#sin lag sin feature, sin nada , solo haciendo el merge\n\n\n#agrego day of week y month para todos los siguientes\n\n#2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# estas variables de lags las quito\n\n#agrego is_daylight binaria\n\n#4to modelo lgb 14.2% y sobre esto split 80/20 de train y test  \n#de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses 14.2% aprox\n# se que no deberia ser comparable al usar de base distinto , pero parte de aca ahora\n#Mae = 86.6747\n\n#agrego weekend binaria\n\n#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n\n#para todos los demas modelos ya tienen incluido\n#day of week\n#month\n#is_daylight\n#weekend\n\n#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo la saco por ahora a is_active_hours\n\n#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que  la saco a is_sleeping_hours\n\n#empiezo a probar con variables\n##Energ√©ticas:\n\n# Ratio production/consumption por prediction_unit_id (hist√≥rico) # no se pudo probar\n# Capacidad instalada per capita (installed_capacity / eic_count) # si mejora el modelo\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n\n#9no modelo lgb  mantengo las 3 anteriores y prueba una nueva Ratio production/cn\n# no optimizado no pudo correr\n\n#10mo modelo lgb mantengo las 3 anteriores y pruebo  Capacidad instalada per capita (installed_capacity / eic_count)\n#Mae = 72.3572 mejora el modelo \n\n#para los demas modelos sigo usando las siguientes variables\n#capacity_per_obs\n#production_obs_ratio\n#consumption_obs_ratio\n#capacity_per_capita\n\n#11vo modelo lgb con esto #Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)#\n#Mae anterior = 72.3572\n#Mae con eficiencia de paneles = 85.0309 empeora , pruebo con lag de esto  y sino no la uso por ahora\n\n\n#12vo modelo lgb , lo anterior pero ahora efficiency panel es log , lo saco\n# ‚úÖ MAE con panel efficiency LOG: 85.3200\n# üìä MAE anterior: 72.3572\n\n\n# #13vo modelo probando volatilidad de precio de electricidad 7 dias\n# # ‚úÖ MAE con electricity price volatility: 73.2246 si , falta probar mas versiones y gas\n# # üìä MAE anterior: 72.3572\n\n#14vo modelo ademas de electricity_volatility pruebo con gas\n# ‚úÖ MAE con electricity + gas price volatility: 70.6517 se queda\n# üìä MAE anterior: 73.2246\n\n#creo train6 de 150 dias para agregar\n#capacity_per_obs\n#production_obs_ratio\n#consumption_obs_ratio\n#capacity_per_capita\n#elect price volatility\n#gas price volatility\n\n#15vo probar gas y/o electricity de 30 dias , agrega mucho ruido no sirve\n\n# 16vo modelo Lag 24h de target por prediction_unit_id , no va\n#‚úÖ MAE con lag 24h (90 d√≠as): 104.3003\n#üìä MAE anterior: 70.6517\n\n# 17vo modelo Lag 48h de target por prediction_unit_id , no va\n#‚úÖ MAE con lag 48h (90 d√≠as): 111.3501\n#üìä MAE anterior: 70.6517\n\n# # Modelo #18: Cyclical encoding de hora (hour_sin, hour_cos)\n# ‚úÖ MAE con hour_sin/cos: 122.4835 # algo esta pasado ver como corregirlo\n# üìä MAE anterior: 70.6517\n\n# Modelo #19: Cyclical encoding de semana (week_sin, week_cos)\n#‚úÖ MAE con week_sin/cos: 118.3710 los temporales , de la forma que los planteo no estan funcionando\n#üìä MAE anterior: 70.6517\n\n# Modelo #20: Cambios d√≠a a d√≠a (delta vs d√≠a anterior) 'temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation'\n# ‚úÖ MAE con deltas weather 24h: 124.1003 algo mal estoy haciendo o el overfit es grande por 90dias ,capaz con mas dias menos overfit\n# üìä MAE anterior: 70.6517\n\n# Modelo #21: Media m√≥vil de target por prediction_unit_id\n# ‚úÖ MAE con media m√≥vil 7d: 56.1320 MMM data leakage por como estan los datos , veo como lo cambio\n# üìä MAE anterior: 70.6517\n\n# Modelo #22: Media m√≥vil de target por prediction_unit_id lag 5 dias\n# ‚úÖ MAE con media m√≥vil 7d (lag 5d): 61.2636\n# üìä MAE anterior: 70.6517\n\n# Modelo #23: Media m√≥vil de target por prediction_unit_id lag 5 dias  \n# voy a probar con una sola de las 2 y tambien probar de 5 dias para ser consistente con el lag\n#‚úÖ MAE con ambas media m√≥vil 7d y 3d (lag 5d): 64.0943\n#üìä MAE anterior: 61.2636\n\n \n# Modelo #24: Media m√≥vil de target por prediction_unit_id lag 5 dias\n#‚úÖ MAE con media m√≥vil 3d (lag 5d): 63.5897 # sigue siendo mejor solo la de 7 dias\n#üìä MAE anterior: 61.2636\n\n# Modelo #25: Media m√≥vil de target por prediction_unit_id lag 5 dias\n#‚úÖ MAE con media m√≥vil 5d (lag 5d): 71.2301 # sigue siendo mejor solo la de 7 dias\n#üìä MAE anterior: 61.2636\n\n#creo train7 que contiene la media movil de 7d con lag de 5d\n\n# Modelo #26 (deberia haber sido antes) forecast Weather vs historical Weather (historical no esta en test)\n# teniendo en cuenta varias cosas esto deberia haber sido hecho antes , tanto por memoria/optimizacion/forma prolijas de trabajar y etc\n# mismo el pipeline deberia haber sido hecho solo con info de los ultimos 6 meses para entrenar mas rapido y no cortarlo despues\n# pese a los varios e importantes errores que tuve sigo probando unas cosas mas en este dataset\n# agrego esto a cuando creo train7 y sigo ese como base\n#‚úÖ MAE sin weather hist√≥rico: 62.9796\n#üìä MAE con weather hist√≥rico: 61.2636\n\n# Modelo #27: Weather lags (variables solares) - 24h lag\n# ‚úÖ MAE con weather lags 24h: 64.9544 #prueba de 5 a la vez , no se si alguna reduce y las otras son ruido , paso a probar de a 1\n# üìä MAE anterior: 62.9796\n\n# Modelo #28: Weather lag 24h - solo ftemperature\n# ‚úÖ MAE con ftemperature lag 24h: 63.2238 # nop , pruebo 48h y sigo con la siguiente\n# üìä MAE anterior: 62.9796\n\n# # Modelo #29: Weather lag 48h - solo ftemperature\n# # ‚úÖ MAE con ftemperature lag 48h: 62.3245 #nop , pruebo los demas , igual tendria que hacer una funcion que haga esto con los otras variables \n# #  y en 24h y 48h\n# # üìä MAE anterior: 62.9796\n\n# Modelo #30\n# #üéâ Variables que MEJORAN (ordenadas por MAE):\n#    fdirect_solar_radiation_lag24h: 61.5811 (-1.3985)\n#     ftemperature_lag48h y: 62.3245 (-0.6551)\n#    fcloudcover_total_lag48h: 62.3520 (-0.6276)\n\n#la verdad es que deberia automatizar esto como #mlxtend.feature_selection.SequentialFeatureSelector\n#cuando vaya a probar las otras variables que no sean solares lo pruebo\n\n# Modelo #31 probando las 3 de arriba juntas ,empeoran el modelo\n# ‚úÖ MAE con weather lags 24 y 48h: 67.3127\n# üìä MAE anterior: 62.9796\n\n# Modelo #32 agrego fdirect_solar_radiation_lag24h\n# ‚úÖ MAE con weather lags 24h : 61.5811\n# üìä MAE anterior: 62.9796\n\n# Modelo #33 pruebo ftemperature_lag48h y fcloudcover_total_lag48h\n# ‚úÖ MAE con weather lags 24 y 48h: 62.3582\n# üìä MAE anterior: 62.9796\n\n# Modelo #34 pruebo fdirect_solar_radiation_lag24h y ftemperature_lag48h\n# ‚úÖ MAE con weather lags 24 y 48h: 64.1737\n# üìä MAE anterior: 62.9796\n\n# Modelo #35 pruebo fdirect_solar_radiation_lag24h y fcloudcover_total_lag48h me quedo conn el modelo #32\n# ‚úÖ MAE con weather lags 24 y 48h: 62.3281\n# üìä MAE anterior: 62.9796\n \n# agrego fdirect_solar_radiation_lag24h a train7 pasa a ser train8 \n\n\n# Modelo #36 \n# fdewpoint_lag24h: 64.4357 (+2.8546) ‚ùå EMPEORA\n# fdewpoint_lag48h: 67.4056 (+5.8245) ‚ùå EMPEORA\n# fcloudcover_high_lag24h: 66.1002 (+4.5191) ‚ùå EMPEORA\n# fcloudcover_high_lag48h: 66.3328 (+4.7517) ‚ùå EMPEORA\n# fcloudcover_low_lag24h: 62.4589 (+0.8778) ‚ùå EMPEORA\n# fcloudcover_low_lag48h: 66.0350 (+4.4539) ‚ùå EMPEORA\n# fcloudcover_mid_lag24h: 64.7723 (+3.1912) ‚ùå EMPEORA\n# fcloudcover_mid_lag48h: 67.0185 (+5.4374) ‚ùå EMPEORA\n# f10_metre_u_wind_component_lag24h: 63.2744 (+1.6933) ‚ùå EMPEORA\n# f10_metre_u_wind_component_lag48h: 64.4923 (+2.9112) ‚ùå EMPEORA\n# f10_metre_v_wind_component_lag24h: 68.0118 (+6.4307) ‚ùå EMPEORA\n# f10_metre_v_wind_component_lag48h: 64.3495 (+2.7684) ‚ùå EMPEORA\n# ftotal_precipitation_lag24h: 64.6852 (+3.1041) ‚ùå EMPEORA\n# ftotal_precipitation_lag48h: 67.5245 (+5.9434) ‚ùå EMPEORA\n\n# Modelo #37\n# # temp_solar_interaction: 63.5858 (+2.0047) ‚ùå EMPEORA\n# # cloud_solar_ratio: 64.5474 (+2.9663) ‚ùå EMPEORA\n# # wind_magnitude: 66.2764 (+4.6953) ‚ùå EMPEORA\n# # temp_dewpoint_diff: 65.7131 (+4.1320) ‚ùå EMPEORA\n# # surface_direct_ratio: 65.2653 (+3.6842) ‚ùå EMPEORA\n# # ftemperature_roll3h: 64.2013 (+2.6202) ‚ùå EMPEORA\n# # ftemperature_roll6h: 64.3444 (+2.7633) ‚ùå EMPEORA\n# # fdirect_solar_radiation_roll3h: 64.2987 (+2.7176) ‚ùå EMPEORA\n# # fdirect_solar_radiation_roll6h: 66.8305 (+5.2494) ‚ùå EMPEORA\n# # fcloudcover_total_roll3h: 64.4380 (+2.8569) ‚ùå EMPEORA\n# # fcloudcover_total_roll6h: 65.9477 (+4.3666) ‚ùå EMPEORA\n\n# # Modelo #38 la forma de calculo tiene filtracion de datos y aun asi no funciona\n# # Clustering de prediction_units por comportamiento similar\n# # cluster: 64.9886 (+3.4075) ‚ùå EMPEORA\n# # cluster_hour_avg_target: 61.8005 (+0.2194) ‚ùå EMPEORA\n# # üî• Probando ambas cluster features juntas...\n# # cluster + cluster_hour_avg_target: 62.4489 (+0.8678) ‚ùå EMPEORA\n\n# Modelo #39 long_lag_features = ['fdirect_solar_radiation_lag168h', 'fdirect_solar_radiation_lag336h', 'ftemperature_lag168h']\n# fdirect_solar_radiation_lag168h: 65.1855 (+3.6044) ‚ùå EMPEORA\n# fdirect_solar_radiation_lag336h: 65.7386 (+4.1575) ‚ùå EMPEORA\n# ftemperature_lag168h: 63.7261 (+2.1450) ‚ùå EMPEORA\n\n\n# Modelo #40 fdirect_solar_radiation la problematica , train8 tiene 150dias y luego corta a 90dias train8 tiene incorporada la variable\n#                                                      train7 corta primero a 90dias y luego calcula , incorpora aca la variable\n#‚úÖ MAE: 64.9886 con train8\n# MAE: 62.9796 con train7\n\n\n# Modelo #41 pruebas con distintios dias ,parace haber concept drift\n\n# D√≠as\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7 #modelo 31\n# 100\t63.9248\t\t63.4369\t\t-0.4878\t\tTRAIN8\n# 110\t64.4976\t\t65.3186\t\t+0.8210\t\tTRAIN7\n# 120\t65.7664\t\t66.9320\t\t+1.1656\t\tTRAIN7\n\n# Modelo 42 no ayuda y puede ser un cambio en los patrones subyacentes\n# D√≠as\tTrain7 MAE\tTrain8 MAE\tDiferencia\tGanador\n# -------------------------------------------------------\n# 60\t62.6199\t\t63.9901\t\t+1.3702\t\tTRAIN7\n# 70\t66.6343\t\t62.8047\t\t-3.8296\t\tTRAIN8\n# 80\t62.7194\t\t63.1605\t\t+0.4411\t\tTRAIN7\n# 90\t61.5811\t\t64.9886\t\t+3.4075\t\tTRAIN7 #modelo 31","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.295270Z","iopub.execute_input":"2025-09-17T20:15:33.295656Z","iopub.status.idle":"2025-09-17T20:15:33.309513Z","shell.execute_reply.started":"2025-09-17T20:15:33.295628Z","shell.execute_reply":"2025-09-17T20:15:33.307981Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Ratio production/consumption por prediction_unit_id (hist√≥rico) #no se pudo probar\n# Capacidad instalada per capita (installed_capacity / eic_count) # si mejora el modelo\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False) # empeora el modelo\n# Volatilidad de precios (rolling std de electricity/gas prices) # mejor poco 1 cosa falta probar mas\n# Volatilidad individual + combinada\n# electricity_price_volatility_7d = rolling(7).std() # si funciona bien\n# gas_price_volatility_7d = rolling(7).std() # si funciona bien\n\n\n\n##Temporales:\n\n# Lag de consumo/producci√≥n del mismo prediction_unit_id (24h, 48h, 168h)# empeoran el modelo\n# hour_sin y hour_cos # no va\n# week_sin, week_cos # no va\n# Media m√≥vil de target por prediction_unit_id # \n# Cambios d√≠a a d√≠a (delta vs d√≠a anterior) ['temperature', 'euros_per_mwh', 'lowest_price_per_mwh', 'direct_solar_radiation']# no funciono\n# Media m√≥vil de target por prediction_unit_id lag 5 dias si y muy bien\n\n##Weather engineering:\n\n# weather lag #['ftemperature', 'fdirect_solar_radiation', 'fsurface_solar_radiation_downwards', \n              #'fcloudcover_total', 'fsnowfall'] pruebo estas primero con 24h y 48h\n# √çndice de confort t√©rmico (combinando temp + humidity) # nop\n# Potencial solar real (direct + diffuse radiation ajustado por cloudcover) #nop\n# Diferencia forecast vs historical weather (para medir accuracy del forecast) # lo elimine porque el historico no esta en el test set\n\n##Segmentaci√≥n:\n\n# Clustering de prediction_units por comportamiento similar\n# Ratios county-level (agregaciones por county vs individual)\n\n\n\n\n#para la red neuronal probar lo siguiente\n#MLP\n#DeepAR\n#TFT (temporal fusion transformer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.311292Z","iopub.execute_input":"2025-09-17T20:15:33.311729Z","iopub.status.idle":"2025-09-17T20:15:33.359739Z","shell.execute_reply.started":"2025-09-17T20:15:33.311698Z","shell.execute_reply":"2025-09-17T20:15:33.357652Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"#test5 = run_test_pipeline(\n#    test, gas_prices_t, electricity_prices_t, client_t, \n#    forecast_weather_t, weather_station\n#)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T20:15:33.361023Z","iopub.execute_input":"2025-09-17T20:15:33.361454Z","iopub.status.idle":"2025-09-17T20:15:33.391137Z","shell.execute_reply.started":"2025-09-17T20:15:33.361416Z","shell.execute_reply":"2025-09-17T20:15:33.389814Z"}},"outputs":[],"execution_count":60}]}