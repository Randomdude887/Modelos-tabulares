{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":100725,"databundleVersionId":12103520,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.kaggle.com/competitions/premiumpulse-risk-modeling/overview\n\n# === MANIPULACI√ìN DE DATOS ===\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# === VISUALIZACI√ìN ===\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# === PREPROCESAMIENTO ===\nfrom sklearn.preprocessing import (\n    StandardScaler, MinMaxScaler, RobustScaler,\n    LabelEncoder, OneHotEncoder, OrdinalEncoder,\n    PowerTransformer, QuantileTransformer\n)\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# === DIVISI√ìN DE DATOS ===\nfrom sklearn.model_selection import (\n    train_test_split, cross_val_score, cross_validate,\n    StratifiedKFold, KFold, GridSearchCV, RandomizedSearchCV,\n    validation_curve, learning_curve\n)\n\n# === MODELOS DE CLASIFICACI√ìN ===\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier, ExtraTreesClassifier,\n    GradientBoostingClassifier, AdaBoostClassifier,\n    VotingClassifier, BaggingClassifier\n)\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\n# === MODELOS DE REGRESI√ìN ===\nfrom sklearn.linear_model import (\n    LinearRegression, Ridge, Lasso, ElasticNet,\n    SGDRegressor, BayesianRidge, HuberRegressor\n)\nfrom sklearn.ensemble import (\n    RandomForestRegressor, ExtraTreesRegressor,\n    GradientBoostingRegressor, AdaBoostRegressor,\n    VotingRegressor, BaggingRegressor\n)\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# === CLUSTERING ===\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\n\n# === REDUCCI√ìN DE DIMENSIONALIDAD ===\nfrom sklearn.decomposition import PCA, TruncatedSVD, FastICA\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_selection import (\n    SelectKBest, SelectFromModel, RFE, RFECV,\n    chi2, f_classif, f_regression, mutual_info_classif\n)\n\n# === M√âTRICAS ===\nfrom sklearn.metrics import (\n    # Clasificaci√≥n\n    accuracy_score, precision_score, recall_score, f1_score,\n    classification_report, confusion_matrix, roc_auc_score,\n    roc_curve, precision_recall_curve, auc, log_loss,\n \n    # Regresi√≥n\n    mean_squared_error, mean_absolute_error, r2_score,\n    mean_squared_log_error, mean_absolute_percentage_error,\n    \n    # Clustering\n    silhouette_score, adjusted_rand_score, calinski_harabasz_score\n)\n\n# === GRADIENT BOOSTING \n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# === AN√ÅLISIS ESTAD√çSTICO ===\n\nimport scipy.stats as stats\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# === TIME SERIES ===\n\nfrom prophet import Prophet\n\n# === OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS ===\n\nimport optuna\n\n# === UTILIDADES ===\nimport os\nimport sys\nimport json\nimport pickle\nimport joblib\nfrom datetime import datetime, timedelta\nimport itertools\nfrom collections import Counter\nimport gc\n\n# === CONFIGURACI√ìN DE PANDAS ===\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.4f}'.format)\npd.set_option('display.width', None)\n\n# === CONFIGURACI√ìN DE MATPLOTLIB ===\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\n# === CONFIGURACI√ìN DE SEABORN ===\nsns.set_palette(\"husl\")\nsns.set_style(\"whitegrid\")\n\n# === CONFIGURACI√ìN DE NUMPY ===\nnp.random.seed(1722)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/premiumpulse-risk-modeling/train.csv')\ntest = pd.read_csv('/kaggle/input/premiumpulse-risk-modeling/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lo primero sera ver como correr rapido un RandomForest y ver como esta la cosa\n#variable a predecir \"Premium Amount\"\n#ver si sacar \"id\" y la agrego al final\n#Pasar a logaritmo el \"Annual Income\" y tal vez el \"Credit Score\"\n#Ver los NaN cuantos son y como proceder\n#Variables categoricas:\"Marital Status\",\"Location\",\"Property Type\"\n#Variables categoricas ordinales: \"Education Level\",\"Occupation\",\"Policy Type\",\"Customer Feedback\",\"Exercise Frequency\"\n#Analisis de correlaciones\n#separar Policy Start Date en varias columnas a√±o/mes/dia/hora ver si es relevante el dia como lun/mar/mier o por semanas\n#ver si las variables numericas parecieran seguir algun tipo de distribucion normal/poisson/uniforme etc..\n# Variable \"Gender\" por un tema de discriminacion la saco o no ? , espero a ver la importancia en un random forest para decidir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.describe(include=\"all\")# los valores entre test y train parecieran salir de una misma poblacion por como estan distribuidos , parece sintetico/ficticio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train.isnull().sum()\n#train[\"Age\"].value_counts() parece seguir una distribucion uniforme entre 18 a 64 a√±os ,\n# voy a reemplazar los NaN en la edad con la mediana , pero voy a crear una columna binaria para que el modelo sepa si habia NaN en la Edad\n# es por si en esas edad en particular el cambio que hice perjudica al modelo , ya sea porque asigne mas riesgo a esa edad o saque conclusiones\n# que no son correctas debido a mi manipulacion y el hecho de que hace ruido que la edad este distribuida uniformemente entre todos los rangos\n\ntrain1 = train.copy()\ntest1 = test.copy()\n\ntrain1[\"Age_NaN\"] = train[\"Age\"].isna().astype(int)\ntest1[\"Age_NaN\"] = test[\"Age\"].isna().astype(int)\n\n\ntrain1[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n\n#train[\"Age\"].value_counts()\n\ntest1[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].median())\n\n#test1[\"Age\"].value_counts()\n\ntrain1[\"Age_NaN\"] = train[\"Age\"].isna().astype(int)\ntest1[\"Age_NaN\"] = test[\"Age\"].isna().astype(int)\n\n#train1.head(10)\n#test1.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1.head(10)\ntrain1[\"Age_NaN\"].value_counts()\n#test1[\"Age_NaN\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1[\"An_Income_NaN\"] = train[\"Annual Income\"].isna().astype(int)\ntest1[\"An_Income_NaN\"] = test[\"Annual Income\"].isna().astype(int)\n\n\ntrain1[\"Annual Income\"] = train[\"Annual Income\"].fillna(train[\"Annual Income\"].median())\ntest1[\"Annual Income\"] = test[\"Annual Income\"].fillna(test[\"Annual Income\"].median())\n\n\ntrain1[\"Log Annual Income\"] = np.log(train1[\"Annual Income\"])\ntest1[\"Log Annual Income\"] = np.log(test1[\"Annual Income\"])\n\ntrain1 = train1.drop(\"Annual Income\", axis = 1)\ntest1 = test1.drop(\"Annual Income\", axis = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1.head(10)\ntrain1[\"An_Income_NaN\"].value_counts()\ntest1[\"An_Income_NaN\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#para el \"Credit Score\" le hago categorias , uso Ordinal Scoring\n\ndef categorize_Credit_Score(score):\n    if pd.isna(score):\n        return 'X'  # Sin historial\n    elif score >= 800:\n        return 'A'  # Excepcional (800-850)\n    elif score >= 750:\n        return 'B'  # Muy Bueno\n    elif score >= 700:\n        return 'C'  # Bueno\n    elif score >= 650:\n        return 'D'  # Regular+\n    elif score >= 600:\n        return 'E'  # Regular\n    elif score >= 550:\n        return 'F'  # Malo+\n    elif score >= 500:\n        return 'G'  # Malo\n    elif score >= 400:\n        return 'H'  # Muy Malo\n    else:  # 300-399\n        return 'I'  # P√©simo\n\ntrain1['Credit_Grade'] = train1['Credit Score'].apply(categorize_Credit_Score)\ntest1['Credit_Grade'] = test1['Credit Score'].apply(categorize_Credit_Score)\n\n\ngrade_map = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'H':2, 'I':1, 'X':0}\ntrain1['Credit Score'] = train1['Credit_Grade'].map(grade_map)\ntest1['Credit Score'] = test1['Credit_Grade'].map(grade_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1 = train1.drop(\"Credit_Grade\", axis = 1)\ntest1 = test1.drop(\"Credit_Grade\", axis = 1)\n\ntest1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train2 = train1.copy()\ntest2 = test1.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Reemplazo NaN por Unknown\n\ntrain2['Marital Status'] = train2['Marital Status'].fillna('Unknown')\ntest2['Marital Status'] = test2['Marital Status'].fillna('Unknown')\n\ntrain2.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(7,2)) \nplt.title(\"titulo\")\nsns.boxplot(x=train2[\"Number of Dependents\"])\nprint(train2[\"Number of Dependents\"].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15,3)) \nplt.title(\"Number of Dependents\")\nsns.histplot(train2[\"Number of Dependents\"]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#para 'Number of Dependents' voy a poner -1 en los NaN y crear una columna que lo aclare\n\ntrain2['Number of Dependents'] = train2['Number of Dependents'].fillna(-1)\ntest2['Number of Dependents'] = test2['Number of Dependents'].fillna(-1)\n\n\ntrain2['Dependents_NaN'] = (train2['Number of Dependents'] == -1).astype(int)\ntest2['Dependents_NaN'] = (test2['Number of Dependents'] == -1).astype(int)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test2.head()\ntrain2['Dependents_NaN'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train2.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# para Occupation reemplazo NaN por Unknown\n\ntrain2['Occupation'] = train2['Occupation'].fillna('Unknown')\ntest2['Occupation'] = test2['Occupation'].fillna('Unknown')\n\ntest2.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train2.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Boxplot\nax1.boxplot(train2['Health Score'].dropna())\nax1.set_title('Health Score - Boxplot')\n\n# Histograma\nax2.hist(train2['Health Score'].dropna(), bins=20, edgecolor='black')\nax2.set_title('Health Score - Histogram')\n\nplt.tight_layout()\nplt.show()\n\nprint(train2['Health Score'].describe())\nprint(f\"NaN: {train2['Health Score'].isna().sum()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Crear rangos y columna nueva para los NaN\n\n# Binning\ntrain2['Health_Score_Binned'] = pd.cut(train2['Health Score'], bins=8, labels=range(8)).astype(float)\ntrain2['Health_Score_Binned'] = train2['Health_Score_Binned'].fillna(-1)\ntest2['Health_Score_Binned'] = pd.cut(test2['Health Score'], bins=8, labels=range(8)).astype(float)\ntest2['Health_Score_Binned'] = test2['Health_Score_Binned'].fillna(-1)\n\n\n# Indicator\ntrain2['Health_NaN'] = (train2['Health_Score_Binned'] == -1).astype(int)\ntest2['Health_NaN'] = (test2['Health_Score_Binned'] == -1).astype(int)\n\ntrain2 = train2.drop(\"Health Score\", axis = 1)\ntest2 = test2.drop(\"Health Score\", axis = 1)\n\n\ntrain2.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Boxplot\nax1.boxplot(train2['Previous Claims'].dropna())\nax1.set_title('Previous Claims - Boxplot')\n\n# Histograma\nax2.hist(train2['Previous Claims'].dropna(), bins=2, edgecolor='black')\nax2.set_title('Previous Claims - Histogram')\n\nplt.tight_layout()\nplt.show()\n\nprint(train2['Previous Claims'].describe())\nprint(f\"NaN: {train2['Previous Claims'].isna().sum()}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Previous Claim parece una poisson , aplico -1 en los NaN que son bastantes y agrego Columna NaN binaria\n\ntrain2['Previous Claims'] = train2['Previous Claims'].fillna(-1)\ntest2['Previous Claims'] = test2['Previous Claims'].fillna(-1)\n\n\ntrain2['Previous_Claims_NaN'] = (train2['Previous Claims'] == -1).astype(int)\ntest2['Previous_Claims_NaN'] = (test2['Previous Claims'] == -1).astype(int)\n\ntrain2.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train3 = train2.copy()\ntest3 = test2.copy()\n\ntrain3.describe(include = \"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Boxplot\nax1.boxplot(train3['Vehicle Age'].dropna())\nax1.set_title('Vehicle Age - Boxplot')\n\n# Histograma\nax2.hist(train3['Vehicle Age'].dropna(), bins=2, edgecolor='black')\nax2.set_title('Vehicle Age - Histogram')\n\nplt.tight_layout()\nplt.show()\n\nprint(train3['Vehicle Age'].describe())\nprint(f\"NaN: {train3['Vehicle Age'].isna().sum()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# por solo 6 valores faltantes podria hacer lo que quiera , ya sea eliminar las filas , reemplazar por la media o mediana , etc\n# elijo reemplazar por la mediana para conservar 1,2kk de datos\n\ntrain3['Vehicle Age'] = train3['Vehicle Age'].fillna(train3[\"Vehicle Age\"].median())\ntest3['Vehicle Age'] = test3['Vehicle Age'].fillna(test3[\"Vehicle Age\"].median())\n\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train3.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#insurance 1 solo valor , al ser alguien de 64 a√±os osea la edad maxima , le pongo 1 de duration , porque en el dataset no se ve a nadie \n#con una edad superior con poliza activa\n\nnan_row = train3[train3['Insurance Duration'].isna()]\nprint(nan_row)\n\ntrain3['Insurance Duration'] = train3['Insurance Duration'].fillna(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# son 2 casos distintos , pero 2 datos entre 800k no me va a cambiar ,asi que tambien pongo 1 \n\nnan_row = test3[test3['Insurance Duration'].isna()]\nprint(nan_row)\n\ntest3['Insurance Duration'] = test3['Insurance Duration'].fillna(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# para Policy Start date extraigo  A√ëO/TRIMESTRE/MES/DIA/ DIA SEMANA  hora/minutos/segundos puede ser ruido , capaz las horas por querer ver\n# el horario laboral de cuando se hacen mas inicio de poliza\n# luego elimino la fecha\n\n# Convertir a datetime si no lo est√°\ntrain3['Policy Start Date'] = pd.to_datetime(train3['Policy Start Date'])\ntest3['Policy Start Date'] = pd.to_datetime(test3['Policy Start Date'])\n\n\ntrain3['Year'] = train3['Policy Start Date'].dt.year\ntrain3['Quarter'] = train3['Policy Start Date'].dt.quarter\ntrain3['Month'] = train3['Policy Start Date'].dt.month\ntrain3['Day'] = train3['Policy Start Date'].dt.day\ntrain3['DayOfWeek'] = train3['Policy Start Date'].dt.dayofweek  # 0=Lunes, 6=Domingo\ntrain3['Hour'] = train3['Policy Start Date'].dt.hour\n\ntest3['Year'] = test3['Policy Start Date'].dt.year\ntest3['Quarter'] = test3['Policy Start Date'].dt.quarter\ntest3['Month'] = test3['Policy Start Date'].dt.month\ntest3['Day'] = test3['Policy Start Date'].dt.day\ntest3['DayOfWeek'] = test3['Policy Start Date'].dt.dayofweek\ntest3['Hour'] = test3['Policy Start Date'].dt.hour","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train3 = train3.drop('Policy Start Date', axis=1)\ntest3 = test3.drop('Policy Start Date', axis=1)\n\n\ntrain3.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train3.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# para customer feedback , reemplazo con unknowk los NaN\n\ntrain3['Customer Feedback'] = train3['Customer Feedback'].fillna('Unknown')\ntest3['Customer Feedback'] = test3['Customer Feedback'].fillna('Unknown')\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train3.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quick_info(df):\n    \"\"\"Informaci√≥n r√°pida del dataset\"\"\"\n    print(f\"üìä Dataset Shape: {df.shape}\")\n    print(f\"üî¢ Columnas num√©ricas: {df.select_dtypes(include=[np.number]).columns.tolist()}\")\n    print(f\"üìù Columnas categ√≥ricas: {df.select_dtypes(include=['object']).columns.tolist()}\")\n    print(f\"‚ùå Valores nulos por columna:\")\n    print(df.isnull().sum()[df.isnull().sum() > 0])\n\n\nquick_info(train3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quick_info(test3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ahora me tengo que encargar de hacer one-hot encoding y/o label encoding\n\n# Ordinales - Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nordinal_cols = ['Education Level', 'Customer Feedback', 'Exercise Frequency','Policy Type']\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    train3[col] = le.fit_transform(train3[col].astype(str))\n    test3[col] = le.transform(test3[col].astype(str))\n\n# Nominales - One-Hot Encoding  \nnominal_cols = ['Gender', 'Marital Status', 'Occupation', 'Location', 'Smoking Status', 'Property Type']\ntrain_encoded = pd.get_dummies(train3, columns=nominal_cols, drop_first=True)\ntest_encoded = pd.get_dummies(test3, columns=nominal_cols, drop_first=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quick_info(train_encoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quick_info(test_encoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#saco \"id\" que no lo utilizo ahora , solo lo uso  en la submission\ntrain_ids = train_encoded['id'].copy()\ntest_ids = test_encoded['id'].copy()\n\n# Al final, para submission:\n#submission = pd.DataFrame({\n#    'id': test_ids,\n#    'prediction': predictions\n#})\n\n\ny = train_encoded[\"Premium Amount\"]\nX = train_encoded.drop([\"Premium Amount\",\"id\"], axis=1)\n\nX_t = test_encoded.drop('id', axis=1)\n\n# Split para validaci√≥n final\n#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1722)\n\n# Grid de par√°metros con estos parametros tardo mucho y tiro error\n#param_grid = {\n#    'n_estimators': [100, 200, 300],\n#    'max_depth': [10, 15, 20, None],\n#    'min_samples_split': [2, 5, 10],\n#    'min_samples_leaf': [1, 2, 4],\n#    'max_features': ['sqrt', 'log2', None]\n#}\n\n#param_grid = {\n#    'n_estimators': [100, 200],\n#    'max_depth': [10, 20],\n#    'min_samples_split': [2, 10],\n#    'min_samples_leaf': [1, 4],\n#    'max_features': ['sqrt', None]\n#}\n#random_search = RandomizedSearchCV(\n#    RandomForestRegressor(random_state=1722, n_jobs=-4),\n#    param_grid,\n#    n_iter=20,\n#    cv=3,\n#    scoring='neg_mean_squared_log_error', \n#    n_jobs=-4,\n#    verbose=3,\n#    random_state=1722\n#)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%time\n#random_search.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mejores par√°metros\n#print(\"Mejores par√°metros:\", random_search.best_params_)\n#print(\"Mejor score CV:\", -random_search.best_score_)  # Negativo por el 'neg_'\n\n# Evaluar en validaci√≥n con RMSLE\n#best_rf = random_search.best_estimator_\n#y_pred = best_rf.predict(X_val)\n#rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\n#print(f\"RMSLE: {rmsle:.4f}\")\n\n#Mejores par√°metros: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 10}\n#Mejor score CV: 1.28097918942279\n#RMSLE: 1.1405 con este score entraria en el puesto 13 , dado que la competencia se reinicia cada cierto tiempo ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#best_model =RandomForestRegressor(\n#    random_state=1722,\n#    n_jobs=-1,\n#    n_estimators=100,\n#    max_depth=10,\n#    min_samples_split=10,\n#    min_samples_leaf=2,\n#    max_features=\"sqrt\"\n#)\n\n#best_model.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#predictions = best_model.predict(X_t)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission = pd.DataFrame({\n#    'id': test_ids,  # Los IDs que guardaste\n#    'Premium Amount': predictions  # Cambia por el nombre correcto de la columna target\n#})\n\n#submission.to_csv('submission.csv', index=False)\n#print(\"Submission creado!\")\n#print(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dejo por aca el notebook , puedo probar XGBoost/LGBM/Catboot y DNN / ensemble y/o hacer un feature eng , por ser una competencia con pocos participantes\n# no se puede comparar y ver en que fueron mejor los demas competidores y obtener ideas\n# prefiero continuar con otros para seguir aprendiendo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#v5\n#param_grid = {\n#    'n_estimators': [50, 90, 140],\n#    'max_depth': [7, 14],\n#    'min_samples_split': [2, 4],\n#    'min_samples_leaf': [1, 4],\n#    'max_features': ['sqrt', 'log2']\n#}\n#random_search1 = RandomizedSearchCV(\n#    RandomForestRegressor(random_state=1, n_jobs=-1),\n#    param_grid,\n#    n_iter=10,\n#    cv=3,\n#    scoring='neg_mean_squared_log_error', \n#    n_jobs=-1,\n#    verbose=3,\n#    random_state=1\n#)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%time\n#random_search1.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#random_best_params1 = random_search1.best_params_\n\n#print(\"Mejores par√°metros:\", random_search1.best_params_)\n#print(\"Mejor score CV:\", -random_search1.best_score_)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#random1 = RandomForestRegressor(\n#    random_state=1,\n#    n_jobs=-1,\n#    n_estimators=random_best_params1['n_estimators'],\n#    max_depth=random_best_params1['max_depth'],\n#    min_samples_split=random_best_params1['min_samples_split'],\n#    min_samples_leaf=random_best_params1['min_samples_leaf'],\n#    max_features=random_best_params1['max_features']\n#)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%time\n#random1.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#predictions1 = random1.predict(X_t)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission = pd.DataFrame({\n#    'id': test_ids,  # Los IDs que guardaste\n#    'Premium Amount': predictions1  # Cambia por el nombre correcto de la columna target\n#})\n\n#submission.to_csv('submission.csv', index=False)\n#print(\"Submission creado!\")\n#print(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#v6\n#param_grid = {\n#    'n_estimators': [50, 90, 140],\n#    'max_depth': [7, 14],\n#    'min_samples_split': [2, 4],\n#    'min_samples_leaf': [1, 4],\n#    'max_features': ['sqrt', 'log2']\n#}\n#random_search2 = RandomizedSearchCV(\n#    RandomForestRegressor(random_state=2, n_jobs=-1),\n#    param_grid,\n#    n_iter=10,\n#    cv=3,\n#    scoring='neg_mean_squared_log_error', \n#    n_jobs=-1,\n#    verbose=3,\n#    random_state=2\n#)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%time\n#random_search2.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#random_best_params2 = random_search2.best_params_\n\n#print(\"Mejores par√°metros:\", random_search2.best_params_)\n#print(\"Mejor score CV:\", -random_search2.best_score_)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#random2 = RandomForestRegressor(\n#    random_state=2,\n#    n_jobs=-1,\n#    n_estimators=random_best_params2['n_estimators'],\n#    max_depth=random_best_params2['max_depth'],\n#    min_samples_split=random_best_params2['min_samples_split'],\n#    min_samples_leaf=random_best_params2['min_samples_leaf'],\n#    max_features=random_best_params2['max_features']\n#)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%time\n#random2.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#predictions2 = random2.predict(X_t)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission = pd.DataFrame({\n#    'id': test_ids,  # Los IDs que guardaste\n#    'Premium Amount': predictions2  # Cambia por el nombre correcto de la columna target\n#})\n\n#submission.to_csv('submission.csv', index=False)\n#print(\"Submission creado!\")\n#print(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#v7 xgboost hecho en otro lado\n#Mejores par√°metros: {'subsample': 0.9, 'reg_lambda': 2, 'reg_alpha': 1, 'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n#Mejor score: 1.288746636454776\n# va con ensemble , pero aunque mejore un poco , si o si feature eng para mejorar mas\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Xgb1 = xgb.XGBRegressor(\n    random_state=2,\n    subsample  = 0.9,\n    reg_lambda = 2,\n    reg_alpha = 1,\n    n_estimators = 100,\n    max_depth = 9,\n    learning_rate = 0.1,\n    colsample_bytree = 1.0\n)\n\nXgb1.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random1 = RandomForestRegressor(\n    random_state=1,\n    n_jobs=-1,\n    n_estimators= 100,\n    max_depth= 10,\n    min_samples_split= 2,\n    min_samples_leaf= 4,\n    max_features= None\n)\n\nrandom1.fit(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Hacer predicciones con ambos\npred_rf = random1.predict(X_t)\npred_xgb = Xgb1.predict(X_t)\n\n# Ensemble simple: promedio\npredictions = (pred_rf + pred_xgb) / 2\n\n# Crear submission\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'Premium Amount': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission creado!\")\nprint(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv', index=False)\n# 1.13569 RMSLE logro en el score de la competencia , los anteriores era 1.1519\n# si o si para mejorar es feature eng / feature importante y trabajar sobre esas columnas y ordenarlas y mas","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}