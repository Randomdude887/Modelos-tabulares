{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57094,"databundleVersionId":6197974,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.kaggle.com/competitions/forecasting-mini-course-sales/overview\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\nfrom sklearn.preprocessing import (\n    StandardScaler, MinMaxScaler, RobustScaler,\n    LabelEncoder, OneHotEncoder, OrdinalEncoder,\n    PowerTransformer, QuantileTransformer\n)\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.model_selection import (\n    train_test_split, cross_val_score, cross_validate,\n    StratifiedKFold, KFold, GridSearchCV, RandomizedSearchCV,\n    validation_curve, learning_curve\n)\n\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier, ExtraTreesClassifier,\n    GradientBoostingClassifier, AdaBoostClassifier,\n    VotingClassifier, BaggingClassifier\n)\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\n\nfrom sklearn.linear_model import (\n    LinearRegression, Ridge, Lasso, ElasticNet,\n    SGDRegressor, BayesianRidge, HuberRegressor\n)\nfrom sklearn.ensemble import (\n    RandomForestRegressor, ExtraTreesRegressor,\n    GradientBoostingRegressor, AdaBoostRegressor,\n    VotingRegressor, BaggingRegressor\n)\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\n\n\nfrom sklearn.decomposition import PCA, TruncatedSVD, FastICA\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_selection import (\n    SelectKBest, SelectFromModel, RFE, RFECV,\n    chi2, f_classif, f_regression, mutual_info_classif\n)\n\n\nfrom sklearn.metrics import (\n    # Clasificaci√≥n\n    accuracy_score, precision_score, recall_score, f1_score,\n    classification_report, confusion_matrix, roc_auc_score,\n    roc_curve, precision_recall_curve, auc, log_loss,\n    \n    # Regresi√≥n\n    mean_squared_error, mean_absolute_error, r2_score,\n    mean_squared_log_error, mean_absolute_percentage_error,\n    \n    # Clustering\n    silhouette_score, adjusted_rand_score, calinski_harabasz_score\n)\n\ntry:\n    import xgboost as xgb\n    print(\"‚úÖ XGBoost disponible\")\nexcept ImportError:\n    print(\"‚ùå XGBoost no instalado\")\n\ntry:\n    import lightgbm as lgb\n    print(\"‚úÖ LightGBM disponible\")\nexcept ImportError:\n    print(\"‚ùå LightGBM no instalado\")\n\ntry:\n    from catboost import CatBoostClassifier, CatBoostRegressor\n    print(\"‚úÖ CatBoost disponible\")\nexcept ImportError:\n    print(\"‚ùå CatBoost no instalado\")\n\n\ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n    print(\"‚úÖ TensorFlow disponible\")\nexcept ImportError:\n    print(\"‚ùå TensorFlow no instalado\")\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    print(\"‚úÖ PyTorch disponible\")\nexcept ImportError:\n    print(\"‚ùå PyTorch no instalado\")\n\n\ntry:\n    import spacy\n    print(\"‚úÖ SpaCy disponible\")\nexcept ImportError:\n    print(\"‚ùå SpaCy no instalado\")\n\n\ntry:\n    import scipy.stats as stats\n    from scipy import stats\n    print(\"‚úÖ SciPy disponible\")\nexcept ImportError:\n    print(\"‚ùå SciPy no instalado\")\n\ntry:\n    import statsmodels.api as sm\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    print(\"‚úÖ Statsmodels disponible\")\nexcept ImportError:\n    print(\"‚ùå Statsmodels no instalado\")\n\n\ntry:\n    from prophet import Prophet\n    print(\"‚úÖ Prophet disponible\")\nexcept ImportError:\n    print(\"‚ùå Prophet no instalado\")\n\n\ntry:\n    import optuna\n    print(\"‚úÖ Optuna disponible\")\nexcept ImportError:\n    print(\"‚ùå Optuna no instalado\")\n\n\nimport os\nimport sys\nimport json\nimport pickle\nimport joblib\nfrom datetime import datetime, timedelta\nimport itertools\nfrom collections import Counter\nimport gc\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.4f}'.format)\npd.set_option('display.width', None)\n\n\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\n\nsns.set_palette(\"husl\")\nsns.set_style(\"whitegrid\")\n\n\nnp.random.seed(1722)\n\n\ndef info_basica(df):\n    \"\"\"Funci√≥n para explorar un dataset r√°pidamente\"\"\"\n    print(\"=\" * 50)\n    print(\"INFORMACI√ìN B√ÅSICA DEL DATASET\")\n    print(\"=\" * 50)\n    print(f\"Forma del dataset: {df.shape}\")\n    print(f\"Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    print(\"\\nTipos de datos:\")\n    print(df.dtypes.value_counts())\n    print(\"\\nValores nulos:\")\n    print(df.isnull().sum().sort_values(ascending=False))\n    print(\"\\nValores duplicados:\", df.duplicated().sum())\n    print(\"\\nPrimeras 5 filas:\")\n    print(df.head())\n\ndef plot_distribucion(df, columna, tipo='auto'):\n    \"\"\"Plotea la distribuci√≥n de una columna\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    if tipo == 'auto':\n        tipo = 'continua' if df[columna].dtype in ['float64', 'int64'] else 'categorica'\n    \n    if tipo == 'continua':\n        # Histograma\n        ax1.hist(df[columna].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n        ax1.set_title(f'Distribuci√≥n de {columna}')\n        ax1.set_xlabel(columna)\n        ax1.set_ylabel('Frecuencia')\n        \n        # Boxplot\n        ax2.boxplot(df[columna].dropna())\n        ax2.set_title(f'Boxplot de {columna}')\n        ax2.set_ylabel(columna)\n    else:\n        # Gr√°fico de barras\n        df[columna].value_counts().plot(kind='bar', ax=ax1, color='lightcoral')\n        ax1.set_title(f'Distribuci√≥n de {columna}')\n        ax1.set_xlabel(columna)\n        ax1.set_ylabel('Frecuencia')\n        ax1.tick_params(axis='x', rotation=45)\n        \n        # Gr√°fico de pie\n        df[columna].value_counts().plot(kind='pie', ax=ax2, autopct='%1.1f%%')\n        ax2.set_title(f'Proporci√≥n de {columna}')\n        ax2.set_ylabel('')\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e19/train.csv') \ntest  = pd.read_csv(\"/kaggle/input/playground-series-s3e19/test.csv\")\ntest_orig = pd.read_csv(\"/kaggle/input/playground-series-s3e19/test.csv\")\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# agrego 1 mes de datos a test que despues voy a usar\n\ndecember_2021 = train[(train['date'] >= '2021-12-01') & (train['date'] <= '2021-12-31')]\ntest = pd.concat([december_2021, test], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info_basica(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info_basica(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# num_sold lo que tengo que predecir para 27375 filas osea (27375/365) = 75 id cada dia\n# 5 paises , 3 tiendas, 5 productos ,(136950/1826) = 75 id para cada dia , (75/5) = 15 de cada pais ,\n# cada tienda vende 5 productos en cada pais cada dia (5 * 3 * 5) = 75 id , productos = (136950/27390) = 5\n# fecha desde 01-01-2017 a 31-12-2021 5 a√±os * 365 = 1825 , 1826 por un a√±o bisiesto\n# predecir todo el a√±o de ventas del 2022\n\n#por como esta hecha toda sintetica la info lo primero que se me viene en agrupar informacion y enfocar el modelo\n#en predecir distribuciones sea nivel pais o dia o tienda o producto , ver si sigue alguna distrib de conteo conocida\n# y comparar si a√±o a a√±o cambia\n\ntrain.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train[\"date\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test[\"date\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1 = train.copy()\ntest1  = test.copy()\n\n# date lo separo \n\ntrain1['date'] = pd.to_datetime(train1['date'])\ntest1['date'] = pd.to_datetime(test1['date'])\n\n\ntrain1['Year'] = train1['date'].dt.year\ntrain1['Quarter'] = train1['date'].dt.quarter\ntrain1['Month'] = train1['date'].dt.month\ntrain1['Day'] = train1['date'].dt.day\ntrain1['DayOfWeek'] = train1['date'].dt.dayofweek  # 0=Lunes, 6=Domingo\n\ntest1['Year'] = test1['date'].dt.year\ntest1['Quarter'] = test1['date'].dt.quarter\ntest1['Month'] = test1['date'].dt.month\ntest1['Day'] = test1['date'].dt.day\ntest1['DayOfWeek'] = test1['date'].dt.dayofweek  # 0=Lunes, 6=Domingo\n\ntrain1.head(76)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# voy a prodecer a ver distintos tipos de agrupaciones y estadisticas , para ver de que forma me conviene\n# hacer el modelo , si predecir de arriba para abajo o  al reves\n\n\n# Distribuciones a diferentes niveles de agregaci√≥n\ndaily_sales = train1.groupby('date')['num_sold'].sum()\ncountry_daily = train1.groupby(['date', 'country'])['num_sold'].sum()\nstore_daily = train1.groupby(['date', 'country', 'store'])['num_sold'].sum()\nproduct_daily = train1.groupby(['date', 'product'])['num_sold'].sum()\n\ndaily_sales\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Crear subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Distribuciones de Ventas por Nivel de Agregaci√≥n', fontsize=16)\n\n# 1. Ventas totales diarias\naxes[0,0].hist(daily_sales, bins=30, alpha=0.7, color='blue', edgecolor='black')\naxes[0,0].set_title(f'Ventas Totales Diarias\\nMedia: {daily_sales.mean():.1f}, Std: {daily_sales.std():.1f}')\naxes[0,0].set_xlabel('Ventas Totales por D√≠a')\naxes[0,0].set_ylabel('Frecuencia')\n\n# 2. Ventas por pa√≠s-d√≠a\naxes[0,1].hist(country_daily, bins=30, alpha=0.7, color='green', edgecolor='black')\naxes[0,1].set_title(f'Ventas por Pa√≠s-D√≠a\\nMedia: {country_daily.mean():.1f}, Std: {country_daily.std():.1f}')\naxes[0,1].set_xlabel('Ventas por Pa√≠s-D√≠a')\naxes[0,1].set_ylabel('Frecuencia')\n\n# 3. Ventas por tienda-d√≠a\naxes[1,0].hist(store_daily, bins=30, alpha=0.7, color='orange', edgecolor='black')\naxes[1,0].set_title(f'Ventas por Tienda-D√≠a\\nMedia: {store_daily.mean():.1f}, Std: {store_daily.std():.1f}')\naxes[1,0].set_xlabel('Ventas por Tienda-D√≠a')\naxes[1,0].set_ylabel('Frecuencia')\n\n# 4. Ventas por producto-d√≠a\naxes[1,1].hist(product_daily, bins=30, alpha=0.7, color='red', edgecolor='black')\naxes[1,1].set_title(f'Ventas por Producto-D√≠a\\nMedia: {product_daily.mean():.1f}, Std: {product_daily.std():.1f}')\naxes[1,1].set_xlabel('Ventas por Producto-D√≠a')\naxes[1,1].set_ylabel('Frecuencia')\n\nplt.tight_layout()\nplt.show()\n\n# Estad√≠sticas descriptivas\nprint(\"=== ESTAD√çSTICAS DESCRIPTIVAS ===\")\nprint(f\"\\n1. VENTAS TOTALES DIARIAS (75 registros/d√≠a):\")\nprint(f\"   Min: {daily_sales.min()}, Max: {daily_sales.max()}\")\nprint(f\"   Media: {daily_sales.mean():.2f}, Mediana: {daily_sales.median():.2f}\")\nprint(f\"   Coef. Variaci√≥n: {daily_sales.std()/daily_sales.mean():.3f}\")\n\nprint(f\"\\n2. VENTAS POR PA√çS-D√çA (15 registros/pa√≠s/d√≠a):\")\nprint(f\"   Min: {country_daily.min()}, Max: {country_daily.max()}\")\nprint(f\"   Media: {country_daily.mean():.2f}, Mediana: {country_daily.median():.2f}\")\nprint(f\"   Coef. Variaci√≥n: {country_daily.std()/country_daily.mean():.3f}\")\n\nprint(f\"\\n3. VENTAS POR TIENDA-D√çA (5 registros/tienda/d√≠a):\")\nprint(f\"   Min: {store_daily.min()}, Max: {store_daily.max()}\")\nprint(f\"   Media: {store_daily.mean():.2f}, Mediana: {store_daily.median():.2f}\")\nprint(f\"   Coef. Variaci√≥n: {store_daily.std()/store_daily.mean():.3f}\")\n\nprint(f\"\\n4. VENTAS POR PRODUCTO-D√çA:\")\nprint(f\"   Min: {product_daily.min()}, Max: {product_daily.max()}\")\nprint(f\"   Media: {product_daily.mean():.2f}, Mediana: {product_daily.median():.2f}\")\nprint(f\"   Coef. Variaci√≥n: {product_daily.std()/product_daily.mean():.3f}\")\n\n# Ver la distribuci√≥n individual (nivel m√°s granular)\nprint(f\"\\n5. VENTAS INDIVIDUALES (nivel m√°s granular):\")\nprint(f\"   Min: {train1['num_sold'].min()}, Max: {train1['num_sold'].max()}\")\nprint(f\"   Media: {train1['num_sold'].mean():.2f}, Mediana: {train1['num_sold'].median():.2f}\")\nprint(f\"   Coef. Variaci√≥n: {train1['num_sold'].std()/train1['num_sold'].mean():.3f}\")\n\n# Histograma adicional para el nivel individual\nplt.figure(figsize=(10, 6))\nplt.hist(train1['num_sold'], bins=50, alpha=0.7, color='purple', edgecolor='black')\nplt.title(f'Distribuci√≥n de Ventas Individuales\\nMedia: {train1[\"num_sold\"].mean():.1f}, Std: {train1[\"num_sold\"].std():.1f}')\nplt.xlabel('N√∫mero de Ventas')\nplt.ylabel('Frecuencia')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crear las agregaciones por a√±o\nyears = [2017, 2018, 2019, 2020, 2021]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# 1. VENTAS POR TIENDA-D√çA POR A√ëO\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Distribuciones de Ventas por Tienda-D√≠a por A√±o', fontsize=16)\n\nfor i, year in enumerate(years):\n    year_data = train1[train1['Year'] == year]\n    store_daily_year = year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n    \n    row = i // 3\n    col = i % 3\n    \n    axes[row, col].hist(store_daily_year, bins=25, alpha=0.7, color=colors[i], edgecolor='black')\n    axes[row, col].set_title(f'A√±o {year}\\nMedia: {store_daily_year.mean():.1f}, Std: {store_daily_year.std():.1f}')\n    axes[row, col].set_xlabel('Ventas por Tienda-D√≠a')\n    axes[row, col].set_ylabel('Frecuencia')\n    \n    # Agregar l√≠nea vertical en la media\n    axes[row, col].axvline(store_daily_year.mean(), color='black', linestyle='--', alpha=0.8)\n\n# Remover el subplot extra\naxes[1, 2].remove()\n\nplt.tight_layout()\nplt.show()\n\n# 2. VENTAS TOTALES DIARIAS POR A√ëO\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Distribuciones de Ventas Totales Diarias por A√±o', fontsize=16)\n\nfor i, year in enumerate(years):\n    year_data = train1[train1['Year'] == year]\n    daily_sales_year = year_data.groupby('date')['num_sold'].sum()\n    \n    row = i // 3\n    col = i % 3\n    \n    axes[row, col].hist(daily_sales_year, bins=25, alpha=0.7, color=colors[i], edgecolor='black')\n    axes[row, col].set_title(f'A√±o {year}\\nMedia: {daily_sales_year.mean():.1f}, Std: {daily_sales_year.std():.1f}')\n    axes[row, col].set_xlabel('Ventas Totales Diarias')\n    axes[row, col].set_ylabel('Frecuencia')\n    \n    # Agregar l√≠nea vertical en la media\n    axes[row, col].axvline(daily_sales_year.mean(), color='black', linestyle='--', alpha=0.8)\n\n# Remover el subplot extra\naxes[1, 2].remove()\n\nplt.tight_layout()\nplt.show()\n\n# 3. COMPARACI√ìN DE ESTAD√çSTICAS POR A√ëO\nprint(\"=== EVOLUCI√ìN DE ESTAD√çSTICAS POR A√ëO ===\")\nprint(\"\\n1. VENTAS POR TIENDA-D√çA:\")\nprint(\"A√±o\\tMedia\\tStd\\tCoef.Var\\tMin\\tMax\")\nprint(\"-\" * 50)\n\nstore_stats = []\nfor year in years:\n    year_data = train1[train1['Year'] == year]\n    store_daily_year = year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n    \n    mean_val = store_daily_year.mean()\n    std_val = store_daily_year.std()\n    cv = std_val / mean_val\n    min_val = store_daily_year.min()\n    max_val = store_daily_year.max()\n    \n    store_stats.append([year, mean_val, std_val, cv, min_val, max_val])\n    print(f\"{year}\\t{mean_val:.1f}\\t{std_val:.1f}\\t{cv:.3f}\\t\\t{min_val}\\t{max_val}\")\n\nprint(\"\\n2. VENTAS TOTALES DIARIAS:\")\nprint(\"A√±o\\tMedia\\tStd\\tCoef.Var\\tMin\\tMax\")\nprint(\"-\" * 50)\n\ndaily_stats = []\nfor year in years:\n    year_data = train1[train1['Year'] == year]\n    daily_sales_year = year_data.groupby('date')['num_sold'].sum()\n    \n    mean_val = daily_sales_year.mean()\n    std_val = daily_sales_year.std()\n    cv = std_val / mean_val\n    min_val = daily_sales_year.min()\n    max_val = daily_sales_year.max()\n    \n    daily_stats.append([year, mean_val, std_val, cv, min_val, max_val])\n    print(f\"{year}\\t{mean_val:.1f}\\t{std_val:.1f}\\t{cv:.3f}\\t\\t{min_val}\\t{max_val}\")\n\n# 4. GR√ÅFICOS DE EVOLUCI√ìN TEMPORAL\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Evoluci√≥n de medias\nstore_means = [stats[1] for stats in store_stats]\ndaily_means = [stats[1] for stats in daily_stats]\n\naxes[0].plot(years, store_means, 'o-', color='blue', linewidth=2, markersize=8, label='Tienda-D√≠a')\naxes[0].plot(years, daily_means, 's-', color='red', linewidth=2, markersize=8, label='Total Diario')\naxes[0].set_title('Evoluci√≥n de Medias por A√±o')\naxes[0].set_xlabel('A√±o')\naxes[0].set_ylabel('Media de Ventas')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Evoluci√≥n de coeficientes de variaci√≥n\nstore_cvs = [stats[3] for stats in store_stats]\ndaily_cvs = [stats[3] for stats in daily_stats]\n\naxes[1].plot(years, store_cvs, 'o-', color='blue', linewidth=2, markersize=8, label='Tienda-D√≠a')\naxes[1].plot(years, daily_cvs, 's-', color='red', linewidth=2, markersize=8, label='Total Diario')\naxes[1].set_title('Evoluci√≥n de Coeficiente de Variaci√≥n por A√±o')\naxes[1].set_xlabel('A√±o')\naxes[1].set_ylabel('Coeficiente de Variaci√≥n')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 5. TEST DE NORMALIDAD B√ÅSICO (opcional)\nfrom scipy import stats as scipy_stats\n\nprint(\"\\n=== TESTS DE FORMA DE DISTRIBUCI√ìN ===\")\nprint(\"\\nPara detectar si las distribuciones cambian de forma:\")\nprint(\"- Si Coef.Var ‚âà 1: podr√≠a ser Poisson\")\nprint(\"- Si Coef.Var > 1: podr√≠a ser Binomial Negativa\")\nprint(\"- Si Coef.Var << 1: podr√≠a tender a Normal\")\n\nfor year in years:\n    year_data = train1[train1['Year'] == year]\n    store_daily_year = year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n    \n    cv = store_daily_year.std() / store_daily_year.mean()\n    \n    if abs(cv - 1) < 0.1:\n        distribution_type = \"Poisson-like\"\n    elif cv > 1.2:\n        distribution_type = \"Overdispersed (Binomial Negativa?)\"\n    elif cv < 0.5:\n        distribution_type = \"Underdispersed (Normal?)\"\n    else:\n        distribution_type = \"Intermedio\"\n    \n    print(f\"A√±o {year} - Tienda-D√≠a: CV={cv:.3f} -> {distribution_type}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Asegurarse de que la columna date sea datetime si no lo est√°\ntrain1['date'] = pd.to_datetime(train1['date'])\n\n# Para los d√≠as de la semana, crear nombres legibles\nday_names = {0: 'Lunes', 1: 'Martes', 2: 'Mi√©rcoles', 3: 'Jueves', \n             4: 'Viernes', 5: 'S√°bado', 6: 'Domingo'}\ntrain1['day_name'] = train1['DayOfWeek'].map(day_names)\n\nyears = [2017, 2018, 2019, 2020, 2021]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# ===== 1. AN√ÅLISIS POR PA√çS A√ëO A A√ëO =====\nprint(\"=== AN√ÅLISIS POR PA√çS A√ëO A A√ëO ===\")\n\n# Obtener los pa√≠ses √∫nicos\ncountries = train1['country'].unique()\nprint(f\"Pa√≠ses en el dataset: {countries}\")\n\n# Crear figura para distribuciones por pa√≠s\nfig, axes = plt.subplots(len(countries), len(years), figsize=(20, 4*len(countries)))\nfig.suptitle('Distribuciones de Ventas por Pa√≠s y A√±o (Tienda-D√≠a)', fontsize=16)\n\ncountry_stats = {}\nfor i, country in enumerate(countries):\n    country_stats[country] = []\n    for j, year in enumerate(years):\n        # Filtrar por pa√≠s y a√±o\n        country_year_data = train1[(train1['country'] == country) & (train1['Year'] == year)]\n        store_daily = country_year_data.groupby(['date', 'store'])['num_sold'].sum()\n        \n        # Crear histograma\n        axes[i, j].hist(store_daily, bins=20, alpha=0.7, color=colors[j], edgecolor='black')\n        axes[i, j].set_title(f'{country} - {year}\\nMedia: {store_daily.mean():.1f}')\n        axes[i, j].set_xlabel('Ventas Tienda-D√≠a')\n        if j == 0:  # Solo en la primera columna\n            axes[i, j].set_ylabel(f'{country}\\nFrecuencia')\n        \n        # Guardar estad√≠sticas\n        country_stats[country].append({\n            'year': year,\n            'mean': store_daily.mean(),\n            'std': store_daily.std(),\n            'cv': store_daily.std() / store_daily.mean(),\n            'count': len(store_daily)\n        })\n\nplt.tight_layout()\nplt.show()\n\n# Tabla de estad√≠sticas por pa√≠s\nprint(\"\\nESTAD√çSTICAS POR PA√çS:\")\nfor country in countries:\n    print(f\"\\n{country.upper()}:\")\n    print(\"A√±o\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n    print(\"-\" * 40)\n    for stat in country_stats[country]:\n        print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n\n# ===== 2. AN√ÅLISIS POR D√çA DE LA SEMANA A√ëO A A√ëO =====\nprint(\"\\n\\n=== AN√ÅLISIS POR D√çA DE LA SEMANA A√ëO A A√ëO ===\")\n\n# Crear figura para distribuciones por d√≠a de la semana\nfig, axes = plt.subplots(7, len(years), figsize=(20, 28))\nfig.suptitle('Distribuciones de Ventas por D√≠a de la Semana y A√±o (Tienda-D√≠a)', fontsize=16)\n\nday_stats = {}\ndays_order = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']\n\nfor i, day in enumerate(days_order):\n    day_stats[day] = []\n    for j, year in enumerate(years):\n        # Filtrar por d√≠a de la semana y a√±o\n        day_year_data = train1[(train1['day_name'] == day) & (train1['Year'] == year)]\n        store_daily = day_year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n        \n        # Crear histograma\n        axes[i, j].hist(store_daily, bins=20, alpha=0.7, color=colors[j], edgecolor='black')\n        axes[i, j].set_title(f'{day} - {year}\\nMedia: {store_daily.mean():.1f}')\n        axes[i, j].set_xlabel('Ventas Tienda-D√≠a')\n        if j == 0:  # Solo en la primera columna\n            axes[i, j].set_ylabel(f'{day}\\nFrecuencia')\n        \n        # Guardar estad√≠sticas\n        day_stats[day].append({\n            'year': year,\n            'mean': store_daily.mean(),\n            'std': store_daily.std(),\n            'cv': store_daily.std() / store_daily.mean(),\n            'count': len(store_daily)\n        })\n\nplt.tight_layout()\nplt.show()\n\n# Tabla de estad√≠sticas por d√≠a de la semana\nprint(\"\\nESTAD√çSTICAS POR D√çA DE LA SEMANA:\")\nfor day in days_order:\n    print(f\"\\n{day.upper()}:\")\n    print(\"A√±o\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n    print(\"-\" * 40)\n    for stat in day_stats[day]:\n        print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n\n# ===== 3. GR√ÅFICOS DE EVOLUCI√ìN TEMPORAL =====\n\n# 3.1 Evoluci√≥n por pa√≠s\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nfor country in countries:\n    means = [stat['mean'] for stat in country_stats[country]]\n    cvs = [stat['cv'] for stat in country_stats[country]]\n    \n    axes[0].plot(years, means, 'o-', linewidth=2, markersize=8, label=country)\n    axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=8, label=country)\n\naxes[0].set_title('Evoluci√≥n de Medias por Pa√≠s')\naxes[0].set_xlabel('A√±o')\naxes[0].set_ylabel('Media de Ventas (Tienda-D√≠a)')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].set_title('Evoluci√≥n de Coef. Variaci√≥n por Pa√≠s')\naxes[1].set_xlabel('A√±o')\naxes[1].set_ylabel('Coeficiente de Variaci√≥n')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 3.2 Evoluci√≥n por d√≠a de la semana\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nfor day in days_order:\n    means = [stat['mean'] for stat in day_stats[day]]\n    cvs = [stat['cv'] for stat in day_stats[day]]\n    \n    axes[0].plot(years, means, 'o-', linewidth=2, markersize=6, label=day)\n    axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=6, label=day)\n\naxes[0].set_title('Evoluci√≥n de Medias por D√≠a de la Semana')\naxes[0].set_xlabel('A√±o')\naxes[0].set_ylabel('Media de Ventas (Tienda-D√≠a)')\naxes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].set_title('Evoluci√≥n de Coef. Variaci√≥n por D√≠a de la Semana')\naxes[1].set_xlabel('A√±o')\naxes[1].set_ylabel('Coeficiente de Variaci√≥n')\naxes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ===== 4. AN√ÅLISIS DE PATRONES =====\nprint(\"\\n=== AN√ÅLISIS DE PATRONES ===\")\n\n# 4.1 Ranking promedio por pa√≠s (todos los a√±os)\nprint(\"\\nRANKING PROMEDIO DE VENTAS POR PA√çS:\")\ncountry_overall = []\nfor country in countries:\n    overall_mean = np.mean([stat['mean'] for stat in country_stats[country]])\n    country_overall.append((country, overall_mean))\n\ncountry_overall.sort(key=lambda x: x[1], reverse=True)\nfor i, (country, mean) in enumerate(country_overall, 1):\n    print(f\"{i}. {country}: {mean:.1f}\")\n\n# 4.2 Ranking promedio por d√≠a de la semana\nprint(\"\\nRANKING PROMEDIO DE VENTAS POR D√çA:\")\nday_overall = []\nfor day in days_order:\n    overall_mean = np.mean([stat['mean'] for stat in day_stats[day]])\n    day_overall.append((day, overall_mean))\n\nday_overall.sort(key=lambda x: x[1], reverse=True)\nfor i, (day, mean) in enumerate(day_overall, 1):\n    print(f\"{i}. {day}: {mean:.1f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"years = [2017, 2018, 2019, 2020, 2021]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# Obtener valores √∫nicos\ncountries = train1['country'].unique()\nproducts = train1['product'].unique()\nstores = train1['store'].unique()\n\nprint(f\"Pa√≠ses: {len(countries)} - {countries}\")\nprint(f\"Productos: {len(products)} - {products}\")\nprint(f\"Tiendas: {len(stores)} - {stores}\")\n\n# ===== 1. AN√ÅLISIS POR PRODUCTO-PA√çS-A√ëO =====\nprint(\"\\n=== AN√ÅLISIS POR PRODUCTO-PA√çS-A√ëO ===\")\n\n# Para cada pa√≠s, mostrar productos por a√±o\nfor country in countries:\n    print(f\"\\n{'='*50}\")\n    print(f\"AN√ÅLISIS PARA {country.upper()}\")\n    print(f\"{'='*50}\")\n    \n    fig, axes = plt.subplots(len(products), len(years), figsize=(20, 4*len(products)))\n    fig.suptitle(f'Distribuciones de Ventas por Producto y A√±o - {country}', fontsize=16)\n    \n    product_stats = {}\n    \n    for i, product in enumerate(products):\n        product_stats[product] = []\n        \n        for j, year in enumerate(years):\n            # Filtrar por pa√≠s, producto y a√±o\n            filtered_data = train1[(train1['country'] == country) & \n                                 (train1['product'] == product) & \n                                 (train1['Year'] == year)]\n            \n            # Agrupar por fecha y tienda (cada producto-tienda-d√≠a)\n            daily_sales = filtered_data.groupby(['date', 'store'])['num_sold'].sum()\n            \n            # Si hay un solo producto, usar axes unidimensional\n            if len(products) == 1:\n                ax = axes[j]\n            else:\n                ax = axes[i, j]\n            \n            # Crear histograma\n            ax.hist(daily_sales, bins=15, alpha=0.7, color=colors[j], edgecolor='black')\n            ax.set_title(f'{product}\\n{year} - Media: {daily_sales.mean():.1f}')\n            ax.set_xlabel('Ventas Diarias')\n            \n            if j == 0:  # Solo en la primera columna\n                if len(products) == 1:\n                    ax.set_ylabel('Frecuencia')\n                else:\n                    ax.set_ylabel(f'{product}\\nFrecuencia')\n            \n            # Guardar estad√≠sticas\n            product_stats[product].append({\n                'year': year,\n                'mean': daily_sales.mean(),\n                'std': daily_sales.std(),\n                'cv': daily_sales.std() / daily_sales.mean() if daily_sales.mean() > 0 else 0,\n                'count': len(daily_sales)\n            })\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Tabla de estad√≠sticas por producto en este pa√≠s\n    print(f\"\\nESTAD√çSTICAS DE PRODUCTOS EN {country}:\")\n    for product in products:\n        print(f\"\\n{product}:\")\n        print(\"A√±o\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n        print(\"-\" * 40)\n        for stat in product_stats[product]:\n            print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n    \n    # Gr√°fico de evoluci√≥n para este pa√≠s\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    for product in products:\n        means = [stat['mean'] for stat in product_stats[product]]\n        cvs = [stat['cv'] for stat in product_stats[product]]\n        \n        axes[0].plot(years, means, 'o-', linewidth=2, markersize=8, label=product)\n        axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=8, label=product)\n    \n    axes[0].set_title(f'Evoluci√≥n de Medias por Producto - {country}')\n    axes[0].set_xlabel('A√±o')\n    axes[0].set_ylabel('Media de Ventas')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    axes[1].set_title(f'Evoluci√≥n de Coef. Variaci√≥n por Producto - {country}')\n    axes[1].set_xlabel('A√±o')\n    axes[1].set_ylabel('Coeficiente de Variaci√≥n')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# ===== 2. AN√ÅLISIS POR TIENDA-A√ëO =====\nprint(f\"\\n{'='*60}\")\nprint(\"AN√ÅLISIS POR TIENDA-A√ëO\")\nprint(f\"{'='*60}\")\n\n# Crear figura para distribuciones por tienda\nfig, axes = plt.subplots(len(stores), len(years), figsize=(20, 4*len(stores)))\nfig.suptitle('Distribuciones de Ventas por Tienda y A√±o', fontsize=16)\n\nstore_stats = {}\n\nfor i, store in enumerate(stores):\n    store_stats[store] = []\n    \n    for j, year in enumerate(years):\n        # Filtrar por tienda y a√±o\n        store_year_data = train1[(train1['store'] == store) & (train1['Year'] == year)]\n        # Agrupar por fecha, pa√≠s y producto (cada tienda-d√≠a)\n        daily_sales = store_year_data.groupby(['date', 'country', 'product'])['num_sold'].sum()\n        \n        # Si hay una sola tienda, usar axes unidimensional\n        if len(stores) == 1:\n            ax = axes[j]\n        else:\n            ax = axes[i, j]\n        \n        # Crear histograma\n        ax.hist(daily_sales, bins=20, alpha=0.7, color=colors[j], edgecolor='black')\n        ax.set_title(f'{store} - {year}\\nMedia: {daily_sales.mean():.1f}')\n        ax.set_xlabel('Ventas Diarias')\n        \n        if j == 0:  # Solo en la primera columna\n            if len(stores) == 1:\n                ax.set_ylabel('Frecuencia')\n            else:\n                ax.set_ylabel(f'{store}\\nFrecuencia')\n        \n        # Guardar estad√≠sticas\n        store_stats[store].append({\n            'year': year,\n            'mean': daily_sales.mean(),\n            'std': daily_sales.std(),\n            'cv': daily_sales.std() / daily_sales.mean() if daily_sales.mean() > 0 else 0,\n            'count': len(daily_sales)\n        })\n\nplt.tight_layout()\nplt.show()\n\n# Tabla de estad√≠sticas por tienda\nprint(\"\\nESTAD√çSTICAS POR TIENDA:\")\nfor store in stores:\n    print(f\"\\n{store.upper()}:\")\n    print(\"A√±o\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n    print(\"-\" * 40)\n    for stat in store_stats[store]:\n        print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n\n# Gr√°fico de evoluci√≥n por tienda\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nfor store in stores:\n    means = [stat['mean'] for stat in store_stats[store]]\n    cvs = [stat['cv'] for stat in store_stats[store]]\n    \n    axes[0].plot(years, means, 'o-', linewidth=2, markersize=8, label=store)\n    axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=8, label=store)\n\naxes[0].set_title('Evoluci√≥n de Medias por Tienda')\naxes[0].set_xlabel('A√±o')\naxes[0].set_ylabel('Media de Ventas')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].set_title('Evoluci√≥n de Coef. Variaci√≥n por Tienda')\naxes[1].set_xlabel('A√±o')\naxes[1].set_ylabel('Coeficiente de Variaci√≥n')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ===== 3. RANKINGS Y AN√ÅLISIS COMPARATIVO =====\nprint(f\"\\n{'='*60}\")\nprint(\"RANKINGS Y AN√ÅLISIS COMPARATIVO\")\nprint(f\"{'='*60}\")\n\n# 3.1 Ranking de productos por pa√≠s (promedio todos los a√±os)\nprint(\"\\nRANKING DE PRODUCTOS POR PA√çS (Promedio 2017-2021):\")\nfor country in countries:\n    print(f\"\\n{country.upper()}:\")\n    country_data = train1[train1['country'] == country]\n    product_means = country_data.groupby('product')['num_sold'].mean().sort_values(ascending=False)\n    \n    for i, (product, mean) in enumerate(product_means.items(), 1):\n        print(f\"  {i}. {product}: {mean:.1f}\")\n\n# 3.2 Ranking de tiendas (promedio todos los a√±os)\nprint(f\"\\nRANKING DE TIENDAS (Promedio 2017-2021):\")\nstore_means = train1.groupby('store')['num_sold'].mean().sort_values(ascending=False)\nfor i, (store, mean) in enumerate(store_means.items(), 1):\n    print(f\"  {i}. {store}: {mean:.1f}\")\n\n# 3.3 Productos m√°s consistentes (menor CV)\nprint(f\"\\nPRODUCTOS M√ÅS CONSISTENTES (Menor Coef. Variaci√≥n):\")\nproduct_consistency = train1.groupby('product')['num_sold'].agg(['mean', 'std'])\nproduct_consistency['cv'] = product_consistency['std'] / product_consistency['mean']\nproduct_consistency = product_consistency.sort_values('cv')\n\nfor i, (product, stats) in enumerate(product_consistency.iterrows(), 1):\n    print(f\"  {i}. {product}: CV={stats['cv']:.3f} (Media: {stats['mean']:.1f})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# de todas estas estadisticas se obtuvo informacion muy relevante\n# lo que mas destaco es ver una distribucion bimodal poisson-normal  o binomial_neg-normal en varios lados, \n# ejemplo la distrib de ventas por producto/a√±o/pais\n# en la distrib por tienda y a√±o se ve como todas comparten la misma tendencia cada a√±o\n# la distrib por dia y pais se ve las ventas de lu-ju una forma y el fin de semana como cambia el viernes un 5 a 7% aprox mas que el jueves\n# el sabado entre 3 a 5 % mas y el domingo 6 a 8 % mas\n# cada pais sus propias ventas pero parecen multiplos de una base ,porque respetan proporciones\n\n# lo mismo que pasa abajo , se de tienda en tienda , teniendo a Kaggle Learn como base\n#   Arg   Canada(Arg/4.1) Estonia(Arg/2.1)  Japon(Arg/3.6) Espa√±a(Arg/2.6)\n# 1  82.3      83.7          83.2              81.6           82.4\n# 2  81.2      82.5          82.2              80.8           81.3\n# 3  68.8      69.8          69.5              68.1           68.8\n# 4  62.6      63.4          63.2              61.9           62.4\n# 5  12.4      12.9          12.8              12.6           12.7\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\n# ===== 1. AN√ÅLISIS DE SEPARACI√ìN CON THRESHOLD =====\ndef analyze_threshold_split(data, threshold=74):\n    \"\"\"Analiza la separaci√≥n de datos en dos grupos usando un threshold\"\"\"\n    \n    low_regime = data[data <= threshold]\n    high_regime = data[data > threshold]\n    \n    total_count = len(data)\n    low_count = len(low_regime)\n    high_count = len(high_regime)\n    \n    low_prop = low_count / total_count\n    high_prop = high_count / total_count\n    \n    results = {\n        'threshold': threshold,\n        'total_count': total_count,\n        'low_regime': {\n            'count': low_count,\n            'proportion': low_prop,\n            'mean': low_regime.mean() if len(low_regime) > 0 else 0,\n            'std': low_regime.std() if len(low_regime) > 0 else 0,\n            'data': low_regime\n        },\n        'high_regime': {\n            'count': high_count,\n            'proportion': high_prop,\n            'mean': high_regime.mean() if len(high_regime) > 0 else 0,\n            'std': high_regime.std() if len(high_regime) > 0 else 0,\n            'data': high_regime\n        }\n    }\n    \n    return results\n\n# ===== 2. TEST DE DISTRIBUCIONES POR R√âGIMEN =====\ndef test_distributions(data, regime_name):\n    \"\"\"Testa qu√© distribuci√≥n se ajusta mejor a los datos\"\"\"\n    \n    if len(data) < 10:\n        return f\"Datos insuficientes para {regime_name}\"\n    \n    results = {}\n    \n    # Test Poisson (solo para enteros positivos)\n    if regime_name == \"low\" and all(data >= 0) and all(data == data.astype(int)):\n        try:\n            poisson_param = stats.poisson.fit(data, method='MLE')[0]\n            poisson_ks = stats.kstest(data, lambda x: stats.poisson.cdf(x, poisson_param))\n            results['poisson'] = {\n                'param': poisson_param,\n                'ks_stat': poisson_ks.statistic,\n                'p_value': poisson_ks.pvalue\n            }\n        except:\n            results['poisson'] = {'error': 'No se pudo ajustar Poisson'}\n    \n    # Test Normal\n    try:\n        normal_params = stats.norm.fit(data)\n        normal_ks = stats.kstest(data, lambda x: stats.norm.cdf(x, *normal_params))\n        results['normal'] = {\n            'params': normal_params,\n            'ks_stat': normal_ks.statistic,\n            'p_value': normal_ks.pvalue\n        }\n    except:\n        results['normal'] = {'error': 'No se pudo ajustar Normal'}\n    \n    # Test Gamma (para datos positivos)\n    if all(data > 0):\n        try:\n            gamma_params = stats.gamma.fit(data)\n            gamma_ks = stats.kstest(data, lambda x: stats.gamma.cdf(x, *gamma_params))\n            results['gamma'] = {\n                'params': gamma_params,\n                'ks_stat': gamma_ks.statistic,\n                'p_value': gamma_ks.pvalue\n            }\n        except:\n            results['gamma'] = {'error': 'No se pudo ajustar Gamma'}\n    \n    return results\n\n# ===== 3. AN√ÅLISIS PARA UN PRODUCTO ESPEC√çFICO =====\ndef analyze_product_mixture(train1, product_name, country_name=None, threshold=74):\n    \"\"\"Analiza mixture distribution para un producto espec√≠fico\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"AN√ÅLISIS DE MIXTURE DISTRIBUTION\")\n    print(f\"Producto: {product_name}\")\n    if country_name:\n        print(f\"Pa√≠s: {country_name}\")\n    print(f\"Threshold: {threshold}\")\n    print(f\"{'='*60}\")\n    \n    # Filtrar datos\n    if country_name:\n        data = train1[(train1['product'] == product_name) & \n                     (train1['country'] == country_name)]['num_sold']\n    else:\n        data = train1[train1['product'] == product_name]['num_sold']\n    \n    if len(data) == 0:\n        print(\"No se encontraron datos para los filtros especificados\")\n        return\n    \n    # An√°lisis de threshold\n    analysis = analyze_threshold_split(data, threshold)\n    \n    print(f\"\\nRESULTADOS DE SEPARACI√ìN:\")\n    print(f\"Total observaciones: {analysis['total_count']}\")\n    print(f\"\\nR√âGIMEN BAJO (‚â§{threshold}):\")\n    print(f\"  Observaciones: {analysis['low_regime']['count']} ({analysis['low_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['low_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['low_regime']['std']:.2f}\")\n    \n    print(f\"\\nR√âGIMEN ALTO (>{threshold}):\")\n    print(f\"  Observaciones: {analysis['high_regime']['count']} ({analysis['high_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['high_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['high_regime']['std']:.2f}\")\n    \n    # Visualizaci√≥n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Histograma completo\n    axes[0,0].hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    axes[0,0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n    axes[0,0].set_title(f'Distribuci√≥n Completa - {product_name}')\n    axes[0,0].set_xlabel('Ventas')\n    axes[0,0].set_ylabel('Frecuencia')\n    axes[0,0].legend()\n    \n    # R√©gimen bajo con Poisson ajustada\n    if len(analysis['low_regime']['data']) > 0:\n        low_data = analysis['low_regime']['data']\n        n, bins, patches = axes[0,1].hist(low_data, bins=20, alpha=0.7, color='green', edgecolor='black', density=True)\n        \n        # Ajustar Poisson\n        poisson_lambda = low_data.mean()\n        x_poisson = np.arange(0, int(low_data.max()) + 1)\n        y_poisson = stats.poisson.pmf(x_poisson, poisson_lambda)\n        axes[0,1].plot(x_poisson, y_poisson, 'ro-', linewidth=2, markersize=4, label=f'Poisson(Œª={poisson_lambda:.2f})')\n        \n        axes[0,1].set_title(f'R√©gimen Bajo (‚â§{threshold}) vs Poisson')\n        axes[0,1].set_xlabel('Ventas')\n        axes[0,1].set_ylabel('Densidad')\n        axes[0,1].legend()\n    \n    # R√©gimen alto con Normal ajustada\n    if len(analysis['high_regime']['data']) > 0:\n        high_data = analysis['high_regime']['data']\n        n, bins, patches = axes[1,0].hist(high_data, bins=20, alpha=0.7, color='orange', edgecolor='black', density=True)\n        \n        # Ajustar Normal\n        normal_mu = high_data.mean()\n        normal_sigma = high_data.std()\n        x_normal = np.linspace(high_data.min(), high_data.max(), 100)\n        y_normal = stats.norm.pdf(x_normal, normal_mu, normal_sigma)\n        axes[1,0].plot(x_normal, y_normal, 'b-', linewidth=2, label=f'Normal(Œº={normal_mu:.1f}, œÉ={normal_sigma:.1f})')\n        \n        axes[1,0].set_title(f'R√©gimen Alto (>{threshold}) vs Normal')\n        axes[1,0].set_xlabel('Ventas')\n        axes[1,0].set_ylabel('Densidad')\n        axes[1,0].legend()\n    \n    # Comparaci√≥n de distribuciones\n    axes[1,1].hist(analysis['low_regime']['data'], bins=20, alpha=0.5, color='green', label=f'Bajo (‚â§{threshold})', density=True)\n    axes[1,1].hist(analysis['high_regime']['data'], bins=20, alpha=0.5, color='orange', label=f'Alto (>{threshold})', density=True)\n    axes[1,1].set_title('Comparaci√≥n de Reg√≠menes (Densidad)')\n    axes[1,1].set_xlabel('Ventas')\n    axes[1,1].set_ylabel('Densidad')\n    axes[1,1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Test distribuciones\n    if len(analysis['low_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - R√âGIMEN BAJO:\")\n        low_tests = test_distributions(analysis['low_regime']['data'], \"low\")\n        for dist_name, result in low_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'poisson':\n                    print(f\"    Lambda estimado: {result['param']:.2f}\")\n                elif dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    if len(analysis['high_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - R√âGIMEN ALTO:\")\n        high_tests = test_distributions(analysis['high_regime']['data'], \"high\")\n        for dist_name, result in high_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    return analysis\n\n# ===== 4. AN√ÅLISIS PARA M√öLTIPLES THRESHOLDS =====\ndef find_optimal_threshold(data, threshold_range=range(50, 100, 5)):\n    \"\"\"Encuentra el threshold √≥ptimo basado en criterios estad√≠sticos\"\"\"\n    \n    results = []\n    \n    for threshold in threshold_range:\n        analysis = analyze_threshold_split(data, threshold)\n        \n        # Criterios para evaluar la separaci√≥n\n        # 1. Balance en las proporciones (evitar que un grupo sea muy peque√±o)\n        balance_score = 1 - abs(0.5 - analysis['low_regime']['proportion'])\n        \n        # 2. Diferencia en medias (queremos que los grupos sean bien diferentes)\n        if analysis['high_regime']['count'] > 0 and analysis['low_regime']['count'] > 0:\n            mean_diff = abs(analysis['high_regime']['mean'] - analysis['low_regime']['mean'])\n        else:\n            mean_diff = 0\n        \n        # 3. Ratio de medias\n        if analysis['low_regime']['mean'] > 0:\n            mean_ratio = analysis['high_regime']['mean'] / analysis['low_regime']['mean']\n        else:\n            mean_ratio = 0\n        \n        results.append({\n            'threshold': threshold,\n            'low_prop': analysis['low_regime']['proportion'],\n            'high_prop': analysis['high_regime']['proportion'],\n            'low_mean': analysis['low_regime']['mean'],\n            'high_mean': analysis['high_regime']['mean'],\n            'mean_diff': mean_diff,\n            'mean_ratio': mean_ratio,\n            'balance_score': balance_score\n        })\n    \n    results_df = pd.DataFrame(results)\n    return results_df\n\n# ===== EJECUCI√ìN DEL AN√ÅLISIS =====\n\n# Obtener lista de productos\nproducts = train1['product'].unique()\ncountries = train1['country'].unique()\n\nprint(\"PRODUCTOS DISPONIBLES:\")\nfor i, product in enumerate(products):\n    print(f\"{i+1}. {product}\")\n\nprint(\"\\nPA√çSES DISPONIBLES:\")\nfor i, country in enumerate(countries):\n    print(f\"{i+1}. {country}\")\n\n# An√°lisis para el primer producto como ejemplo\nif len(products) > 0:\n    example_product = products[0]\n    example_country = countries[0]\n    \n    print(f\"\\nüîç AN√ÅLISIS DE EJEMPLO:\")\n    print(f\"Producto: {example_product}\")\n    print(f\"Pa√≠s: {example_country}\")\n    \n    # An√°lisis con threshold 74\n    analysis_74 = analyze_product_mixture(train1, example_product, example_country, threshold=74)\n    \n    # Buscar threshold √≥ptimo\n    product_data = train1[(train1['product'] == example_product) & \n                         (train1['country'] == example_country)]['num_sold']\n    \n    print(f\"\\nüéØ B√öSQUEDA DE THRESHOLD √ìPTIMO:\")\n    threshold_analysis = find_optimal_threshold(product_data)\n    \n    # Mostrar los mejores thresholds\n    threshold_analysis['score'] = (threshold_analysis['balance_score'] * \n                                  threshold_analysis['mean_diff'] / 100)\n    \n    best_thresholds = threshold_analysis.nlargest(3, 'score')\n    print(\"\\nTOP 3 THRESHOLDS:\")\n    for idx, row in best_thresholds.iterrows():\n        print(f\"  Threshold: {row['threshold']}\")\n        print(f\"    Proporci√≥n bajo: {row['low_prop']:.1%}, alto: {row['high_prop']:.1%}\")\n        print(f\"    Media bajo: {row['low_mean']:.1f}, alto: {row['high_mean']:.1f}\")\n        print(f\"    Ratio medias: {row['mean_ratio']:.2f}\")\n        print()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\n# ===== 1. AN√ÅLISIS DE SEPARACI√ìN CON THRESHOLD =====\ndef analyze_threshold_split(data, threshold=74):\n    \"\"\"Analiza la separaci√≥n de datos en dos grupos usando un threshold\"\"\"\n    \n    low_regime = data[data <= threshold]\n    high_regime = data[data > threshold]\n    \n    total_count = len(data)\n    low_count = len(low_regime)\n    high_count = len(high_regime)\n    \n    low_prop = low_count / total_count\n    high_prop = high_count / total_count\n    \n    results = {\n        'threshold': threshold,\n        'total_count': total_count,\n        'low_regime': {\n            'count': low_count,\n            'proportion': low_prop,\n            'mean': low_regime.mean() if len(low_regime) > 0 else 0,\n            'std': low_regime.std() if len(low_regime) > 0 else 0,\n            'data': low_regime\n        },\n        'high_regime': {\n            'count': high_count,\n            'proportion': high_prop,\n            'mean': high_regime.mean() if len(high_regime) > 0 else 0,\n            'std': high_regime.std() if len(high_regime) > 0 else 0,\n            'data': high_regime\n        }\n    }\n    \n    return results\n\n# ===== 2. TEST DE DISTRIBUCIONES POR R√âGIMEN =====\ndef test_distributions(data, regime_name):\n    \"\"\"Testa qu√© distribuci√≥n se ajusta mejor a los datos\"\"\"\n    \n    if len(data) < 10:\n        return f\"Datos insuficientes para {regime_name}\"\n    \n    results = {}\n    \n    # Test Poisson (solo para enteros positivos)\n    if regime_name == \"low\" and all(data >= 0) and all(data == data.astype(int)):\n        try:\n            poisson_param = stats.poisson.fit(data, method='MLE')[0]\n            poisson_ks = stats.kstest(data, lambda x: stats.poisson.cdf(x, poisson_param))\n            results['poisson'] = {\n                'param': poisson_param,\n                'ks_stat': poisson_ks.statistic,\n                'p_value': poisson_ks.pvalue\n            }\n        except:\n            results['poisson'] = {'error': 'No se pudo ajustar Poisson'}\n    \n    # Test Normal\n    try:\n        normal_params = stats.norm.fit(data)\n        normal_ks = stats.kstest(data, lambda x: stats.norm.cdf(x, *normal_params))\n        results['normal'] = {\n            'params': normal_params,\n            'ks_stat': normal_ks.statistic,\n            'p_value': normal_ks.pvalue\n        }\n    except:\n        results['normal'] = {'error': 'No se pudo ajustar Normal'}\n    \n    # Test Gamma (para datos positivos)\n    if all(data > 0):\n        try:\n            gamma_params = stats.gamma.fit(data)\n            gamma_ks = stats.kstest(data, lambda x: stats.gamma.cdf(x, *gamma_params))\n            results['gamma'] = {\n                'params': gamma_params,\n                'ks_stat': gamma_ks.statistic,\n                'p_value': gamma_ks.pvalue\n            }\n        except:\n            results['gamma'] = {'error': 'No se pudo ajustar Gamma'}\n    \n    return results\n\n# ===== 3. AN√ÅLISIS PARA UN PRODUCTO ESPEC√çFICO =====\ndef analyze_product_mixture(train1, product_name, country_name=None, threshold=74):\n    \"\"\"Analiza mixture distribution para un producto espec√≠fico\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"AN√ÅLISIS DE MIXTURE DISTRIBUTION\")\n    print(f\"Producto: {product_name}\")\n    if country_name:\n        print(f\"Pa√≠s: {country_name}\")\n    print(f\"Threshold: {threshold}\")\n    print(f\"{'='*60}\")\n    \n    # Filtrar datos\n    if country_name:\n        data = train1[(train1['product'] == product_name) & \n                     (train1['country'] == country_name)]['num_sold']\n    else:\n        data = train1[train1['product'] == product_name]['num_sold']\n    \n    if len(data) == 0:\n        print(\"No se encontraron datos para los filtros especificados\")\n        return\n    \n    # An√°lisis de threshold\n    analysis = analyze_threshold_split(data, threshold)\n    \n    print(f\"\\nRESULTADOS DE SEPARACI√ìN:\")\n    print(f\"Total observaciones: {analysis['total_count']}\")\n    print(f\"\\nR√âGIMEN BAJO (‚â§{threshold}):\")\n    print(f\"  Observaciones: {analysis['low_regime']['count']} ({analysis['low_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['low_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['low_regime']['std']:.2f}\")\n    \n    print(f\"\\nR√âGIMEN ALTO (>{threshold}):\")\n    print(f\"  Observaciones: {analysis['high_regime']['count']} ({analysis['high_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['high_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['high_regime']['std']:.2f}\")\n    \n    # Visualizaci√≥n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Histograma completo\n    axes[0,0].hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    axes[0,0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n    axes[0,0].set_title(f'Distribuci√≥n Completa - {product_name}')\n    axes[0,0].set_xlabel('Ventas')\n    axes[0,0].set_ylabel('Frecuencia')\n    axes[0,0].legend()\n    \n    # R√©gimen bajo con m√∫ltiples distribuciones\n    if len(analysis['low_regime']['data']) > 0:\n        low_data = analysis['low_regime']['data']\n        n, bins, patches = axes[0,1].hist(low_data, bins=20, alpha=0.7, color='green', edgecolor='black', density=True)\n        \n        # 1. Poisson\n        poisson_lambda = low_data.mean()\n        x_discrete = np.arange(0, int(low_data.max()) + 1)\n        y_poisson = stats.poisson.pmf(x_discrete, poisson_lambda)\n        axes[0,1].plot(x_discrete, y_poisson, 'ro-', linewidth=2, markersize=3, label=f'Poisson(Œª={poisson_lambda:.1f})')\n        \n        # 2. Binomial Negativa (usando m√©todo de momentos)\n        try:\n            mean_low = low_data.mean()\n            var_low = low_data.var()\n            if var_low > mean_low:  # Solo si hay sobredispersi√≥n\n                # Par√°metros por m√©todo de momentos: n, p\n                p_est = mean_low / var_low\n                n_est = mean_low * p_est / (1 - p_est)\n                if 0 < p_est < 1 and n_est > 0:\n                    y_nbinom = stats.nbinom.pmf(x_discrete, n_est, p_est)\n                    axes[0,1].plot(x_discrete, y_nbinom, 'bs-', linewidth=2, markersize=3, \n                                 label=f'Binomial Neg(n={n_est:.1f}, p={p_est:.2f})')\n        except:\n            pass\n        \n        # 3. Uniforme Discreta (aproximaci√≥n)\n        min_val = int(low_data.min())\n        max_val = int(low_data.max())\n        uniform_prob = 1.0 / (max_val - min_val + 1)\n        x_uniform = np.arange(min_val, max_val + 1)\n        y_uniform = np.full(len(x_uniform), uniform_prob)\n        axes[0,1].plot(x_uniform, y_uniform, 'm-', linewidth=3, alpha=0.8, \n                     label=f'Uniforme({min_val},{max_val})')\n        \n        # 4. Uniforme continua (para referencia visual)\n        x_cont = np.linspace(low_data.min(), low_data.max(), 100)\n        y_uniform_cont = stats.uniform.pdf(x_cont, low_data.min(), low_data.max() - low_data.min())\n        axes[0,1].plot(x_cont, y_uniform_cont, 'c--', linewidth=2, alpha=0.7, \n                     label=f'Uniforme Continua')\n        \n        axes[0,1].set_title(f'R√©gimen Bajo (‚â§{threshold}) - Comparaci√≥n Distribuciones')\n        axes[0,1].set_xlabel('Ventas')\n        axes[0,1].set_ylabel('Densidad')\n        axes[0,1].legend(fontsize=8)\n        axes[0,1].grid(True, alpha=0.3)\n    \n    # R√©gimen alto con Normal ajustada\n    if len(analysis['high_regime']['data']) > 0:\n        high_data = analysis['high_regime']['data']\n        n, bins, patches = axes[1,0].hist(high_data, bins=20, alpha=0.7, color='orange', edgecolor='black', density=True)\n        \n        # Ajustar Normal\n        normal_mu = high_data.mean()\n        normal_sigma = high_data.std()\n        x_normal = np.linspace(high_data.min(), high_data.max(), 100)\n        y_normal = stats.norm.pdf(x_normal, normal_mu, normal_sigma)\n        axes[1,0].plot(x_normal, y_normal, 'b-', linewidth=2, label=f'Normal(Œº={normal_mu:.1f}, œÉ={normal_sigma:.1f})')\n        \n        axes[1,0].set_title(f'R√©gimen Alto (>{threshold}) vs Normal')\n        axes[1,0].set_xlabel('Ventas')\n        axes[1,0].set_ylabel('Densidad')\n        axes[1,0].legend()\n    \n    # Comparaci√≥n de distribuciones\n    axes[1,1].hist(analysis['low_regime']['data'], bins=20, alpha=0.5, color='green', label=f'Bajo (‚â§{threshold})', density=True)\n    axes[1,1].hist(analysis['high_regime']['data'], bins=20, alpha=0.5, color='orange', label=f'Alto (>{threshold})', density=True)\n    axes[1,1].set_title('Comparaci√≥n de Reg√≠menes (Densidad)')\n    axes[1,1].set_xlabel('Ventas')\n    axes[1,1].set_ylabel('Densidad')\n    axes[1,1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Test distribuciones\n    if len(analysis['low_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - R√âGIMEN BAJO:\")\n        low_tests = test_distributions(analysis['low_regime']['data'], \"low\")\n        for dist_name, result in low_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'poisson':\n                    print(f\"    Lambda estimado: {result['param']:.2f}\")\n                elif dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    if len(analysis['high_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - R√âGIMEN ALTO:\")\n        high_tests = test_distributions(analysis['high_regime']['data'], \"high\")\n        for dist_name, result in high_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    return analysis\n\n# ===== 4. AN√ÅLISIS PARA M√öLTIPLES THRESHOLDS =====\ndef find_optimal_threshold(data, threshold_range=range(50, 100, 5)):\n    \"\"\"Encuentra el threshold √≥ptimo basado en criterios estad√≠sticos\"\"\"\n    \n    results = []\n    \n    for threshold in threshold_range:\n        analysis = analyze_threshold_split(data, threshold)\n        \n        # Criterios para evaluar la separaci√≥n\n        # 1. Balance en las proporciones (evitar que un grupo sea muy peque√±o)\n        balance_score = 1 - abs(0.5 - analysis['low_regime']['proportion'])\n        \n        # 2. Diferencia en medias (queremos que los grupos sean bien diferentes)\n        if analysis['high_regime']['count'] > 0 and analysis['low_regime']['count'] > 0:\n            mean_diff = abs(analysis['high_regime']['mean'] - analysis['low_regime']['mean'])\n        else:\n            mean_diff = 0\n        \n        # 3. Ratio de medias\n        if analysis['low_regime']['mean'] > 0:\n            mean_ratio = analysis['high_regime']['mean'] / analysis['low_regime']['mean']\n        else:\n            mean_ratio = 0\n        \n        results.append({\n            'threshold': threshold,\n            'low_prop': analysis['low_regime']['proportion'],\n            'high_prop': analysis['high_regime']['proportion'],\n            'low_mean': analysis['low_regime']['mean'],\n            'high_mean': analysis['high_regime']['mean'],\n            'mean_diff': mean_diff,\n            'mean_ratio': mean_ratio,\n            'balance_score': balance_score\n        })\n    \n    results_df = pd.DataFrame(results)\n    return results_df\n\n# ===== EJECUCI√ìN DEL AN√ÅLISIS =====\n\n# Obtener lista de productos\nproducts = train1['product'].unique()\ncountries = train1['country'].unique()\n\nprint(\"PRODUCTOS DISPONIBLES:\")\nfor i, product in enumerate(products):\n    print(f\"{i+1}. {product}\")\n\nprint(\"\\nPA√çSES DISPONIBLES:\")\nfor i, country in enumerate(countries):\n    print(f\"{i+1}. {country}\")\n\n# An√°lisis para el primer producto como ejemplo\nif len(products) > 0:\n    example_product = products[0]\n    example_country = countries[0]\n    \n    print(f\"\\nüîç AN√ÅLISIS DE EJEMPLO:\")\n    print(f\"Producto: {example_product}\")\n    print(f\"Pa√≠s: {example_country}\")\n    \n    # An√°lisis con threshold 74\n    analysis_74 = analyze_product_mixture(train1, example_product, example_country, threshold=74)\n    \n    # Buscar threshold √≥ptimo\n    product_data = train1[(train1['product'] == example_product) & \n                         (train1['country'] == example_country)]['num_sold']\n    \n    print(f\"\\nüéØ B√öSQUEDA DE THRESHOLD √ìPTIMO:\")\n    threshold_analysis = find_optimal_threshold(product_data)\n    \n    # Mostrar los mejores thresholds\n    threshold_analysis['score'] = (threshold_analysis['balance_score'] * \n                                  threshold_analysis['mean_diff'] / 100)\n    \n    best_thresholds = threshold_analysis.nlargest(3, 'score')\n    print(\"\\nTOP 3 THRESHOLDS:\")\n    for idx, row in best_thresholds.iterrows():\n        print(f\"  Threshold: {row['threshold']}\")\n        print(f\"    Proporci√≥n bajo: {row['low_prop']:.1%}, alto: {row['high_prop']:.1%}\")\n        print(f\"    Media bajo: {row['low_mean']:.1f}, alto: {row['high_mean']:.1f}\")\n        print(f\"    Ratio medias: {row['mean_ratio']:.2f}\")\n        print()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# se ve , regimen bajo =<74 66% es binomial negativa o \n#         regiment alto >74 34% es normal\n# mejor dicho se puede adepatar de esa manera\n# este caso es para Argentina con el producto \"Using LLMs to Improve Your Coding\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndef analyze_regime_threshold(data, threshold_candidates=range(10, 200, 10)):\n    \"\"\"Encuentra el mejor threshold para separar reg√≠menes (tu funci√≥n original).\"\"\"\n    results = []\n    for threshold in threshold_candidates:\n        low_regime = data[data <= threshold]\n        high_regime = data[data > threshold]\n        \n        if len(low_regime) < 10 or len(high_regime) < 10:\n            continue\n            \n        low_prop = len(low_regime) / len(data)\n        \n        # Criterios para evaluar la separaci√≥n\n        balance_score = 1 - abs(0.5 - low_prop)\n        mean_separation = abs(high_regime.mean() - low_regime.mean())\n        cv_low = low_regime.std() / low_regime.mean() if low_regime.mean() > 0 else 99\n        \n        # Score combinado\n        combined_score = (balance_score * 0.3 + \n                          (mean_separation / 100) * 0.5 + \n                          (1 / (1 + abs(cv_low - 1))) * 0.2)\n        \n        results.append({\n            'threshold': threshold,\n            'low_prop': low_prop,\n            'high_prop': 1 - low_prop,\n            'low_mean': low_regime.mean(),\n            'high_mean': high_regime.mean(),\n            'low_cv': cv_low,\n            'mean_separation': mean_separation,\n            'combined_score': combined_score\n        })\n    \n    if not results:\n        return None\n    \n    results_df = pd.DataFrame(results)\n    best_threshold = results_df.loc[results_df['combined_score'].idxmax(), 'threshold']\n    \n    return best_threshold, results_df\n\ndef test_regime_distributions(low_data, high_data):\n    \"\"\"Testa qu√© distribuciones se ajustan mejor a cada r√©gimen (tu funci√≥n original).\"\"\"\n    results = {'low_regime': {}, 'high_regime': {}}\n    \n    # Test r√©gimen bajo\n    if len(low_data) > 10:\n        # Poisson\n        try:\n            poisson_lambda = low_data.mean()\n            poisson_ks = stats.kstest(low_data, lambda x: stats.poisson.cdf(x, poisson_lambda))\n            results['low_regime']['poisson'] = {'lambda': poisson_lambda, 'ks_pvalue': poisson_ks.pvalue}\n        except: pass\n        \n        # Binomial Negativa\n        try:\n            mean_val, var_val = low_data.mean(), low_data.var()\n            if var_val > mean_val:\n                p_est = mean_val / var_val\n                n_est = mean_val * p_est / (1 - p_est)\n                if 0 < p_est < 1 and n_est > 0:\n                    nbinom_ks = stats.kstest(low_data, lambda x: stats.nbinom.cdf(x, n_est, p_est))\n                    results['low_regime']['nbinom'] = {'n': n_est, 'p': p_est, 'ks_pvalue': nbinom_ks.pvalue}\n        except: pass\n    \n    # Test r√©gimen alto\n    if len(high_data) > 10:\n        # Normal\n        try:\n            normal_mu, normal_sigma = stats.norm.fit(high_data)\n            normal_ks = stats.kstest(high_data, lambda x: stats.norm.cdf(x, normal_mu, normal_sigma))\n            results['high_regime']['normal'] = {'mu': normal_mu, 'sigma': normal_sigma, 'ks_pvalue': normal_ks.pvalue}\n        except: pass\n        \n    return results\n\ndef plot_regime_distributions(data, threshold, low_regime, high_regime):\n    \"\"\"\n    NUEVA FUNCI√ìN: Genera un gr√°fico 2x2 para visualizar los reg√≠menes y\n    comparar las distribuciones.\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle(f'An√°lisis de Distribuci√≥n con Threshold = {threshold}', fontsize=16)\n\n    # 1. Histograma completo\n    axes[0, 0].hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    axes[0, 0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n    axes[0, 0].set_title('Distribuci√≥n Completa de Ventas')\n    axes[0, 0].set_xlabel('Ventas')\n    axes[0, 0].set_ylabel('Frecuencia')\n    axes[0, 0].legend()\n\n    # 2. R√©gimen bajo vs. Distribuciones de Conteo\n    if not low_regime.empty:\n        axes[0, 1].hist(low_regime, bins=20, alpha=0.7, color='green', edgecolor='black', density=True, label='Datos Observados')\n        \n        # Curva Poisson\n        poisson_lambda = low_regime.mean()\n        x_poisson = np.arange(0, int(low_regime.max()) + 1)\n        y_poisson = stats.poisson.pmf(x_poisson, poisson_lambda)\n        axes[0, 1].plot(x_poisson, y_poisson, 'ro-', linewidth=2, markersize=4, label=f'Poisson (Œª={poisson_lambda:.2f})')\n        \n        # Curva Binomial Negativa\n        mean, var = low_regime.mean(), low_regime.var()\n        if var > mean:\n            p = mean / var\n            n = mean * p / (1 - p)\n            if 0 < p < 1 and n > 0:\n                y_nbinom = stats.nbinom.pmf(x_poisson, n, p)\n                axes[0, 1].plot(x_poisson, y_nbinom, 'bs-', linewidth=2, markersize=4, label=f'B. Negativa (n={n:.1f}, p={p:.2f})')\n\n        axes[0, 1].set_title(f'R√©gimen Bajo (‚â§{threshold})')\n        axes[0, 1].set_xlabel('Ventas')\n        axes[0, 1].set_ylabel('Densidad')\n        axes[0, 1].legend()\n\n    # 3. R√©gimen alto vs. Distribuci√≥n Normal\n    if not high_regime.empty:\n        axes[1, 0].hist(high_regime, bins=30, alpha=0.7, color='orange', edgecolor='black', density=True, label='Datos Observados')\n        \n        # Curva Normal\n        mu, sigma = stats.norm.fit(high_regime)\n        x_normal = np.linspace(high_regime.min(), high_regime.max(), 100)\n        y_normal = stats.norm.pdf(x_normal, mu, sigma)\n        axes[1, 0].plot(x_normal, y_normal, 'b-', linewidth=2, label=f'Normal (Œº={mu:.1f}, œÉ={sigma:.1f})')\n        \n        axes[1, 0].set_title(f'R√©gimen Alto (>{threshold})')\n        axes[1, 0].set_xlabel('Ventas')\n        axes[1, 0].set_ylabel('Densidad')\n        axes[1, 0].legend()\n\n    # 4. Comparaci√≥n de densidades\n    if not low_regime.empty:\n        low_regime.plot(kind='kde', ax=axes[1, 1], color='green', label=f'Bajo (‚â§{threshold})')\n    if not high_regime.empty:\n        high_regime.plot(kind='kde', ax=axes[1, 1], color='orange', label=f'Alto (>{threshold})')\n    axes[1, 1].set_title('Comparaci√≥n de Densidades (KDE)')\n    axes[1, 1].set_xlabel('Ventas')\n    axes[1, 1].legend()\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()\n\n\n# ===== AN√ÅLISIS COMPLETO POR PA√çS-PRODUCTO (MODIFICADO) =====\n\n# Cargar datos (necesitas tener tu dataframe 'train1' cargado)\n# train1 = pd.read_csv('tu_archivo.csv') # Ejemplo de carga\n\ncountries = train1['country'].unique()\nproducts = train1['product'].unique()\n\nregime_analysis_results = {}\noptimal_thresholds = {}\n\nprint(\"=== AN√ÅLISIS DE REG√çMENES Y VISUALIZACI√ìN ===\\n\")\n\nfor country in countries:\n    print(f\"{'='*60}\\nPA√çS: {country.upper()}\\n{'='*60}\")\n    \n    regime_analysis_results[country] = {}\n    optimal_thresholds[country] = {}\n    \n    for product in products:\n        print(f\"\\n--- Producto: {product} ---\")\n        \n        data = train1[(train1['country'] == country) & (train1['product'] == product)]['num_sold']\n        \n        if len(data) < 20: # Aumentamos el m√≠nimo para un an√°lisis robusto\n            print(f\"No hay datos suficientes ({len(data)} obs.)\")\n            continue\n        \n        print(f\"Total observaciones: {len(data)}, Rango: {data.min()}-{data.max()}, Media: {data.mean():.1f}\")\n        \n        # Encontrar threshold √≥ptimo\n        threshold_result = analyze_regime_threshold(data)\n        \n        if threshold_result is None:\n            print(\"No se pudo encontrar un threshold adecuado.\")\n            continue\n        \n        best_threshold, _ = threshold_result\n        optimal_thresholds[country][product] = best_threshold\n        \n        print(f\"\\nüéØ THRESHOLD √ìPTIMO ENCONTRADO: {best_threshold}\")\n        \n        # Separar en reg√≠menes\n        low_regime = data[data <= best_threshold]\n        high_regime = data[data > best_threshold]\n        \n        # Test de distribuciones\n        dist_results = test_regime_distributions(low_regime, high_regime)\n        regime_analysis_results[country][product] = {'threshold': best_threshold, 'distributions': dist_results}\n\n        # Mostrar mejor ajuste y generar gr√°fico\n        print(\"\\nüìä MEJOR AJUSTE DE DISTRIBUCIONES (basado en p-valor de KS test):\")\n        # (Aqu√≠ puedes agregar la l√≥gica para imprimir el mejor ajuste si lo deseas)\n\n        # ¬°NUEVO! Generar y mostrar el gr√°fico\n        plot_regime_distributions(data, best_threshold, low_regime, high_regime)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# el caso de argentina primero si ajusta bien ese producto , ahora hay algunos que si y otros no , el threshold va cambiando bastante\n# en algunos casos me lleva a pensar en otras distribuciones\n# para el primer modelo va sin regimen\n\n\n# abajo primero resumen de todos los efectos que se vieron y todos los cambios a efectuar para un modelo simple \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1. ESTACIONALIDAD SEMANAL\n# Factores de crecimiento semanal identificados\n\"\"\"\nweekend_boost = {\n    0: 1.00,  # Lunes (base)\n    1: 1.00,  # Martes  \n    2: 1.00,  # Mi√©rcoles\n    3: 1.00,  # Jueves\n    4: 1.06,  # Viernes (+5-7% vs jueves)\n    5: 1.09,  # S√°bado (+3-5% vs viernes) \n    6: 1.17   # Domingo (+6-8% vs s√°bado)\n}\ntrain1['weekend_factor'] = train1['DayOfWeek'].map(weekend_boost)\n\n# Features categ√≥ricas adicionales\ntrain1['is_weekend'] = (train1['DayOfWeek'] >= 5).astype(int)\ntrain1['is_friday'] = (train1['DayOfWeek'] == 4).astype(int)\n\"\"\"\n#2. EFECTOS FIJOS POR PA√çS (Multiplicadores constantes)\n\"\"\"\n# Multiplicadores identificados (Argentina = base)\ncountry_multipliers = {\n    'Argentina': 1.0,\n    'Canada': 4.1, \n    'Estonia': 2.1,\n    'Japan': 3.6,\n    'Spain': 2.6\n}\ntrain1['country_multiplier'] = train1['country'].map(country_multipliers)\n\n# One-hot encoding para efectos fijos\ntrain1 = pd.get_dummies(train1, columns=['country'], prefix='country')\n\"\"\"\n#3. EFECTOS FIJOS POR TIENDA (Multiplicadores identificados)\n\"\"\"\n# Multiplicadores por tienda (Kaggle Learn = base)\nstore_multipliers = {\n    'Kaggle Learn': 1.0,      # Base\n    'Kaggle Store': 1.54,     # 1.54x base\n    'Kagglazon': 5.67         # 5.67x base  \n}\ntrain1['store_multiplier'] = train1['store'].map(store_multipliers)\n\n# One-hot encoding para tiendas\ntrain1 = pd.get_dummies(train1, columns=['store'], prefix='store')\n\"\"\"\n#4. EFECTOS FIJOS POR PRODUCTO (Ranking consistente)\n\"\"\"\n# Ranking de productos (mismo orden en todos los pa√≠ses)\nproduct_ranking = {\n    'Using LLMs to Improve Your Coding': 1,\n    'Using LLMs to Train More LLMs': 2, \n    'Using LLMs to Win More Kaggle Competitions': 3,\n    'Using LLMs to Write Better': 4,\n    'Using LLMs to Win Friends and Influence People': 5\n}\ntrain1['product_rank'] = train1['product'].map(product_ranking)\n\n# Valores base por producto (usando Argentina + Kaggle Learn como referencia)\nproduct_base_values = {\n    'Using LLMs to Improve Your Coding': 82.3,\n    'Using LLMs to Train More LLMs': 81.2,\n    'Using LLMs to Win More Kaggle Competitions': 68.8, \n    'Using LLMs to Write Better': 62.6,\n    'Using LLMs to Win Friends and Influence People': 12.4\n}\ntrain1['product_base_value'] = train1['product'].map(product_base_values)\n\n# One-hot encoding\ntrain1 = pd.get_dummies(train1, columns=['product'], prefix='product')\n\"\"\"\n#5. FEATURES TEMPORALES\n\"\"\"\n# Tendencias temporales\ntrain1['year_trend'] = train1['Year'] - 2017  # 0,1,2,3,4\ntrain1['days_since_start'] = (train1['date'] - train1['date'].min()).dt.days\n\n# Seasonality dentro del a√±o\ntrain1['month_sin'] = np.sin(2 * np.pi * train1['Month'] / 12)\ntrain1['month_cos'] = np.cos(2 * np.pi * train1['Month'] / 12)\ntrain1['day_of_year'] = train1['date'].dt.dayofyear\ntrain1['quarter_sin'] = np.sin(2 * np.pi * train1['Quarter'] / 4)\ntrain1['quarter_cos'] = np.cos(2 * np.pi * train1['Quarter'] / 4)\n\"\"\"\n#6. MIXTURE MODEL FEATURES (R√©gimen identificado) no utilizado\n\n#7. FEATURES DE INTERACCI√ìN (Actualizadas con tienda)\n\"\"\"\n# Interacciones clave identificadas\ntrain1['country_store'] = train1['country_multiplier'] * train1['store_multiplier']\ntrain1['country_weekend'] = train1['country_multiplier'] * train1['weekend_factor']\ntrain1['store_weekend'] = train1['store_multiplier'] * train1['weekend_factor']\ntrain1['product_weekend'] = train1['product_base_value'] * train1['weekend_factor']\ntrain1['product_store'] = train1['product_base_value'] * train1['store_multiplier']\n\n# Interacci√≥n triple principal\ntrain1['country_store_product'] = (train1['country_multiplier'] * \n                                  train1['store_multiplier'] * \n                                  train1['product_base_value'])\n\n# Interacci√≥n a√±o-producto (por si hay trends espec√≠ficos)\nfor year in [2017, 2018, 2019, 2020, 2021]:\n    train1[f'product_rank_year_{year}'] = (train1['product_rank'] * \n                                          (train1['Year'] == year).astype(int))\n\"\"\"\n#8. LAG FEATURES (Series temporales)\n\"\"\"\n# Ordenar por entidad y fecha\ntrain1 = train1.sort_values(['country', 'store', 'product', 'date'])\n\n# Lags por entidad (pa√≠s-tienda-producto)\nfor lag in [1, 7, 30]:\n    train1[f'num_sold_lag_{lag}'] = (train1.groupby(['country', 'store', 'product'])\n                                    ['num_sold'].shift(lag))\n\n# Rolling statistics\nfor window in [7, 30]:\n    train1[f'num_sold_rolling_mean_{window}'] = (train1.groupby(['country', 'store', 'product'])\n                                                ['num_sold'].rolling(window).mean()\n                                                .reset_index(0, drop=True))\n    \n    train1[f'num_sold_rolling_std_{window}'] = (train1.groupby(['country', 'store', 'product'])\n                                               ['num_sold'].rolling(window).std()\n                                               .reset_index(0, drop=True))\n\"\"\"\n#9. FEATURES TARGET TRANSFORMADAS\n\"\"\"\n# Para el r√©gimen normal (alta demanda)\ntrain1['log_num_sold'] = np.log1p(train1['num_sold'])\ntrain1['sqrt_num_sold'] = np.sqrt(train1['num_sold'])\n\n# Para an√°lisis de residuos\ntrain1['residual_from_base'] = train1['num_sold'] - train1['expected_base']\n\"\"\"\n#10. FEATURES FINALES PARA MODELO\n\"\"\"\n# Lista de features para usar en el modelo\nbase_features = [\n    # Efectos fijos multiplicativos\n    'country_multiplier', 'store_multiplier', 'product_base_value', 'product_rank',\n    \n    # Estacionalidad \n    'weekend_factor', 'is_weekend', 'is_friday',\n    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n    \n    # Temporales\n    'year_trend', 'days_since_start',\n    \n    # Interacciones clave\n    'country_store', 'country_weekend', 'store_weekend', \n    'product_weekend', 'product_store', 'country_store_product',\n    \n    # Base esperada y desviaciones\n    'expected_base', 'deviation_from_expected',\n    \n    # Lags (si disponibles)\n    'num_sold_lag_1', 'num_sold_lag_7', 'num_sold_rolling_mean_7'\n]\n\n# Features categ√≥ricas (one-hot encoded)\ncategorical_features = [col for col in train1.columns \n                       if col.startswith(('country_', 'product_', 'store_'))]\n\n# Features finales\nmodel_features = base_features + categorical_features\n\"\"\"\n#Como quedaria\n#Expected_Sales = Argentina_Base √ó Country_Mult √ó Store_Mult √ó Product_Base √ó Weekend_Factor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  ESTACIONALIDAD SEMANAL\n# Factores de crecimiento semanal identificados\nweekend_boost = {\n    0: 1.00,  # Lunes (base)\n    1: 1.00,  # Martes  \n    2: 1.00,  # Mi√©rcoles\n    3: 1.00,  # Jueves\n    4: 1.06,  # Viernes (+5-7% vs jueves)\n    5: 1.09,  # S√°bado (+3-5% vs viernes) \n    6: 1.17   # Domingo (+6-8% vs s√°bado)\n}\ntrain1['weekend_factor'] = train1['DayOfWeek'].map(weekend_boost)\ntest1['weekend_factor'] = test1['DayOfWeek'].map(weekend_boost)\n# Features categ√≥ricas adicionales\ntrain1['is_weekend'] = (train1['DayOfWeek'] >= 5).astype(int)\ntrain1['is_friday'] = (train1['DayOfWeek'] == 4).astype(int)\ntest1['is_weekend'] = (test1['DayOfWeek'] >= 5).astype(int)\ntest1['is_friday'] = (test1['DayOfWeek'] == 4).astype(int)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EFECTOS FIJOS POR PA√çS\n# Multiplicadores identificados (Argentina = base)\ncountry_multipliers = {\n    'Argentina': 1.0,\n    'Canada': 4.1, \n    'Estonia': 2.1,\n    'Japan': 3.6,\n    'Spain': 2.6\n}\ntrain1['country_multiplier'] = train1['country'].map(country_multipliers)\ntest1['country_multiplier'] = test1['country'].map(country_multipliers)\n# One-hot encoding para efectos fijos\n#train1 = pd.get_dummies(train1, columns=['country'], prefix='country')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  EFECTOS FIJOS POR TIENDA\n# Multiplicadores por tienda (Kaggle Learn = base)\nstore_multipliers = {\n    'Kaggle Learn': 1.0,      # Base\n    'Kaggle Store': 1.54,     # 1.54x base\n    'Kagglazon': 5.67         # 5.67x base  \n}\ntrain1['store_multiplier'] = train1['store'].map(store_multipliers)\ntest1['store_multiplier'] = test1['store'].map(store_multipliers)\n# One-hot encoding para tiendas\n#train1 = pd.get_dummies(train1, columns=['store'], prefix='store')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  EFECTOS FIJOS POR PRODUCTO\n# Ranking de productos (mismo orden en todos los pa√≠ses)\nproduct_ranking = {\n    'Using LLMs to Improve Your Coding': 1,\n    'Using LLMs to Train More LLMs': 2, \n    'Using LLMs to Win More Kaggle Competitions': 3,\n    'Using LLMs to Write Better': 4,\n    'Using LLMs to Win Friends and Influence People': 5\n}\ntrain1['product_rank'] = train1['product'].map(product_ranking)\ntest1['product_rank'] = test1['product'].map(product_ranking)\n# Valores base por producto (usando Argentina + Kaggle Learn como referencia)\nproduct_base_values = {\n    'Using LLMs to Improve Your Coding': 82.3,\n    'Using LLMs to Train More LLMs': 81.2,\n    'Using LLMs to Win More Kaggle Competitions': 68.8, \n    'Using LLMs to Write Better': 62.6,\n    'Using LLMs to Win Friends and Influence People': 12.4\n}\ntrain1['product_base_value'] = train1['product'].map(product_base_values)\ntrain1['product_base_value'] = train1['product'].map(product_base_values)\ntest1['product_base_value'] = test1['product'].map(product_base_values)\ntest1['product_base_value'] = test1['product'].map(product_base_values)\n# One-hot encoding\n#train1 = pd.get_dummies(train1, columns=['product'], prefix='product')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURES TEMPORALES\n# Tendencias temporales\ntrain1['year_trend'] = train1['Year'] - 2017  # 0,1,2,3,4\ntrain1['days_since_start'] = (train1['date'] - train1['date'].min()).dt.days\ntest1['year_trend'] = test1['Year'] - 2017\ntrain_min_date = pd.to_datetime('2017-01-01')\ntest1['days_since_start'] = (test1['date'] - train_min_date).dt.days\n\n# Seasonality dentro del a√±o\ntrain1['month_sin'] = np.sin(2 * np.pi * train1['Month'] / 12)\ntrain1['month_cos'] = np.cos(2 * np.pi * train1['Month'] / 12)\ntrain1['day_of_year'] = train1['date'].dt.dayofyear\ntrain1['quarter_sin'] = np.sin(2 * np.pi * train1['Quarter'] / 4)\ntrain1['quarter_cos'] = np.cos(2 * np.pi * train1['Quarter'] / 4)\n\ntest1['month_sin'] = np.sin(2 * np.pi * test1['Month'] / 12)\ntest1['month_cos'] = np.cos(2 * np.pi * test1['Month'] / 12)\ntest1['day_of_year'] = test1['date'].dt.dayofyear\ntest1['quarter_sin'] = np.sin(2 * np.pi * test1['Quarter'] / 4)\ntest1['quarter_cos'] = np.cos(2 * np.pi * test1['Quarter'] / 4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#FEATURES DE INTERACCI√ìN\n# Interacciones clave identificadas\ntrain1['country_store'] = train1['country_multiplier'] * train1['store_multiplier']\ntrain1['country_weekend'] = train1['country_multiplier'] * train1['weekend_factor']\ntrain1['store_weekend'] = train1['store_multiplier'] * train1['weekend_factor']\ntrain1['product_weekend'] = train1['product_base_value'] * train1['weekend_factor']\ntrain1['product_store'] = train1['product_base_value'] * train1['store_multiplier']\n\ntest1['country_store'] = test1['country_multiplier'] * test1['store_multiplier']\ntest1['country_weekend'] = test1['country_multiplier'] * test1['weekend_factor']\ntest1['store_weekend'] = test1['store_multiplier'] * test1['weekend_factor']\ntest1['product_weekend'] = test1['product_base_value'] * train1['weekend_factor']\ntest1['product_store'] = test1['product_base_value'] * test1['store_multiplier']\n\n\n\n\n# Interacci√≥n triple principal\ntrain1['country_store_product'] = (train1['country_multiplier'] * \n                                  train1['store_multiplier'] * \n                                  train1['product_base_value'])\n\ntest1['country_store_product'] = (test1['country_multiplier'] * \n                                  test1['store_multiplier'] * \n                                  test1['product_base_value'])\n\n\n# Interacci√≥n a√±o-producto (por si hay trends espec√≠ficos)\nfor year in [2017, 2018, 2019, 2020, 2021]:\n    train1[f'product_rank_year_{year}'] = (train1['product_rank'] * \n                                          (train1['Year'] == year).astype(int))\n\nfor year in [2017, 2018, 2019, 2020, 2021, 2022]:\n        test1[f'product_rank_year_{year}'] = (test1['product_rank'] * \n                                                      (test1['Year'] == year).astype(int))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LAG FEATURES\n# Ordenar por entidad y fecha\ntrain1 = train1.sort_values(['country', 'store', 'product', 'date'])\n\n# Lags por entidad (pa√≠s-tienda-producto)\nfor lag in [1, 7, 30]:\n    train1[f'num_sold_lag_{lag}'] = (train1.groupby(['country', 'store', 'product'])\n                                    ['num_sold'].shift(lag))\n\n# Rolling statistics\nfor window in [7, 30]:\n    train1[f'num_sold_rolling_mean_{window}'] = (\n        train1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).mean())\n    )\n\n    train1[f'num_sold_rolling_std_{window}'] = (\n        train1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).std())\n    )\n\ntest1 = test1.sort_values(['country', 'store', 'product', 'date'])\n\n# Lags por entidad (pa√≠s-tienda-producto)\nfor lag in [1, 7, 30]:\n    test1[f'num_sold_lag_{lag}'] = (test1.groupby(['country', 'store', 'product'])\n                                    ['num_sold'].shift(lag))\n\n# Rolling statistics\nfor window in [7, 30]:\n    test1[f'num_sold_rolling_mean_{window}'] = (\n        test1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).mean())\n    )\n\n    test1[f'num_sold_rolling_std_{window}'] = (\n        test1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).std())\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURES TARGET TRANSFORMADAS\n# Para el r√©gimen normal (alta demanda)\ntrain1['log_num_sold'] = np.log1p(train1['num_sold']) \n#train1['sqrt_num_sold'] = np.sqrt(train1['num_sold']) no lo uso x ahora\n\n#one hot al final\n#train1 = pd.get_dummies(train1, columns=['country'], prefix='country')\n#train1 = pd.get_dummies(train1, columns=['store'], prefix='store')\n#train1 = pd.get_dummies(train1, columns=['product'], prefix='product')\n\n#test1 = pd.get_dummies(test1, columns=['country'], prefix='country')\n#test1 = pd.get_dummies(test1, columns=['store'], prefix='store')\n#test1 = pd.get_dummies(test1, columns=['product'], prefix='product')\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURES FINALES PARA MODELO\n# Lista de features para usar en el modelo\nbase_features = [\n    # Efectos fijos multiplicativos\n    'country_multiplier', 'store_multiplier', 'product_base_value', 'product_rank',\n    \n    # Estacionalidad \n    'weekend_factor', 'is_weekend', 'is_friday',\n    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n    \n    # Temporales\n    'year_trend', 'days_since_start',\n    \n    # Interacciones clave\n    'country_store', 'country_weekend', 'store_weekend', \n    'product_weekend', 'product_store', 'country_store_product',\n    \n    # Base esperada y desviaciones\n    'expected_base', 'deviation_from_expected',\n    \n    # Lags (si disponibles)\n    'num_sold_lag_1', 'num_sold_lag_7', 'num_sold_rolling_mean_7'\n]\n\n# Features categ√≥ricas (one-hot encoded)\ncategorical_features = [col for col in train1.columns \n                       if col.startswith(('country_', 'product_', 'store_'))]\n\n# Features finales\nmodel_features = base_features + categorical_features\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test1.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#eliminar los NaN/ reemplzar por 0\n# no saque la columna ID que no aporta poder predictivo\n\nfeatures_con_nans = [col for col in train1.columns if 'lag' in col or 'rolling' in col]\ntrain1[features_con_nans] = train1[features_con_nans].fillna(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nmodel_features = [\n    'country_multiplier', 'store_multiplier', 'product_base_value', 'product_rank',\n    'weekend_factor', 'is_weekend', 'is_friday',\n    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n    'year_trend', 'days_since_start',\n    'country_store', 'country_weekend', 'store_weekend', \n    'product_weekend', 'product_store', 'country_store_product',\n    'num_sold_lag_1', 'num_sold_lag_7', 'num_sold_lag_30',\n    'num_sold_rolling_mean_7', 'num_sold_rolling_std_7',\n    'num_sold_rolling_mean_30', 'num_sold_rolling_std_30'\n]\n\nX = train1[model_features]\ny = train1['log_num_sold']\n\nX_test = test1[model_features]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  fecha de corte voy a usar 3 meses solamente de validacion\nfecha_corte = '2021-09-30'\nindices_train = train1[train1['date'] < fecha_corte].index\nindices_val = train1[train1['date'] >= fecha_corte].index\n\n# DATA LEAKEAGE , despues termine viendo que las variables de lag y rolling producieron data LEAKEAGE\n# asi que lo que probe en el test de validacion esta mal ,\n# Separar X e y\nX_train, X_val = X.loc[indices_train], X.loc[indices_val]\ny_train, y_val = y.loc[indices_train], y.loc[indices_val]\n\nprint(f\"Datos de entrenamiento: {len(X_train)} filas\")\nprint(f\"Datos de validaci√≥n: {len(X_val)} filas\")\n\"\"\"\n# Valores prediciendo los ultimos 3 meses del 2021\n\n[LightGBM] [Info] Total Bins 2271\n[LightGBM] [Info] Number of data points in the train set: 129975, number of used features: 26\n[LightGBM] [Info] Start training from score 4.534541\nError del modelo (RMSE) en el conjunto de validaci√≥n: 21.39\nError del modelo (SMAPE): 5.41%\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_reg = lgb.LGBMRegressor(random_state=1722, n_jobs=-1)\nlgbm_reg.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# Hacer predicciones sobre el conjunto de validaci√≥n\nlog_predicciones = lgbm_reg.predict(X_val)\n\n# Convertir las predicciones de vuelta a la escala original\n# Si tu target 'y' era log_num_sold, us√° np.expm1()\npredicciones_finales = np.expm1(log_predicciones)\n\n# Obtener los valores reales tambi√©n en la escala original\nvalores_reales = np.expm1(y_val)\n\n\nrmse = np.sqrt(mean_squared_error(valores_reales, predicciones_finales))\nprint(f\"\\nError del modelo (RMSE) en el conjunto de validaci√≥n: {rmse:.2f}\")\n\n\nplt.figure(figsize=(15, 6))\nplt.plot(train1.loc[indices_val, 'date'], valores_reales, label='Valores Reales', alpha=0.7)\nplt.plot(train1.loc[indices_val, 'date'], predicciones_finales, label='Predicciones del Modelo', linestyle='--')\nplt.title('Comparaci√≥n de Valores Reales vs. Predicciones')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0  \n    return np.mean(diff) * 100\n# Calcular SMAPE\nsmape_error = smape(valores_reales, predicciones_finales)\nprint(f\"Error del modelo (SMAPE): {smape_error:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Modelo para enviar a kaggle\n# voy a tener que ir prediciendo de a 1 mes por la forma en que arme mis variables\n\nlgbm_1 = lgb.LGBMRegressor(random_state=1722, n_jobs=-1)\nlgbm_1.fit(X, y)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dividir X_test por meses usando days_since_start\nmonthly_tests = {}\ninicio_2022 = 1795  # tu valor m√≠nimo\ndias_acumulados = 0\ndias_por_mes = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\nfor month in range(1, 13):\n   inicio_mes = inicio_2022 + dias_acumulados\n   fin_mes = inicio_2022 + dias_acumulados + dias_por_mes[month-1] - 1\n   \n   mask = (X_test['days_since_start'] >= inicio_mes) & (X_test['days_since_start'] <= fin_mes)\n   monthly_tests[f'month_{month}'] = X_test[mask]\n   \n   dias_acumulados += dias_por_mes[month-1]\n\n# Inicializar historial con datos reales hasta diciembre 2021\nhistorial = train1.copy()\npredicciones_finales = []\n\n# Loop iterativo\nfor month in range(1, 13):\n    print(f\"=== MES {month} ===\")\n    \n    # Predecir mes actual\n    X_mes_actual = monthly_tests[f'month_{month}']\n    print(f\"Filas a predecir: {len(X_mes_actual)}\")\n    \n    pred_mes = lgbm_1.predict(X_mes_actual)\n    print(f\"Predicciones generadas: {len(pred_mes)}\")\n    print(f\"Ejemplo predicciones: {pred_mes[:5]}\")  # primeras 5\n    \n    # Agregar a lista final\n    predicciones_finales.extend(pred_mes)\n    \n    # Agregar predicciones al historial para pr√≥ximos lags\n    # (actualizar historial con pred_mes) - POR IMPLEMENTAR\n    \n    # Recalcular lags para pr√≥ximo mes  \n    # (calcular nuevos lags usando historial actualizado) - POR IMPLEMENTAR\n\nprint(f\"\\n=== RESUMEN FINAL ===\")  \nprint(f\"Total predicciones: {len(predicciones_finales)}\")\nprint(f\"Primeras 10 predicciones: {predicciones_finales[:10]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Aplicar exponencial a las predicciones\npredicciones_finales_exp = [np.exp(pred) for pred in predicciones_finales]\n\n# 2. Crear DataFrame con las predicciones\npred_df = pd.DataFrame({\n    'num_sold': predicciones_finales_exp\n})\n\n# 3. Agregar las columnas originales desde test1\npred_df_completo = pd.concat([\n    test1[['date', 'country', 'store', 'product']].reset_index(drop=True),\n    pred_df\n], axis=1)\n\n# 4. Convertir fechas al mismo formato antes del merge\ntest_merge = test[['id', 'date', 'country', 'store', 'product']].copy()\ntest_merge['date'] = pd.to_datetime(test_merge['date'])\npred_df_completo['date'] = pd.to_datetime(pred_df_completo['date'])\n\n# 5. Hacer merge\nsubmission = test_merge.merge(\n    pred_df_completo,\n    on=['date', 'country', 'store', 'product'],\n    how='left'\n)\n\n# 6. Submission final\nsubmission_final = submission[['id', 'num_sold']]\nprint(f\"Submission shape: {submission_final.shape}\")\nprint(submission_final.head())\n\n# 7. Guardar para Kaggle\nsubmission_final.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {submission_final.shape}\")\nprint(f\"Columnas: {submission_final.columns.tolist()}\")\nprint(f\"Tipos: {submission_final.dtypes}\")\nprint(f\"Valores nulos: {submission_final.isnull().sum()}\")\nprint(submission_final.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#podria hacer ensemble/DNN y hacer que prediga 75 variables entonces es analizar cada pais/tienda/producto y ya sea hacer una variable aleatoria de c/u\n# osea ejemplo ,para ARG/TIENDA1/Producto 1 , y esto es una variable conocida o combinacion o hago que la red aprenda como esta distribuido en los \n# cuartiles y asi la forme , pero la pag hoy no esta andando bien y no deja ver como quedo en relacion al leaderboard(fijo)\n# tmb en este mismo notebook quedaron muchas cosas por probar , con las variables one-hot y mas","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}