{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57094,"databundleVersionId":6197974,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.kaggle.com/competitions/forecasting-mini-course-sales/overview\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\nfrom sklearn.preprocessing import (\n    StandardScaler, MinMaxScaler, RobustScaler,\n    LabelEncoder, OneHotEncoder, OrdinalEncoder,\n    PowerTransformer, QuantileTransformer\n)\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.model_selection import (\n    train_test_split, cross_val_score, cross_validate,\n    StratifiedKFold, KFold, GridSearchCV, RandomizedSearchCV,\n    validation_curve, learning_curve\n)\n\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier, ExtraTreesClassifier,\n    GradientBoostingClassifier, AdaBoostClassifier,\n    VotingClassifier, BaggingClassifier\n)\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\n\nfrom sklearn.linear_model import (\n    LinearRegression, Ridge, Lasso, ElasticNet,\n    SGDRegressor, BayesianRidge, HuberRegressor\n)\nfrom sklearn.ensemble import (\n    RandomForestRegressor, ExtraTreesRegressor,\n    GradientBoostingRegressor, AdaBoostRegressor,\n    VotingRegressor, BaggingRegressor\n)\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\n\n\nfrom sklearn.decomposition import PCA, TruncatedSVD, FastICA\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_selection import (\n    SelectKBest, SelectFromModel, RFE, RFECV,\n    chi2, f_classif, f_regression, mutual_info_classif\n)\n\n\nfrom sklearn.metrics import (\n    # Clasificación\n    accuracy_score, precision_score, recall_score, f1_score,\n    classification_report, confusion_matrix, roc_auc_score,\n    roc_curve, precision_recall_curve, auc, log_loss,\n    \n    # Regresión\n    mean_squared_error, mean_absolute_error, r2_score,\n    mean_squared_log_error, mean_absolute_percentage_error,\n    \n    # Clustering\n    silhouette_score, adjusted_rand_score, calinski_harabasz_score\n)\n\ntry:\n    import xgboost as xgb\n    print(\"✅ XGBoost disponible\")\nexcept ImportError:\n    print(\"❌ XGBoost no instalado\")\n\ntry:\n    import lightgbm as lgb\n    print(\"✅ LightGBM disponible\")\nexcept ImportError:\n    print(\"❌ LightGBM no instalado\")\n\ntry:\n    from catboost import CatBoostClassifier, CatBoostRegressor\n    print(\"✅ CatBoost disponible\")\nexcept ImportError:\n    print(\"❌ CatBoost no instalado\")\n\n\ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n    print(\"✅ TensorFlow disponible\")\nexcept ImportError:\n    print(\"❌ TensorFlow no instalado\")\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    print(\"✅ PyTorch disponible\")\nexcept ImportError:\n    print(\"❌ PyTorch no instalado\")\n\n\ntry:\n    import spacy\n    print(\"✅ SpaCy disponible\")\nexcept ImportError:\n    print(\"❌ SpaCy no instalado\")\n\n\ntry:\n    import scipy.stats as stats\n    from scipy import stats\n    print(\"✅ SciPy disponible\")\nexcept ImportError:\n    print(\"❌ SciPy no instalado\")\n\ntry:\n    import statsmodels.api as sm\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    print(\"✅ Statsmodels disponible\")\nexcept ImportError:\n    print(\"❌ Statsmodels no instalado\")\n\n\ntry:\n    from prophet import Prophet\n    print(\"✅ Prophet disponible\")\nexcept ImportError:\n    print(\"❌ Prophet no instalado\")\n\n\ntry:\n    import optuna\n    print(\"✅ Optuna disponible\")\nexcept ImportError:\n    print(\"❌ Optuna no instalado\")\n\n\nimport os\nimport sys\nimport json\nimport pickle\nimport joblib\nfrom datetime import datetime, timedelta\nimport itertools\nfrom collections import Counter\nimport gc\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.4f}'.format)\npd.set_option('display.width', None)\n\n\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\n\nsns.set_palette(\"husl\")\nsns.set_style(\"whitegrid\")\n\n\nnp.random.seed(1722)\n\n\ndef info_basica(df):\n    \"\"\"Función para explorar un dataset rápidamente\"\"\"\n    print(\"=\" * 50)\n    print(\"INFORMACIÓN BÁSICA DEL DATASET\")\n    print(\"=\" * 50)\n    print(f\"Forma del dataset: {df.shape}\")\n    print(f\"Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    print(\"\\nTipos de datos:\")\n    print(df.dtypes.value_counts())\n    print(\"\\nValores nulos:\")\n    print(df.isnull().sum().sort_values(ascending=False))\n    print(\"\\nValores duplicados:\", df.duplicated().sum())\n    print(\"\\nPrimeras 5 filas:\")\n    print(df.head())\n\ndef plot_distribucion(df, columna, tipo='auto'):\n    \"\"\"Plotea la distribución de una columna\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    if tipo == 'auto':\n        tipo = 'continua' if df[columna].dtype in ['float64', 'int64'] else 'categorica'\n    \n    if tipo == 'continua':\n        # Histograma\n        ax1.hist(df[columna].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n        ax1.set_title(f'Distribución de {columna}')\n        ax1.set_xlabel(columna)\n        ax1.set_ylabel('Frecuencia')\n        \n        # Boxplot\n        ax2.boxplot(df[columna].dropna())\n        ax2.set_title(f'Boxplot de {columna}')\n        ax2.set_ylabel(columna)\n    else:\n        # Gráfico de barras\n        df[columna].value_counts().plot(kind='bar', ax=ax1, color='lightcoral')\n        ax1.set_title(f'Distribución de {columna}')\n        ax1.set_xlabel(columna)\n        ax1.set_ylabel('Frecuencia')\n        ax1.tick_params(axis='x', rotation=45)\n        \n        # Gráfico de pie\n        df[columna].value_counts().plot(kind='pie', ax=ax2, autopct='%1.1f%%')\n        ax2.set_title(f'Proporción de {columna}')\n        ax2.set_ylabel('')\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e19/train.csv') \ntest  = pd.read_csv(\"/kaggle/input/playground-series-s3e19/test.csv\")\ntest_orig = pd.read_csv(\"/kaggle/input/playground-series-s3e19/test.csv\")\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# agrego 1 mes de datos a test que despues voy a usar\n\ndecember_2021 = train[(train['date'] >= '2021-12-01') & (train['date'] <= '2021-12-31')]\ntest = pd.concat([december_2021, test], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info_basica(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info_basica(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# num_sold lo que tengo que predecir para 27375 filas osea (27375/365) = 75 id cada dia\n# 5 paises , 3 tiendas, 5 productos ,(136950/1826) = 75 id para cada dia , (75/5) = 15 de cada pais ,\n# cada tienda vende 5 productos en cada pais cada dia (5 * 3 * 5) = 75 id , productos = (136950/27390) = 5\n# fecha desde 01-01-2017 a 31-12-2021 5 años * 365 = 1825 , 1826 por un año bisiesto\n# predecir todo el año de ventas del 2022\n\n#por como esta hecha toda sintetica la info lo primero que se me viene en agrupar informacion y enfocar el modelo\n#en predecir distribuciones sea nivel pais o dia o tienda o producto , ver si sigue alguna distrib de conteo conocida\n# y comparar si año a año cambia\n\ntrain.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.describe(include=\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train[\"date\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test[\"date\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1 = train.copy()\ntest1  = test.copy()\n\n# date lo separo \n\ntrain1['date'] = pd.to_datetime(train1['date'])\ntest1['date'] = pd.to_datetime(test1['date'])\n\n\ntrain1['Year'] = train1['date'].dt.year\ntrain1['Quarter'] = train1['date'].dt.quarter\ntrain1['Month'] = train1['date'].dt.month\ntrain1['Day'] = train1['date'].dt.day\ntrain1['DayOfWeek'] = train1['date'].dt.dayofweek  # 0=Lunes, 6=Domingo\n\ntest1['Year'] = test1['date'].dt.year\ntest1['Quarter'] = test1['date'].dt.quarter\ntest1['Month'] = test1['date'].dt.month\ntest1['Day'] = test1['date'].dt.day\ntest1['DayOfWeek'] = test1['date'].dt.dayofweek  # 0=Lunes, 6=Domingo\n\ntrain1.head(76)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# voy a prodecer a ver distintos tipos de agrupaciones y estadisticas , para ver de que forma me conviene\n# hacer el modelo , si predecir de arriba para abajo o  al reves\n\n\n# Distribuciones a diferentes niveles de agregación\ndaily_sales = train1.groupby('date')['num_sold'].sum()\ncountry_daily = train1.groupby(['date', 'country'])['num_sold'].sum()\nstore_daily = train1.groupby(['date', 'country', 'store'])['num_sold'].sum()\nproduct_daily = train1.groupby(['date', 'product'])['num_sold'].sum()\n\ndaily_sales\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Crear subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Distribuciones de Ventas por Nivel de Agregación', fontsize=16)\n\n# 1. Ventas totales diarias\naxes[0,0].hist(daily_sales, bins=30, alpha=0.7, color='blue', edgecolor='black')\naxes[0,0].set_title(f'Ventas Totales Diarias\\nMedia: {daily_sales.mean():.1f}, Std: {daily_sales.std():.1f}')\naxes[0,0].set_xlabel('Ventas Totales por Día')\naxes[0,0].set_ylabel('Frecuencia')\n\n# 2. Ventas por país-día\naxes[0,1].hist(country_daily, bins=30, alpha=0.7, color='green', edgecolor='black')\naxes[0,1].set_title(f'Ventas por País-Día\\nMedia: {country_daily.mean():.1f}, Std: {country_daily.std():.1f}')\naxes[0,1].set_xlabel('Ventas por País-Día')\naxes[0,1].set_ylabel('Frecuencia')\n\n# 3. Ventas por tienda-día\naxes[1,0].hist(store_daily, bins=30, alpha=0.7, color='orange', edgecolor='black')\naxes[1,0].set_title(f'Ventas por Tienda-Día\\nMedia: {store_daily.mean():.1f}, Std: {store_daily.std():.1f}')\naxes[1,0].set_xlabel('Ventas por Tienda-Día')\naxes[1,0].set_ylabel('Frecuencia')\n\n# 4. Ventas por producto-día\naxes[1,1].hist(product_daily, bins=30, alpha=0.7, color='red', edgecolor='black')\naxes[1,1].set_title(f'Ventas por Producto-Día\\nMedia: {product_daily.mean():.1f}, Std: {product_daily.std():.1f}')\naxes[1,1].set_xlabel('Ventas por Producto-Día')\naxes[1,1].set_ylabel('Frecuencia')\n\nplt.tight_layout()\nplt.show()\n\n# Estadísticas descriptivas\nprint(\"=== ESTADÍSTICAS DESCRIPTIVAS ===\")\nprint(f\"\\n1. VENTAS TOTALES DIARIAS (75 registros/día):\")\nprint(f\"   Min: {daily_sales.min()}, Max: {daily_sales.max()}\")\nprint(f\"   Media: {daily_sales.mean():.2f}, Mediana: {daily_sales.median():.2f}\")\nprint(f\"   Coef. Variación: {daily_sales.std()/daily_sales.mean():.3f}\")\n\nprint(f\"\\n2. VENTAS POR PAÍS-DÍA (15 registros/país/día):\")\nprint(f\"   Min: {country_daily.min()}, Max: {country_daily.max()}\")\nprint(f\"   Media: {country_daily.mean():.2f}, Mediana: {country_daily.median():.2f}\")\nprint(f\"   Coef. Variación: {country_daily.std()/country_daily.mean():.3f}\")\n\nprint(f\"\\n3. VENTAS POR TIENDA-DÍA (5 registros/tienda/día):\")\nprint(f\"   Min: {store_daily.min()}, Max: {store_daily.max()}\")\nprint(f\"   Media: {store_daily.mean():.2f}, Mediana: {store_daily.median():.2f}\")\nprint(f\"   Coef. Variación: {store_daily.std()/store_daily.mean():.3f}\")\n\nprint(f\"\\n4. VENTAS POR PRODUCTO-DÍA:\")\nprint(f\"   Min: {product_daily.min()}, Max: {product_daily.max()}\")\nprint(f\"   Media: {product_daily.mean():.2f}, Mediana: {product_daily.median():.2f}\")\nprint(f\"   Coef. Variación: {product_daily.std()/product_daily.mean():.3f}\")\n\n# Ver la distribución individual (nivel más granular)\nprint(f\"\\n5. VENTAS INDIVIDUALES (nivel más granular):\")\nprint(f\"   Min: {train1['num_sold'].min()}, Max: {train1['num_sold'].max()}\")\nprint(f\"   Media: {train1['num_sold'].mean():.2f}, Mediana: {train1['num_sold'].median():.2f}\")\nprint(f\"   Coef. Variación: {train1['num_sold'].std()/train1['num_sold'].mean():.3f}\")\n\n# Histograma adicional para el nivel individual\nplt.figure(figsize=(10, 6))\nplt.hist(train1['num_sold'], bins=50, alpha=0.7, color='purple', edgecolor='black')\nplt.title(f'Distribución de Ventas Individuales\\nMedia: {train1[\"num_sold\"].mean():.1f}, Std: {train1[\"num_sold\"].std():.1f}')\nplt.xlabel('Número de Ventas')\nplt.ylabel('Frecuencia')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crear las agregaciones por año\nyears = [2017, 2018, 2019, 2020, 2021]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# 1. VENTAS POR TIENDA-DÍA POR AÑO\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Distribuciones de Ventas por Tienda-Día por Año', fontsize=16)\n\nfor i, year in enumerate(years):\n    year_data = train1[train1['Year'] == year]\n    store_daily_year = year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n    \n    row = i // 3\n    col = i % 3\n    \n    axes[row, col].hist(store_daily_year, bins=25, alpha=0.7, color=colors[i], edgecolor='black')\n    axes[row, col].set_title(f'Año {year}\\nMedia: {store_daily_year.mean():.1f}, Std: {store_daily_year.std():.1f}')\n    axes[row, col].set_xlabel('Ventas por Tienda-Día')\n    axes[row, col].set_ylabel('Frecuencia')\n    \n    # Agregar línea vertical en la media\n    axes[row, col].axvline(store_daily_year.mean(), color='black', linestyle='--', alpha=0.8)\n\n# Remover el subplot extra\naxes[1, 2].remove()\n\nplt.tight_layout()\nplt.show()\n\n# 2. VENTAS TOTALES DIARIAS POR AÑO\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Distribuciones de Ventas Totales Diarias por Año', fontsize=16)\n\nfor i, year in enumerate(years):\n    year_data = train1[train1['Year'] == year]\n    daily_sales_year = year_data.groupby('date')['num_sold'].sum()\n    \n    row = i // 3\n    col = i % 3\n    \n    axes[row, col].hist(daily_sales_year, bins=25, alpha=0.7, color=colors[i], edgecolor='black')\n    axes[row, col].set_title(f'Año {year}\\nMedia: {daily_sales_year.mean():.1f}, Std: {daily_sales_year.std():.1f}')\n    axes[row, col].set_xlabel('Ventas Totales Diarias')\n    axes[row, col].set_ylabel('Frecuencia')\n    \n    # Agregar línea vertical en la media\n    axes[row, col].axvline(daily_sales_year.mean(), color='black', linestyle='--', alpha=0.8)\n\n# Remover el subplot extra\naxes[1, 2].remove()\n\nplt.tight_layout()\nplt.show()\n\n# 3. COMPARACIÓN DE ESTADÍSTICAS POR AÑO\nprint(\"=== EVOLUCIÓN DE ESTADÍSTICAS POR AÑO ===\")\nprint(\"\\n1. VENTAS POR TIENDA-DÍA:\")\nprint(\"Año\\tMedia\\tStd\\tCoef.Var\\tMin\\tMax\")\nprint(\"-\" * 50)\n\nstore_stats = []\nfor year in years:\n    year_data = train1[train1['Year'] == year]\n    store_daily_year = year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n    \n    mean_val = store_daily_year.mean()\n    std_val = store_daily_year.std()\n    cv = std_val / mean_val\n    min_val = store_daily_year.min()\n    max_val = store_daily_year.max()\n    \n    store_stats.append([year, mean_val, std_val, cv, min_val, max_val])\n    print(f\"{year}\\t{mean_val:.1f}\\t{std_val:.1f}\\t{cv:.3f}\\t\\t{min_val}\\t{max_val}\")\n\nprint(\"\\n2. VENTAS TOTALES DIARIAS:\")\nprint(\"Año\\tMedia\\tStd\\tCoef.Var\\tMin\\tMax\")\nprint(\"-\" * 50)\n\ndaily_stats = []\nfor year in years:\n    year_data = train1[train1['Year'] == year]\n    daily_sales_year = year_data.groupby('date')['num_sold'].sum()\n    \n    mean_val = daily_sales_year.mean()\n    std_val = daily_sales_year.std()\n    cv = std_val / mean_val\n    min_val = daily_sales_year.min()\n    max_val = daily_sales_year.max()\n    \n    daily_stats.append([year, mean_val, std_val, cv, min_val, max_val])\n    print(f\"{year}\\t{mean_val:.1f}\\t{std_val:.1f}\\t{cv:.3f}\\t\\t{min_val}\\t{max_val}\")\n\n# 4. GRÁFICOS DE EVOLUCIÓN TEMPORAL\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Evolución de medias\nstore_means = [stats[1] for stats in store_stats]\ndaily_means = [stats[1] for stats in daily_stats]\n\naxes[0].plot(years, store_means, 'o-', color='blue', linewidth=2, markersize=8, label='Tienda-Día')\naxes[0].plot(years, daily_means, 's-', color='red', linewidth=2, markersize=8, label='Total Diario')\naxes[0].set_title('Evolución de Medias por Año')\naxes[0].set_xlabel('Año')\naxes[0].set_ylabel('Media de Ventas')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Evolución de coeficientes de variación\nstore_cvs = [stats[3] for stats in store_stats]\ndaily_cvs = [stats[3] for stats in daily_stats]\n\naxes[1].plot(years, store_cvs, 'o-', color='blue', linewidth=2, markersize=8, label='Tienda-Día')\naxes[1].plot(years, daily_cvs, 's-', color='red', linewidth=2, markersize=8, label='Total Diario')\naxes[1].set_title('Evolución de Coeficiente de Variación por Año')\naxes[1].set_xlabel('Año')\naxes[1].set_ylabel('Coeficiente de Variación')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 5. TEST DE NORMALIDAD BÁSICO (opcional)\nfrom scipy import stats as scipy_stats\n\nprint(\"\\n=== TESTS DE FORMA DE DISTRIBUCIÓN ===\")\nprint(\"\\nPara detectar si las distribuciones cambian de forma:\")\nprint(\"- Si Coef.Var ≈ 1: podría ser Poisson\")\nprint(\"- Si Coef.Var > 1: podría ser Binomial Negativa\")\nprint(\"- Si Coef.Var << 1: podría tender a Normal\")\n\nfor year in years:\n    year_data = train1[train1['Year'] == year]\n    store_daily_year = year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n    \n    cv = store_daily_year.std() / store_daily_year.mean()\n    \n    if abs(cv - 1) < 0.1:\n        distribution_type = \"Poisson-like\"\n    elif cv > 1.2:\n        distribution_type = \"Overdispersed (Binomial Negativa?)\"\n    elif cv < 0.5:\n        distribution_type = \"Underdispersed (Normal?)\"\n    else:\n        distribution_type = \"Intermedio\"\n    \n    print(f\"Año {year} - Tienda-Día: CV={cv:.3f} -> {distribution_type}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Asegurarse de que la columna date sea datetime si no lo está\ntrain1['date'] = pd.to_datetime(train1['date'])\n\n# Para los días de la semana, crear nombres legibles\nday_names = {0: 'Lunes', 1: 'Martes', 2: 'Miércoles', 3: 'Jueves', \n             4: 'Viernes', 5: 'Sábado', 6: 'Domingo'}\ntrain1['day_name'] = train1['DayOfWeek'].map(day_names)\n\nyears = [2017, 2018, 2019, 2020, 2021]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# ===== 1. ANÁLISIS POR PAÍS AÑO A AÑO =====\nprint(\"=== ANÁLISIS POR PAÍS AÑO A AÑO ===\")\n\n# Obtener los países únicos\ncountries = train1['country'].unique()\nprint(f\"Países en el dataset: {countries}\")\n\n# Crear figura para distribuciones por país\nfig, axes = plt.subplots(len(countries), len(years), figsize=(20, 4*len(countries)))\nfig.suptitle('Distribuciones de Ventas por País y Año (Tienda-Día)', fontsize=16)\n\ncountry_stats = {}\nfor i, country in enumerate(countries):\n    country_stats[country] = []\n    for j, year in enumerate(years):\n        # Filtrar por país y año\n        country_year_data = train1[(train1['country'] == country) & (train1['Year'] == year)]\n        store_daily = country_year_data.groupby(['date', 'store'])['num_sold'].sum()\n        \n        # Crear histograma\n        axes[i, j].hist(store_daily, bins=20, alpha=0.7, color=colors[j], edgecolor='black')\n        axes[i, j].set_title(f'{country} - {year}\\nMedia: {store_daily.mean():.1f}')\n        axes[i, j].set_xlabel('Ventas Tienda-Día')\n        if j == 0:  # Solo en la primera columna\n            axes[i, j].set_ylabel(f'{country}\\nFrecuencia')\n        \n        # Guardar estadísticas\n        country_stats[country].append({\n            'year': year,\n            'mean': store_daily.mean(),\n            'std': store_daily.std(),\n            'cv': store_daily.std() / store_daily.mean(),\n            'count': len(store_daily)\n        })\n\nplt.tight_layout()\nplt.show()\n\n# Tabla de estadísticas por país\nprint(\"\\nESTADÍSTICAS POR PAÍS:\")\nfor country in countries:\n    print(f\"\\n{country.upper()}:\")\n    print(\"Año\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n    print(\"-\" * 40)\n    for stat in country_stats[country]:\n        print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n\n# ===== 2. ANÁLISIS POR DÍA DE LA SEMANA AÑO A AÑO =====\nprint(\"\\n\\n=== ANÁLISIS POR DÍA DE LA SEMANA AÑO A AÑO ===\")\n\n# Crear figura para distribuciones por día de la semana\nfig, axes = plt.subplots(7, len(years), figsize=(20, 28))\nfig.suptitle('Distribuciones de Ventas por Día de la Semana y Año (Tienda-Día)', fontsize=16)\n\nday_stats = {}\ndays_order = ['Lunes', 'Martes', 'Miércoles', 'Jueves', 'Viernes', 'Sábado', 'Domingo']\n\nfor i, day in enumerate(days_order):\n    day_stats[day] = []\n    for j, year in enumerate(years):\n        # Filtrar por día de la semana y año\n        day_year_data = train1[(train1['day_name'] == day) & (train1['Year'] == year)]\n        store_daily = day_year_data.groupby(['date', 'country', 'store'])['num_sold'].sum()\n        \n        # Crear histograma\n        axes[i, j].hist(store_daily, bins=20, alpha=0.7, color=colors[j], edgecolor='black')\n        axes[i, j].set_title(f'{day} - {year}\\nMedia: {store_daily.mean():.1f}')\n        axes[i, j].set_xlabel('Ventas Tienda-Día')\n        if j == 0:  # Solo en la primera columna\n            axes[i, j].set_ylabel(f'{day}\\nFrecuencia')\n        \n        # Guardar estadísticas\n        day_stats[day].append({\n            'year': year,\n            'mean': store_daily.mean(),\n            'std': store_daily.std(),\n            'cv': store_daily.std() / store_daily.mean(),\n            'count': len(store_daily)\n        })\n\nplt.tight_layout()\nplt.show()\n\n# Tabla de estadísticas por día de la semana\nprint(\"\\nESTADÍSTICAS POR DÍA DE LA SEMANA:\")\nfor day in days_order:\n    print(f\"\\n{day.upper()}:\")\n    print(\"Año\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n    print(\"-\" * 40)\n    for stat in day_stats[day]:\n        print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n\n# ===== 3. GRÁFICOS DE EVOLUCIÓN TEMPORAL =====\n\n# 3.1 Evolución por país\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nfor country in countries:\n    means = [stat['mean'] for stat in country_stats[country]]\n    cvs = [stat['cv'] for stat in country_stats[country]]\n    \n    axes[0].plot(years, means, 'o-', linewidth=2, markersize=8, label=country)\n    axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=8, label=country)\n\naxes[0].set_title('Evolución de Medias por País')\naxes[0].set_xlabel('Año')\naxes[0].set_ylabel('Media de Ventas (Tienda-Día)')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].set_title('Evolución de Coef. Variación por País')\naxes[1].set_xlabel('Año')\naxes[1].set_ylabel('Coeficiente de Variación')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 3.2 Evolución por día de la semana\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nfor day in days_order:\n    means = [stat['mean'] for stat in day_stats[day]]\n    cvs = [stat['cv'] for stat in day_stats[day]]\n    \n    axes[0].plot(years, means, 'o-', linewidth=2, markersize=6, label=day)\n    axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=6, label=day)\n\naxes[0].set_title('Evolución de Medias por Día de la Semana')\naxes[0].set_xlabel('Año')\naxes[0].set_ylabel('Media de Ventas (Tienda-Día)')\naxes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].set_title('Evolución de Coef. Variación por Día de la Semana')\naxes[1].set_xlabel('Año')\naxes[1].set_ylabel('Coeficiente de Variación')\naxes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ===== 4. ANÁLISIS DE PATRONES =====\nprint(\"\\n=== ANÁLISIS DE PATRONES ===\")\n\n# 4.1 Ranking promedio por país (todos los años)\nprint(\"\\nRANKING PROMEDIO DE VENTAS POR PAÍS:\")\ncountry_overall = []\nfor country in countries:\n    overall_mean = np.mean([stat['mean'] for stat in country_stats[country]])\n    country_overall.append((country, overall_mean))\n\ncountry_overall.sort(key=lambda x: x[1], reverse=True)\nfor i, (country, mean) in enumerate(country_overall, 1):\n    print(f\"{i}. {country}: {mean:.1f}\")\n\n# 4.2 Ranking promedio por día de la semana\nprint(\"\\nRANKING PROMEDIO DE VENTAS POR DÍA:\")\nday_overall = []\nfor day in days_order:\n    overall_mean = np.mean([stat['mean'] for stat in day_stats[day]])\n    day_overall.append((day, overall_mean))\n\nday_overall.sort(key=lambda x: x[1], reverse=True)\nfor i, (day, mean) in enumerate(day_overall, 1):\n    print(f\"{i}. {day}: {mean:.1f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"years = [2017, 2018, 2019, 2020, 2021]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# Obtener valores únicos\ncountries = train1['country'].unique()\nproducts = train1['product'].unique()\nstores = train1['store'].unique()\n\nprint(f\"Países: {len(countries)} - {countries}\")\nprint(f\"Productos: {len(products)} - {products}\")\nprint(f\"Tiendas: {len(stores)} - {stores}\")\n\n# ===== 1. ANÁLISIS POR PRODUCTO-PAÍS-AÑO =====\nprint(\"\\n=== ANÁLISIS POR PRODUCTO-PAÍS-AÑO ===\")\n\n# Para cada país, mostrar productos por año\nfor country in countries:\n    print(f\"\\n{'='*50}\")\n    print(f\"ANÁLISIS PARA {country.upper()}\")\n    print(f\"{'='*50}\")\n    \n    fig, axes = plt.subplots(len(products), len(years), figsize=(20, 4*len(products)))\n    fig.suptitle(f'Distribuciones de Ventas por Producto y Año - {country}', fontsize=16)\n    \n    product_stats = {}\n    \n    for i, product in enumerate(products):\n        product_stats[product] = []\n        \n        for j, year in enumerate(years):\n            # Filtrar por país, producto y año\n            filtered_data = train1[(train1['country'] == country) & \n                                 (train1['product'] == product) & \n                                 (train1['Year'] == year)]\n            \n            # Agrupar por fecha y tienda (cada producto-tienda-día)\n            daily_sales = filtered_data.groupby(['date', 'store'])['num_sold'].sum()\n            \n            # Si hay un solo producto, usar axes unidimensional\n            if len(products) == 1:\n                ax = axes[j]\n            else:\n                ax = axes[i, j]\n            \n            # Crear histograma\n            ax.hist(daily_sales, bins=15, alpha=0.7, color=colors[j], edgecolor='black')\n            ax.set_title(f'{product}\\n{year} - Media: {daily_sales.mean():.1f}')\n            ax.set_xlabel('Ventas Diarias')\n            \n            if j == 0:  # Solo en la primera columna\n                if len(products) == 1:\n                    ax.set_ylabel('Frecuencia')\n                else:\n                    ax.set_ylabel(f'{product}\\nFrecuencia')\n            \n            # Guardar estadísticas\n            product_stats[product].append({\n                'year': year,\n                'mean': daily_sales.mean(),\n                'std': daily_sales.std(),\n                'cv': daily_sales.std() / daily_sales.mean() if daily_sales.mean() > 0 else 0,\n                'count': len(daily_sales)\n            })\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Tabla de estadísticas por producto en este país\n    print(f\"\\nESTADÍSTICAS DE PRODUCTOS EN {country}:\")\n    for product in products:\n        print(f\"\\n{product}:\")\n        print(\"Año\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n        print(\"-\" * 40)\n        for stat in product_stats[product]:\n            print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n    \n    # Gráfico de evolución para este país\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    for product in products:\n        means = [stat['mean'] for stat in product_stats[product]]\n        cvs = [stat['cv'] for stat in product_stats[product]]\n        \n        axes[0].plot(years, means, 'o-', linewidth=2, markersize=8, label=product)\n        axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=8, label=product)\n    \n    axes[0].set_title(f'Evolución de Medias por Producto - {country}')\n    axes[0].set_xlabel('Año')\n    axes[0].set_ylabel('Media de Ventas')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    axes[1].set_title(f'Evolución de Coef. Variación por Producto - {country}')\n    axes[1].set_xlabel('Año')\n    axes[1].set_ylabel('Coeficiente de Variación')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# ===== 2. ANÁLISIS POR TIENDA-AÑO =====\nprint(f\"\\n{'='*60}\")\nprint(\"ANÁLISIS POR TIENDA-AÑO\")\nprint(f\"{'='*60}\")\n\n# Crear figura para distribuciones por tienda\nfig, axes = plt.subplots(len(stores), len(years), figsize=(20, 4*len(stores)))\nfig.suptitle('Distribuciones de Ventas por Tienda y Año', fontsize=16)\n\nstore_stats = {}\n\nfor i, store in enumerate(stores):\n    store_stats[store] = []\n    \n    for j, year in enumerate(years):\n        # Filtrar por tienda y año\n        store_year_data = train1[(train1['store'] == store) & (train1['Year'] == year)]\n        # Agrupar por fecha, país y producto (cada tienda-día)\n        daily_sales = store_year_data.groupby(['date', 'country', 'product'])['num_sold'].sum()\n        \n        # Si hay una sola tienda, usar axes unidimensional\n        if len(stores) == 1:\n            ax = axes[j]\n        else:\n            ax = axes[i, j]\n        \n        # Crear histograma\n        ax.hist(daily_sales, bins=20, alpha=0.7, color=colors[j], edgecolor='black')\n        ax.set_title(f'{store} - {year}\\nMedia: {daily_sales.mean():.1f}')\n        ax.set_xlabel('Ventas Diarias')\n        \n        if j == 0:  # Solo en la primera columna\n            if len(stores) == 1:\n                ax.set_ylabel('Frecuencia')\n            else:\n                ax.set_ylabel(f'{store}\\nFrecuencia')\n        \n        # Guardar estadísticas\n        store_stats[store].append({\n            'year': year,\n            'mean': daily_sales.mean(),\n            'std': daily_sales.std(),\n            'cv': daily_sales.std() / daily_sales.mean() if daily_sales.mean() > 0 else 0,\n            'count': len(daily_sales)\n        })\n\nplt.tight_layout()\nplt.show()\n\n# Tabla de estadísticas por tienda\nprint(\"\\nESTADÍSTICAS POR TIENDA:\")\nfor store in stores:\n    print(f\"\\n{store.upper()}:\")\n    print(\"Año\\tMedia\\tStd\\tCoef.Var\\tN_obs\")\n    print(\"-\" * 40)\n    for stat in store_stats[store]:\n        print(f\"{stat['year']}\\t{stat['mean']:.1f}\\t{stat['std']:.1f}\\t{stat['cv']:.3f}\\t\\t{stat['count']}\")\n\n# Gráfico de evolución por tienda\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nfor store in stores:\n    means = [stat['mean'] for stat in store_stats[store]]\n    cvs = [stat['cv'] for stat in store_stats[store]]\n    \n    axes[0].plot(years, means, 'o-', linewidth=2, markersize=8, label=store)\n    axes[1].plot(years, cvs, 'o-', linewidth=2, markersize=8, label=store)\n\naxes[0].set_title('Evolución de Medias por Tienda')\naxes[0].set_xlabel('Año')\naxes[0].set_ylabel('Media de Ventas')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].set_title('Evolución de Coef. Variación por Tienda')\naxes[1].set_xlabel('Año')\naxes[1].set_ylabel('Coeficiente de Variación')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ===== 3. RANKINGS Y ANÁLISIS COMPARATIVO =====\nprint(f\"\\n{'='*60}\")\nprint(\"RANKINGS Y ANÁLISIS COMPARATIVO\")\nprint(f\"{'='*60}\")\n\n# 3.1 Ranking de productos por país (promedio todos los años)\nprint(\"\\nRANKING DE PRODUCTOS POR PAÍS (Promedio 2017-2021):\")\nfor country in countries:\n    print(f\"\\n{country.upper()}:\")\n    country_data = train1[train1['country'] == country]\n    product_means = country_data.groupby('product')['num_sold'].mean().sort_values(ascending=False)\n    \n    for i, (product, mean) in enumerate(product_means.items(), 1):\n        print(f\"  {i}. {product}: {mean:.1f}\")\n\n# 3.2 Ranking de tiendas (promedio todos los años)\nprint(f\"\\nRANKING DE TIENDAS (Promedio 2017-2021):\")\nstore_means = train1.groupby('store')['num_sold'].mean().sort_values(ascending=False)\nfor i, (store, mean) in enumerate(store_means.items(), 1):\n    print(f\"  {i}. {store}: {mean:.1f}\")\n\n# 3.3 Productos más consistentes (menor CV)\nprint(f\"\\nPRODUCTOS MÁS CONSISTENTES (Menor Coef. Variación):\")\nproduct_consistency = train1.groupby('product')['num_sold'].agg(['mean', 'std'])\nproduct_consistency['cv'] = product_consistency['std'] / product_consistency['mean']\nproduct_consistency = product_consistency.sort_values('cv')\n\nfor i, (product, stats) in enumerate(product_consistency.iterrows(), 1):\n    print(f\"  {i}. {product}: CV={stats['cv']:.3f} (Media: {stats['mean']:.1f})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# de todas estas estadisticas se obtuvo informacion muy relevante\n# lo que mas destaco es ver una distribucion bimodal poisson-normal  o binomial_neg-normal en varios lados, \n# ejemplo la distrib de ventas por producto/año/pais\n# en la distrib por tienda y año se ve como todas comparten la misma tendencia cada año\n# la distrib por dia y pais se ve las ventas de lu-ju una forma y el fin de semana como cambia el viernes un 5 a 7% aprox mas que el jueves\n# el sabado entre 3 a 5 % mas y el domingo 6 a 8 % mas\n# cada pais sus propias ventas pero parecen multiplos de una base ,porque respetan proporciones\n\n# lo mismo que pasa abajo , se de tienda en tienda , teniendo a Kaggle Learn como base\n#   Arg   Canada(Arg/4.1) Estonia(Arg/2.1)  Japon(Arg/3.6) España(Arg/2.6)\n# 1  82.3      83.7          83.2              81.6           82.4\n# 2  81.2      82.5          82.2              80.8           81.3\n# 3  68.8      69.8          69.5              68.1           68.8\n# 4  62.6      63.4          63.2              61.9           62.4\n# 5  12.4      12.9          12.8              12.6           12.7\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\n# ===== 1. ANÁLISIS DE SEPARACIÓN CON THRESHOLD =====\ndef analyze_threshold_split(data, threshold=74):\n    \"\"\"Analiza la separación de datos en dos grupos usando un threshold\"\"\"\n    \n    low_regime = data[data <= threshold]\n    high_regime = data[data > threshold]\n    \n    total_count = len(data)\n    low_count = len(low_regime)\n    high_count = len(high_regime)\n    \n    low_prop = low_count / total_count\n    high_prop = high_count / total_count\n    \n    results = {\n        'threshold': threshold,\n        'total_count': total_count,\n        'low_regime': {\n            'count': low_count,\n            'proportion': low_prop,\n            'mean': low_regime.mean() if len(low_regime) > 0 else 0,\n            'std': low_regime.std() if len(low_regime) > 0 else 0,\n            'data': low_regime\n        },\n        'high_regime': {\n            'count': high_count,\n            'proportion': high_prop,\n            'mean': high_regime.mean() if len(high_regime) > 0 else 0,\n            'std': high_regime.std() if len(high_regime) > 0 else 0,\n            'data': high_regime\n        }\n    }\n    \n    return results\n\n# ===== 2. TEST DE DISTRIBUCIONES POR RÉGIMEN =====\ndef test_distributions(data, regime_name):\n    \"\"\"Testa qué distribución se ajusta mejor a los datos\"\"\"\n    \n    if len(data) < 10:\n        return f\"Datos insuficientes para {regime_name}\"\n    \n    results = {}\n    \n    # Test Poisson (solo para enteros positivos)\n    if regime_name == \"low\" and all(data >= 0) and all(data == data.astype(int)):\n        try:\n            poisson_param = stats.poisson.fit(data, method='MLE')[0]\n            poisson_ks = stats.kstest(data, lambda x: stats.poisson.cdf(x, poisson_param))\n            results['poisson'] = {\n                'param': poisson_param,\n                'ks_stat': poisson_ks.statistic,\n                'p_value': poisson_ks.pvalue\n            }\n        except:\n            results['poisson'] = {'error': 'No se pudo ajustar Poisson'}\n    \n    # Test Normal\n    try:\n        normal_params = stats.norm.fit(data)\n        normal_ks = stats.kstest(data, lambda x: stats.norm.cdf(x, *normal_params))\n        results['normal'] = {\n            'params': normal_params,\n            'ks_stat': normal_ks.statistic,\n            'p_value': normal_ks.pvalue\n        }\n    except:\n        results['normal'] = {'error': 'No se pudo ajustar Normal'}\n    \n    # Test Gamma (para datos positivos)\n    if all(data > 0):\n        try:\n            gamma_params = stats.gamma.fit(data)\n            gamma_ks = stats.kstest(data, lambda x: stats.gamma.cdf(x, *gamma_params))\n            results['gamma'] = {\n                'params': gamma_params,\n                'ks_stat': gamma_ks.statistic,\n                'p_value': gamma_ks.pvalue\n            }\n        except:\n            results['gamma'] = {'error': 'No se pudo ajustar Gamma'}\n    \n    return results\n\n# ===== 3. ANÁLISIS PARA UN PRODUCTO ESPECÍFICO =====\ndef analyze_product_mixture(train1, product_name, country_name=None, threshold=74):\n    \"\"\"Analiza mixture distribution para un producto específico\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ANÁLISIS DE MIXTURE DISTRIBUTION\")\n    print(f\"Producto: {product_name}\")\n    if country_name:\n        print(f\"País: {country_name}\")\n    print(f\"Threshold: {threshold}\")\n    print(f\"{'='*60}\")\n    \n    # Filtrar datos\n    if country_name:\n        data = train1[(train1['product'] == product_name) & \n                     (train1['country'] == country_name)]['num_sold']\n    else:\n        data = train1[train1['product'] == product_name]['num_sold']\n    \n    if len(data) == 0:\n        print(\"No se encontraron datos para los filtros especificados\")\n        return\n    \n    # Análisis de threshold\n    analysis = analyze_threshold_split(data, threshold)\n    \n    print(f\"\\nRESULTADOS DE SEPARACIÓN:\")\n    print(f\"Total observaciones: {analysis['total_count']}\")\n    print(f\"\\nRÉGIMEN BAJO (≤{threshold}):\")\n    print(f\"  Observaciones: {analysis['low_regime']['count']} ({analysis['low_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['low_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['low_regime']['std']:.2f}\")\n    \n    print(f\"\\nRÉGIMEN ALTO (>{threshold}):\")\n    print(f\"  Observaciones: {analysis['high_regime']['count']} ({analysis['high_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['high_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['high_regime']['std']:.2f}\")\n    \n    # Visualización\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Histograma completo\n    axes[0,0].hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    axes[0,0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n    axes[0,0].set_title(f'Distribución Completa - {product_name}')\n    axes[0,0].set_xlabel('Ventas')\n    axes[0,0].set_ylabel('Frecuencia')\n    axes[0,0].legend()\n    \n    # Régimen bajo con Poisson ajustada\n    if len(analysis['low_regime']['data']) > 0:\n        low_data = analysis['low_regime']['data']\n        n, bins, patches = axes[0,1].hist(low_data, bins=20, alpha=0.7, color='green', edgecolor='black', density=True)\n        \n        # Ajustar Poisson\n        poisson_lambda = low_data.mean()\n        x_poisson = np.arange(0, int(low_data.max()) + 1)\n        y_poisson = stats.poisson.pmf(x_poisson, poisson_lambda)\n        axes[0,1].plot(x_poisson, y_poisson, 'ro-', linewidth=2, markersize=4, label=f'Poisson(λ={poisson_lambda:.2f})')\n        \n        axes[0,1].set_title(f'Régimen Bajo (≤{threshold}) vs Poisson')\n        axes[0,1].set_xlabel('Ventas')\n        axes[0,1].set_ylabel('Densidad')\n        axes[0,1].legend()\n    \n    # Régimen alto con Normal ajustada\n    if len(analysis['high_regime']['data']) > 0:\n        high_data = analysis['high_regime']['data']\n        n, bins, patches = axes[1,0].hist(high_data, bins=20, alpha=0.7, color='orange', edgecolor='black', density=True)\n        \n        # Ajustar Normal\n        normal_mu = high_data.mean()\n        normal_sigma = high_data.std()\n        x_normal = np.linspace(high_data.min(), high_data.max(), 100)\n        y_normal = stats.norm.pdf(x_normal, normal_mu, normal_sigma)\n        axes[1,0].plot(x_normal, y_normal, 'b-', linewidth=2, label=f'Normal(μ={normal_mu:.1f}, σ={normal_sigma:.1f})')\n        \n        axes[1,0].set_title(f'Régimen Alto (>{threshold}) vs Normal')\n        axes[1,0].set_xlabel('Ventas')\n        axes[1,0].set_ylabel('Densidad')\n        axes[1,0].legend()\n    \n    # Comparación de distribuciones\n    axes[1,1].hist(analysis['low_regime']['data'], bins=20, alpha=0.5, color='green', label=f'Bajo (≤{threshold})', density=True)\n    axes[1,1].hist(analysis['high_regime']['data'], bins=20, alpha=0.5, color='orange', label=f'Alto (>{threshold})', density=True)\n    axes[1,1].set_title('Comparación de Regímenes (Densidad)')\n    axes[1,1].set_xlabel('Ventas')\n    axes[1,1].set_ylabel('Densidad')\n    axes[1,1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Test distribuciones\n    if len(analysis['low_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - RÉGIMEN BAJO:\")\n        low_tests = test_distributions(analysis['low_regime']['data'], \"low\")\n        for dist_name, result in low_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'poisson':\n                    print(f\"    Lambda estimado: {result['param']:.2f}\")\n                elif dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    if len(analysis['high_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - RÉGIMEN ALTO:\")\n        high_tests = test_distributions(analysis['high_regime']['data'], \"high\")\n        for dist_name, result in high_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    return analysis\n\n# ===== 4. ANÁLISIS PARA MÚLTIPLES THRESHOLDS =====\ndef find_optimal_threshold(data, threshold_range=range(50, 100, 5)):\n    \"\"\"Encuentra el threshold óptimo basado en criterios estadísticos\"\"\"\n    \n    results = []\n    \n    for threshold in threshold_range:\n        analysis = analyze_threshold_split(data, threshold)\n        \n        # Criterios para evaluar la separación\n        # 1. Balance en las proporciones (evitar que un grupo sea muy pequeño)\n        balance_score = 1 - abs(0.5 - analysis['low_regime']['proportion'])\n        \n        # 2. Diferencia en medias (queremos que los grupos sean bien diferentes)\n        if analysis['high_regime']['count'] > 0 and analysis['low_regime']['count'] > 0:\n            mean_diff = abs(analysis['high_regime']['mean'] - analysis['low_regime']['mean'])\n        else:\n            mean_diff = 0\n        \n        # 3. Ratio de medias\n        if analysis['low_regime']['mean'] > 0:\n            mean_ratio = analysis['high_regime']['mean'] / analysis['low_regime']['mean']\n        else:\n            mean_ratio = 0\n        \n        results.append({\n            'threshold': threshold,\n            'low_prop': analysis['low_regime']['proportion'],\n            'high_prop': analysis['high_regime']['proportion'],\n            'low_mean': analysis['low_regime']['mean'],\n            'high_mean': analysis['high_regime']['mean'],\n            'mean_diff': mean_diff,\n            'mean_ratio': mean_ratio,\n            'balance_score': balance_score\n        })\n    \n    results_df = pd.DataFrame(results)\n    return results_df\n\n# ===== EJECUCIÓN DEL ANÁLISIS =====\n\n# Obtener lista de productos\nproducts = train1['product'].unique()\ncountries = train1['country'].unique()\n\nprint(\"PRODUCTOS DISPONIBLES:\")\nfor i, product in enumerate(products):\n    print(f\"{i+1}. {product}\")\n\nprint(\"\\nPAÍSES DISPONIBLES:\")\nfor i, country in enumerate(countries):\n    print(f\"{i+1}. {country}\")\n\n# Análisis para el primer producto como ejemplo\nif len(products) > 0:\n    example_product = products[0]\n    example_country = countries[0]\n    \n    print(f\"\\n🔍 ANÁLISIS DE EJEMPLO:\")\n    print(f\"Producto: {example_product}\")\n    print(f\"País: {example_country}\")\n    \n    # Análisis con threshold 74\n    analysis_74 = analyze_product_mixture(train1, example_product, example_country, threshold=74)\n    \n    # Buscar threshold óptimo\n    product_data = train1[(train1['product'] == example_product) & \n                         (train1['country'] == example_country)]['num_sold']\n    \n    print(f\"\\n🎯 BÚSQUEDA DE THRESHOLD ÓPTIMO:\")\n    threshold_analysis = find_optimal_threshold(product_data)\n    \n    # Mostrar los mejores thresholds\n    threshold_analysis['score'] = (threshold_analysis['balance_score'] * \n                                  threshold_analysis['mean_diff'] / 100)\n    \n    best_thresholds = threshold_analysis.nlargest(3, 'score')\n    print(\"\\nTOP 3 THRESHOLDS:\")\n    for idx, row in best_thresholds.iterrows():\n        print(f\"  Threshold: {row['threshold']}\")\n        print(f\"    Proporción bajo: {row['low_prop']:.1%}, alto: {row['high_prop']:.1%}\")\n        print(f\"    Media bajo: {row['low_mean']:.1f}, alto: {row['high_mean']:.1f}\")\n        print(f\"    Ratio medias: {row['mean_ratio']:.2f}\")\n        print()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\n# ===== 1. ANÁLISIS DE SEPARACIÓN CON THRESHOLD =====\ndef analyze_threshold_split(data, threshold=74):\n    \"\"\"Analiza la separación de datos en dos grupos usando un threshold\"\"\"\n    \n    low_regime = data[data <= threshold]\n    high_regime = data[data > threshold]\n    \n    total_count = len(data)\n    low_count = len(low_regime)\n    high_count = len(high_regime)\n    \n    low_prop = low_count / total_count\n    high_prop = high_count / total_count\n    \n    results = {\n        'threshold': threshold,\n        'total_count': total_count,\n        'low_regime': {\n            'count': low_count,\n            'proportion': low_prop,\n            'mean': low_regime.mean() if len(low_regime) > 0 else 0,\n            'std': low_regime.std() if len(low_regime) > 0 else 0,\n            'data': low_regime\n        },\n        'high_regime': {\n            'count': high_count,\n            'proportion': high_prop,\n            'mean': high_regime.mean() if len(high_regime) > 0 else 0,\n            'std': high_regime.std() if len(high_regime) > 0 else 0,\n            'data': high_regime\n        }\n    }\n    \n    return results\n\n# ===== 2. TEST DE DISTRIBUCIONES POR RÉGIMEN =====\ndef test_distributions(data, regime_name):\n    \"\"\"Testa qué distribución se ajusta mejor a los datos\"\"\"\n    \n    if len(data) < 10:\n        return f\"Datos insuficientes para {regime_name}\"\n    \n    results = {}\n    \n    # Test Poisson (solo para enteros positivos)\n    if regime_name == \"low\" and all(data >= 0) and all(data == data.astype(int)):\n        try:\n            poisson_param = stats.poisson.fit(data, method='MLE')[0]\n            poisson_ks = stats.kstest(data, lambda x: stats.poisson.cdf(x, poisson_param))\n            results['poisson'] = {\n                'param': poisson_param,\n                'ks_stat': poisson_ks.statistic,\n                'p_value': poisson_ks.pvalue\n            }\n        except:\n            results['poisson'] = {'error': 'No se pudo ajustar Poisson'}\n    \n    # Test Normal\n    try:\n        normal_params = stats.norm.fit(data)\n        normal_ks = stats.kstest(data, lambda x: stats.norm.cdf(x, *normal_params))\n        results['normal'] = {\n            'params': normal_params,\n            'ks_stat': normal_ks.statistic,\n            'p_value': normal_ks.pvalue\n        }\n    except:\n        results['normal'] = {'error': 'No se pudo ajustar Normal'}\n    \n    # Test Gamma (para datos positivos)\n    if all(data > 0):\n        try:\n            gamma_params = stats.gamma.fit(data)\n            gamma_ks = stats.kstest(data, lambda x: stats.gamma.cdf(x, *gamma_params))\n            results['gamma'] = {\n                'params': gamma_params,\n                'ks_stat': gamma_ks.statistic,\n                'p_value': gamma_ks.pvalue\n            }\n        except:\n            results['gamma'] = {'error': 'No se pudo ajustar Gamma'}\n    \n    return results\n\n# ===== 3. ANÁLISIS PARA UN PRODUCTO ESPECÍFICO =====\ndef analyze_product_mixture(train1, product_name, country_name=None, threshold=74):\n    \"\"\"Analiza mixture distribution para un producto específico\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ANÁLISIS DE MIXTURE DISTRIBUTION\")\n    print(f\"Producto: {product_name}\")\n    if country_name:\n        print(f\"País: {country_name}\")\n    print(f\"Threshold: {threshold}\")\n    print(f\"{'='*60}\")\n    \n    # Filtrar datos\n    if country_name:\n        data = train1[(train1['product'] == product_name) & \n                     (train1['country'] == country_name)]['num_sold']\n    else:\n        data = train1[train1['product'] == product_name]['num_sold']\n    \n    if len(data) == 0:\n        print(\"No se encontraron datos para los filtros especificados\")\n        return\n    \n    # Análisis de threshold\n    analysis = analyze_threshold_split(data, threshold)\n    \n    print(f\"\\nRESULTADOS DE SEPARACIÓN:\")\n    print(f\"Total observaciones: {analysis['total_count']}\")\n    print(f\"\\nRÉGIMEN BAJO (≤{threshold}):\")\n    print(f\"  Observaciones: {analysis['low_regime']['count']} ({analysis['low_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['low_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['low_regime']['std']:.2f}\")\n    \n    print(f\"\\nRÉGIMEN ALTO (>{threshold}):\")\n    print(f\"  Observaciones: {analysis['high_regime']['count']} ({analysis['high_regime']['proportion']:.1%})\")\n    print(f\"  Media: {analysis['high_regime']['mean']:.2f}\")\n    print(f\"  Std: {analysis['high_regime']['std']:.2f}\")\n    \n    # Visualización\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Histograma completo\n    axes[0,0].hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    axes[0,0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n    axes[0,0].set_title(f'Distribución Completa - {product_name}')\n    axes[0,0].set_xlabel('Ventas')\n    axes[0,0].set_ylabel('Frecuencia')\n    axes[0,0].legend()\n    \n    # Régimen bajo con múltiples distribuciones\n    if len(analysis['low_regime']['data']) > 0:\n        low_data = analysis['low_regime']['data']\n        n, bins, patches = axes[0,1].hist(low_data, bins=20, alpha=0.7, color='green', edgecolor='black', density=True)\n        \n        # 1. Poisson\n        poisson_lambda = low_data.mean()\n        x_discrete = np.arange(0, int(low_data.max()) + 1)\n        y_poisson = stats.poisson.pmf(x_discrete, poisson_lambda)\n        axes[0,1].plot(x_discrete, y_poisson, 'ro-', linewidth=2, markersize=3, label=f'Poisson(λ={poisson_lambda:.1f})')\n        \n        # 2. Binomial Negativa (usando método de momentos)\n        try:\n            mean_low = low_data.mean()\n            var_low = low_data.var()\n            if var_low > mean_low:  # Solo si hay sobredispersión\n                # Parámetros por método de momentos: n, p\n                p_est = mean_low / var_low\n                n_est = mean_low * p_est / (1 - p_est)\n                if 0 < p_est < 1 and n_est > 0:\n                    y_nbinom = stats.nbinom.pmf(x_discrete, n_est, p_est)\n                    axes[0,1].plot(x_discrete, y_nbinom, 'bs-', linewidth=2, markersize=3, \n                                 label=f'Binomial Neg(n={n_est:.1f}, p={p_est:.2f})')\n        except:\n            pass\n        \n        # 3. Uniforme Discreta (aproximación)\n        min_val = int(low_data.min())\n        max_val = int(low_data.max())\n        uniform_prob = 1.0 / (max_val - min_val + 1)\n        x_uniform = np.arange(min_val, max_val + 1)\n        y_uniform = np.full(len(x_uniform), uniform_prob)\n        axes[0,1].plot(x_uniform, y_uniform, 'm-', linewidth=3, alpha=0.8, \n                     label=f'Uniforme({min_val},{max_val})')\n        \n        # 4. Uniforme continua (para referencia visual)\n        x_cont = np.linspace(low_data.min(), low_data.max(), 100)\n        y_uniform_cont = stats.uniform.pdf(x_cont, low_data.min(), low_data.max() - low_data.min())\n        axes[0,1].plot(x_cont, y_uniform_cont, 'c--', linewidth=2, alpha=0.7, \n                     label=f'Uniforme Continua')\n        \n        axes[0,1].set_title(f'Régimen Bajo (≤{threshold}) - Comparación Distribuciones')\n        axes[0,1].set_xlabel('Ventas')\n        axes[0,1].set_ylabel('Densidad')\n        axes[0,1].legend(fontsize=8)\n        axes[0,1].grid(True, alpha=0.3)\n    \n    # Régimen alto con Normal ajustada\n    if len(analysis['high_regime']['data']) > 0:\n        high_data = analysis['high_regime']['data']\n        n, bins, patches = axes[1,0].hist(high_data, bins=20, alpha=0.7, color='orange', edgecolor='black', density=True)\n        \n        # Ajustar Normal\n        normal_mu = high_data.mean()\n        normal_sigma = high_data.std()\n        x_normal = np.linspace(high_data.min(), high_data.max(), 100)\n        y_normal = stats.norm.pdf(x_normal, normal_mu, normal_sigma)\n        axes[1,0].plot(x_normal, y_normal, 'b-', linewidth=2, label=f'Normal(μ={normal_mu:.1f}, σ={normal_sigma:.1f})')\n        \n        axes[1,0].set_title(f'Régimen Alto (>{threshold}) vs Normal')\n        axes[1,0].set_xlabel('Ventas')\n        axes[1,0].set_ylabel('Densidad')\n        axes[1,0].legend()\n    \n    # Comparación de distribuciones\n    axes[1,1].hist(analysis['low_regime']['data'], bins=20, alpha=0.5, color='green', label=f'Bajo (≤{threshold})', density=True)\n    axes[1,1].hist(analysis['high_regime']['data'], bins=20, alpha=0.5, color='orange', label=f'Alto (>{threshold})', density=True)\n    axes[1,1].set_title('Comparación de Regímenes (Densidad)')\n    axes[1,1].set_xlabel('Ventas')\n    axes[1,1].set_ylabel('Densidad')\n    axes[1,1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Test distribuciones\n    if len(analysis['low_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - RÉGIMEN BAJO:\")\n        low_tests = test_distributions(analysis['low_regime']['data'], \"low\")\n        for dist_name, result in low_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'poisson':\n                    print(f\"    Lambda estimado: {result['param']:.2f}\")\n                elif dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    if len(analysis['high_regime']['data']) > 10:\n        print(f\"\\nTEST DE DISTRIBUCIONES - RÉGIMEN ALTO:\")\n        high_tests = test_distributions(analysis['high_regime']['data'], \"high\")\n        for dist_name, result in high_tests.items():\n            if isinstance(result, dict) and 'p_value' in result:\n                print(f\"  {dist_name.upper()}: KS p-value = {result['p_value']:.4f}\")\n                if dist_name == 'normal':\n                    print(f\"    Media: {result['params'][0]:.2f}, Std: {result['params'][1]:.2f}\")\n    \n    return analysis\n\n# ===== 4. ANÁLISIS PARA MÚLTIPLES THRESHOLDS =====\ndef find_optimal_threshold(data, threshold_range=range(50, 100, 5)):\n    \"\"\"Encuentra el threshold óptimo basado en criterios estadísticos\"\"\"\n    \n    results = []\n    \n    for threshold in threshold_range:\n        analysis = analyze_threshold_split(data, threshold)\n        \n        # Criterios para evaluar la separación\n        # 1. Balance en las proporciones (evitar que un grupo sea muy pequeño)\n        balance_score = 1 - abs(0.5 - analysis['low_regime']['proportion'])\n        \n        # 2. Diferencia en medias (queremos que los grupos sean bien diferentes)\n        if analysis['high_regime']['count'] > 0 and analysis['low_regime']['count'] > 0:\n            mean_diff = abs(analysis['high_regime']['mean'] - analysis['low_regime']['mean'])\n        else:\n            mean_diff = 0\n        \n        # 3. Ratio de medias\n        if analysis['low_regime']['mean'] > 0:\n            mean_ratio = analysis['high_regime']['mean'] / analysis['low_regime']['mean']\n        else:\n            mean_ratio = 0\n        \n        results.append({\n            'threshold': threshold,\n            'low_prop': analysis['low_regime']['proportion'],\n            'high_prop': analysis['high_regime']['proportion'],\n            'low_mean': analysis['low_regime']['mean'],\n            'high_mean': analysis['high_regime']['mean'],\n            'mean_diff': mean_diff,\n            'mean_ratio': mean_ratio,\n            'balance_score': balance_score\n        })\n    \n    results_df = pd.DataFrame(results)\n    return results_df\n\n# ===== EJECUCIÓN DEL ANÁLISIS =====\n\n# Obtener lista de productos\nproducts = train1['product'].unique()\ncountries = train1['country'].unique()\n\nprint(\"PRODUCTOS DISPONIBLES:\")\nfor i, product in enumerate(products):\n    print(f\"{i+1}. {product}\")\n\nprint(\"\\nPAÍSES DISPONIBLES:\")\nfor i, country in enumerate(countries):\n    print(f\"{i+1}. {country}\")\n\n# Análisis para el primer producto como ejemplo\nif len(products) > 0:\n    example_product = products[0]\n    example_country = countries[0]\n    \n    print(f\"\\n🔍 ANÁLISIS DE EJEMPLO:\")\n    print(f\"Producto: {example_product}\")\n    print(f\"País: {example_country}\")\n    \n    # Análisis con threshold 74\n    analysis_74 = analyze_product_mixture(train1, example_product, example_country, threshold=74)\n    \n    # Buscar threshold óptimo\n    product_data = train1[(train1['product'] == example_product) & \n                         (train1['country'] == example_country)]['num_sold']\n    \n    print(f\"\\n🎯 BÚSQUEDA DE THRESHOLD ÓPTIMO:\")\n    threshold_analysis = find_optimal_threshold(product_data)\n    \n    # Mostrar los mejores thresholds\n    threshold_analysis['score'] = (threshold_analysis['balance_score'] * \n                                  threshold_analysis['mean_diff'] / 100)\n    \n    best_thresholds = threshold_analysis.nlargest(3, 'score')\n    print(\"\\nTOP 3 THRESHOLDS:\")\n    for idx, row in best_thresholds.iterrows():\n        print(f\"  Threshold: {row['threshold']}\")\n        print(f\"    Proporción bajo: {row['low_prop']:.1%}, alto: {row['high_prop']:.1%}\")\n        print(f\"    Media bajo: {row['low_mean']:.1f}, alto: {row['high_mean']:.1f}\")\n        print(f\"    Ratio medias: {row['mean_ratio']:.2f}\")\n        print()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# se ve , regimen bajo =<74 66% es binomial negativa o \n#         regiment alto >74 34% es normal\n# mejor dicho se puede adepatar de esa manera\n# este caso es para Argentina con el producto \"Using LLMs to Improve Your Coding\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndef analyze_regime_threshold(data, threshold_candidates=range(10, 200, 10)):\n    \"\"\"Encuentra el mejor threshold para separar regímenes (tu función original).\"\"\"\n    results = []\n    for threshold in threshold_candidates:\n        low_regime = data[data <= threshold]\n        high_regime = data[data > threshold]\n        \n        if len(low_regime) < 10 or len(high_regime) < 10:\n            continue\n            \n        low_prop = len(low_regime) / len(data)\n        \n        # Criterios para evaluar la separación\n        balance_score = 1 - abs(0.5 - low_prop)\n        mean_separation = abs(high_regime.mean() - low_regime.mean())\n        cv_low = low_regime.std() / low_regime.mean() if low_regime.mean() > 0 else 99\n        \n        # Score combinado\n        combined_score = (balance_score * 0.3 + \n                          (mean_separation / 100) * 0.5 + \n                          (1 / (1 + abs(cv_low - 1))) * 0.2)\n        \n        results.append({\n            'threshold': threshold,\n            'low_prop': low_prop,\n            'high_prop': 1 - low_prop,\n            'low_mean': low_regime.mean(),\n            'high_mean': high_regime.mean(),\n            'low_cv': cv_low,\n            'mean_separation': mean_separation,\n            'combined_score': combined_score\n        })\n    \n    if not results:\n        return None\n    \n    results_df = pd.DataFrame(results)\n    best_threshold = results_df.loc[results_df['combined_score'].idxmax(), 'threshold']\n    \n    return best_threshold, results_df\n\ndef test_regime_distributions(low_data, high_data):\n    \"\"\"Testa qué distribuciones se ajustan mejor a cada régimen (tu función original).\"\"\"\n    results = {'low_regime': {}, 'high_regime': {}}\n    \n    # Test régimen bajo\n    if len(low_data) > 10:\n        # Poisson\n        try:\n            poisson_lambda = low_data.mean()\n            poisson_ks = stats.kstest(low_data, lambda x: stats.poisson.cdf(x, poisson_lambda))\n            results['low_regime']['poisson'] = {'lambda': poisson_lambda, 'ks_pvalue': poisson_ks.pvalue}\n        except: pass\n        \n        # Binomial Negativa\n        try:\n            mean_val, var_val = low_data.mean(), low_data.var()\n            if var_val > mean_val:\n                p_est = mean_val / var_val\n                n_est = mean_val * p_est / (1 - p_est)\n                if 0 < p_est < 1 and n_est > 0:\n                    nbinom_ks = stats.kstest(low_data, lambda x: stats.nbinom.cdf(x, n_est, p_est))\n                    results['low_regime']['nbinom'] = {'n': n_est, 'p': p_est, 'ks_pvalue': nbinom_ks.pvalue}\n        except: pass\n    \n    # Test régimen alto\n    if len(high_data) > 10:\n        # Normal\n        try:\n            normal_mu, normal_sigma = stats.norm.fit(high_data)\n            normal_ks = stats.kstest(high_data, lambda x: stats.norm.cdf(x, normal_mu, normal_sigma))\n            results['high_regime']['normal'] = {'mu': normal_mu, 'sigma': normal_sigma, 'ks_pvalue': normal_ks.pvalue}\n        except: pass\n        \n    return results\n\ndef plot_regime_distributions(data, threshold, low_regime, high_regime):\n    \"\"\"\n    NUEVA FUNCIÓN: Genera un gráfico 2x2 para visualizar los regímenes y\n    comparar las distribuciones.\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle(f'Análisis de Distribución con Threshold = {threshold}', fontsize=16)\n\n    # 1. Histograma completo\n    axes[0, 0].hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    axes[0, 0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n    axes[0, 0].set_title('Distribución Completa de Ventas')\n    axes[0, 0].set_xlabel('Ventas')\n    axes[0, 0].set_ylabel('Frecuencia')\n    axes[0, 0].legend()\n\n    # 2. Régimen bajo vs. Distribuciones de Conteo\n    if not low_regime.empty:\n        axes[0, 1].hist(low_regime, bins=20, alpha=0.7, color='green', edgecolor='black', density=True, label='Datos Observados')\n        \n        # Curva Poisson\n        poisson_lambda = low_regime.mean()\n        x_poisson = np.arange(0, int(low_regime.max()) + 1)\n        y_poisson = stats.poisson.pmf(x_poisson, poisson_lambda)\n        axes[0, 1].plot(x_poisson, y_poisson, 'ro-', linewidth=2, markersize=4, label=f'Poisson (λ={poisson_lambda:.2f})')\n        \n        # Curva Binomial Negativa\n        mean, var = low_regime.mean(), low_regime.var()\n        if var > mean:\n            p = mean / var\n            n = mean * p / (1 - p)\n            if 0 < p < 1 and n > 0:\n                y_nbinom = stats.nbinom.pmf(x_poisson, n, p)\n                axes[0, 1].plot(x_poisson, y_nbinom, 'bs-', linewidth=2, markersize=4, label=f'B. Negativa (n={n:.1f}, p={p:.2f})')\n\n        axes[0, 1].set_title(f'Régimen Bajo (≤{threshold})')\n        axes[0, 1].set_xlabel('Ventas')\n        axes[0, 1].set_ylabel('Densidad')\n        axes[0, 1].legend()\n\n    # 3. Régimen alto vs. Distribución Normal\n    if not high_regime.empty:\n        axes[1, 0].hist(high_regime, bins=30, alpha=0.7, color='orange', edgecolor='black', density=True, label='Datos Observados')\n        \n        # Curva Normal\n        mu, sigma = stats.norm.fit(high_regime)\n        x_normal = np.linspace(high_regime.min(), high_regime.max(), 100)\n        y_normal = stats.norm.pdf(x_normal, mu, sigma)\n        axes[1, 0].plot(x_normal, y_normal, 'b-', linewidth=2, label=f'Normal (μ={mu:.1f}, σ={sigma:.1f})')\n        \n        axes[1, 0].set_title(f'Régimen Alto (>{threshold})')\n        axes[1, 0].set_xlabel('Ventas')\n        axes[1, 0].set_ylabel('Densidad')\n        axes[1, 0].legend()\n\n    # 4. Comparación de densidades\n    if not low_regime.empty:\n        low_regime.plot(kind='kde', ax=axes[1, 1], color='green', label=f'Bajo (≤{threshold})')\n    if not high_regime.empty:\n        high_regime.plot(kind='kde', ax=axes[1, 1], color='orange', label=f'Alto (>{threshold})')\n    axes[1, 1].set_title('Comparación de Densidades (KDE)')\n    axes[1, 1].set_xlabel('Ventas')\n    axes[1, 1].legend()\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()\n\n\n# ===== ANÁLISIS COMPLETO POR PAÍS-PRODUCTO (MODIFICADO) =====\n\n# Cargar datos (necesitas tener tu dataframe 'train1' cargado)\n# train1 = pd.read_csv('tu_archivo.csv') # Ejemplo de carga\n\ncountries = train1['country'].unique()\nproducts = train1['product'].unique()\n\nregime_analysis_results = {}\noptimal_thresholds = {}\n\nprint(\"=== ANÁLISIS DE REGÍMENES Y VISUALIZACIÓN ===\\n\")\n\nfor country in countries:\n    print(f\"{'='*60}\\nPAÍS: {country.upper()}\\n{'='*60}\")\n    \n    regime_analysis_results[country] = {}\n    optimal_thresholds[country] = {}\n    \n    for product in products:\n        print(f\"\\n--- Producto: {product} ---\")\n        \n        data = train1[(train1['country'] == country) & (train1['product'] == product)]['num_sold']\n        \n        if len(data) < 20: # Aumentamos el mínimo para un análisis robusto\n            print(f\"No hay datos suficientes ({len(data)} obs.)\")\n            continue\n        \n        print(f\"Total observaciones: {len(data)}, Rango: {data.min()}-{data.max()}, Media: {data.mean():.1f}\")\n        \n        # Encontrar threshold óptimo\n        threshold_result = analyze_regime_threshold(data)\n        \n        if threshold_result is None:\n            print(\"No se pudo encontrar un threshold adecuado.\")\n            continue\n        \n        best_threshold, _ = threshold_result\n        optimal_thresholds[country][product] = best_threshold\n        \n        print(f\"\\n🎯 THRESHOLD ÓPTIMO ENCONTRADO: {best_threshold}\")\n        \n        # Separar en regímenes\n        low_regime = data[data <= best_threshold]\n        high_regime = data[data > best_threshold]\n        \n        # Test de distribuciones\n        dist_results = test_regime_distributions(low_regime, high_regime)\n        regime_analysis_results[country][product] = {'threshold': best_threshold, 'distributions': dist_results}\n\n        # Mostrar mejor ajuste y generar gráfico\n        print(\"\\n📊 MEJOR AJUSTE DE DISTRIBUCIONES (basado en p-valor de KS test):\")\n        # (Aquí puedes agregar la lógica para imprimir el mejor ajuste si lo deseas)\n\n        # ¡NUEVO! Generar y mostrar el gráfico\n        plot_regime_distributions(data, best_threshold, low_regime, high_regime)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# el caso de argentina primero si ajusta bien ese producto , ahora hay algunos que si y otros no , el threshold va cambiando bastante\n# en algunos casos me lleva a pensar en otras distribuciones\n# para el primer modelo va sin regimen\n\n\n# abajo primero resumen de todos los efectos que se vieron y todos los cambios a efectuar para un modelo simple \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1. ESTACIONALIDAD SEMANAL\n# Factores de crecimiento semanal identificados\n\"\"\"\nweekend_boost = {\n    0: 1.00,  # Lunes (base)\n    1: 1.00,  # Martes  \n    2: 1.00,  # Miércoles\n    3: 1.00,  # Jueves\n    4: 1.06,  # Viernes (+5-7% vs jueves)\n    5: 1.09,  # Sábado (+3-5% vs viernes) \n    6: 1.17   # Domingo (+6-8% vs sábado)\n}\ntrain1['weekend_factor'] = train1['DayOfWeek'].map(weekend_boost)\n\n# Features categóricas adicionales\ntrain1['is_weekend'] = (train1['DayOfWeek'] >= 5).astype(int)\ntrain1['is_friday'] = (train1['DayOfWeek'] == 4).astype(int)\n\"\"\"\n#2. EFECTOS FIJOS POR PAÍS (Multiplicadores constantes)\n\"\"\"\n# Multiplicadores identificados (Argentina = base)\ncountry_multipliers = {\n    'Argentina': 1.0,\n    'Canada': 4.1, \n    'Estonia': 2.1,\n    'Japan': 3.6,\n    'Spain': 2.6\n}\ntrain1['country_multiplier'] = train1['country'].map(country_multipliers)\n\n# One-hot encoding para efectos fijos\ntrain1 = pd.get_dummies(train1, columns=['country'], prefix='country')\n\"\"\"\n#3. EFECTOS FIJOS POR TIENDA (Multiplicadores identificados)\n\"\"\"\n# Multiplicadores por tienda (Kaggle Learn = base)\nstore_multipliers = {\n    'Kaggle Learn': 1.0,      # Base\n    'Kaggle Store': 1.54,     # 1.54x base\n    'Kagglazon': 5.67         # 5.67x base  \n}\ntrain1['store_multiplier'] = train1['store'].map(store_multipliers)\n\n# One-hot encoding para tiendas\ntrain1 = pd.get_dummies(train1, columns=['store'], prefix='store')\n\"\"\"\n#4. EFECTOS FIJOS POR PRODUCTO (Ranking consistente)\n\"\"\"\n# Ranking de productos (mismo orden en todos los países)\nproduct_ranking = {\n    'Using LLMs to Improve Your Coding': 1,\n    'Using LLMs to Train More LLMs': 2, \n    'Using LLMs to Win More Kaggle Competitions': 3,\n    'Using LLMs to Write Better': 4,\n    'Using LLMs to Win Friends and Influence People': 5\n}\ntrain1['product_rank'] = train1['product'].map(product_ranking)\n\n# Valores base por producto (usando Argentina + Kaggle Learn como referencia)\nproduct_base_values = {\n    'Using LLMs to Improve Your Coding': 82.3,\n    'Using LLMs to Train More LLMs': 81.2,\n    'Using LLMs to Win More Kaggle Competitions': 68.8, \n    'Using LLMs to Write Better': 62.6,\n    'Using LLMs to Win Friends and Influence People': 12.4\n}\ntrain1['product_base_value'] = train1['product'].map(product_base_values)\n\n# One-hot encoding\ntrain1 = pd.get_dummies(train1, columns=['product'], prefix='product')\n\"\"\"\n#5. FEATURES TEMPORALES\n\"\"\"\n# Tendencias temporales\ntrain1['year_trend'] = train1['Year'] - 2017  # 0,1,2,3,4\ntrain1['days_since_start'] = (train1['date'] - train1['date'].min()).dt.days\n\n# Seasonality dentro del año\ntrain1['month_sin'] = np.sin(2 * np.pi * train1['Month'] / 12)\ntrain1['month_cos'] = np.cos(2 * np.pi * train1['Month'] / 12)\ntrain1['day_of_year'] = train1['date'].dt.dayofyear\ntrain1['quarter_sin'] = np.sin(2 * np.pi * train1['Quarter'] / 4)\ntrain1['quarter_cos'] = np.cos(2 * np.pi * train1['Quarter'] / 4)\n\"\"\"\n#6. MIXTURE MODEL FEATURES (Régimen identificado) no utilizado\n\n#7. FEATURES DE INTERACCIÓN (Actualizadas con tienda)\n\"\"\"\n# Interacciones clave identificadas\ntrain1['country_store'] = train1['country_multiplier'] * train1['store_multiplier']\ntrain1['country_weekend'] = train1['country_multiplier'] * train1['weekend_factor']\ntrain1['store_weekend'] = train1['store_multiplier'] * train1['weekend_factor']\ntrain1['product_weekend'] = train1['product_base_value'] * train1['weekend_factor']\ntrain1['product_store'] = train1['product_base_value'] * train1['store_multiplier']\n\n# Interacción triple principal\ntrain1['country_store_product'] = (train1['country_multiplier'] * \n                                  train1['store_multiplier'] * \n                                  train1['product_base_value'])\n\n# Interacción año-producto (por si hay trends específicos)\nfor year in [2017, 2018, 2019, 2020, 2021]:\n    train1[f'product_rank_year_{year}'] = (train1['product_rank'] * \n                                          (train1['Year'] == year).astype(int))\n\"\"\"\n#8. LAG FEATURES (Series temporales)\n\"\"\"\n# Ordenar por entidad y fecha\ntrain1 = train1.sort_values(['country', 'store', 'product', 'date'])\n\n# Lags por entidad (país-tienda-producto)\nfor lag in [1, 7, 30]:\n    train1[f'num_sold_lag_{lag}'] = (train1.groupby(['country', 'store', 'product'])\n                                    ['num_sold'].shift(lag))\n\n# Rolling statistics\nfor window in [7, 30]:\n    train1[f'num_sold_rolling_mean_{window}'] = (train1.groupby(['country', 'store', 'product'])\n                                                ['num_sold'].rolling(window).mean()\n                                                .reset_index(0, drop=True))\n    \n    train1[f'num_sold_rolling_std_{window}'] = (train1.groupby(['country', 'store', 'product'])\n                                               ['num_sold'].rolling(window).std()\n                                               .reset_index(0, drop=True))\n\"\"\"\n#9. FEATURES TARGET TRANSFORMADAS\n\"\"\"\n# Para el régimen normal (alta demanda)\ntrain1['log_num_sold'] = np.log1p(train1['num_sold'])\ntrain1['sqrt_num_sold'] = np.sqrt(train1['num_sold'])\n\n# Para análisis de residuos\ntrain1['residual_from_base'] = train1['num_sold'] - train1['expected_base']\n\"\"\"\n#10. FEATURES FINALES PARA MODELO\n\"\"\"\n# Lista de features para usar en el modelo\nbase_features = [\n    # Efectos fijos multiplicativos\n    'country_multiplier', 'store_multiplier', 'product_base_value', 'product_rank',\n    \n    # Estacionalidad \n    'weekend_factor', 'is_weekend', 'is_friday',\n    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n    \n    # Temporales\n    'year_trend', 'days_since_start',\n    \n    # Interacciones clave\n    'country_store', 'country_weekend', 'store_weekend', \n    'product_weekend', 'product_store', 'country_store_product',\n    \n    # Base esperada y desviaciones\n    'expected_base', 'deviation_from_expected',\n    \n    # Lags (si disponibles)\n    'num_sold_lag_1', 'num_sold_lag_7', 'num_sold_rolling_mean_7'\n]\n\n# Features categóricas (one-hot encoded)\ncategorical_features = [col for col in train1.columns \n                       if col.startswith(('country_', 'product_', 'store_'))]\n\n# Features finales\nmodel_features = base_features + categorical_features\n\"\"\"\n#Como quedaria\n#Expected_Sales = Argentina_Base × Country_Mult × Store_Mult × Product_Base × Weekend_Factor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  ESTACIONALIDAD SEMANAL\n# Factores de crecimiento semanal identificados\nweekend_boost = {\n    0: 1.00,  # Lunes (base)\n    1: 1.00,  # Martes  \n    2: 1.00,  # Miércoles\n    3: 1.00,  # Jueves\n    4: 1.06,  # Viernes (+5-7% vs jueves)\n    5: 1.09,  # Sábado (+3-5% vs viernes) \n    6: 1.17   # Domingo (+6-8% vs sábado)\n}\ntrain1['weekend_factor'] = train1['DayOfWeek'].map(weekend_boost)\ntest1['weekend_factor'] = test1['DayOfWeek'].map(weekend_boost)\n# Features categóricas adicionales\ntrain1['is_weekend'] = (train1['DayOfWeek'] >= 5).astype(int)\ntrain1['is_friday'] = (train1['DayOfWeek'] == 4).astype(int)\ntest1['is_weekend'] = (test1['DayOfWeek'] >= 5).astype(int)\ntest1['is_friday'] = (test1['DayOfWeek'] == 4).astype(int)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EFECTOS FIJOS POR PAÍS\n# Multiplicadores identificados (Argentina = base)\ncountry_multipliers = {\n    'Argentina': 1.0,\n    'Canada': 4.1, \n    'Estonia': 2.1,\n    'Japan': 3.6,\n    'Spain': 2.6\n}\ntrain1['country_multiplier'] = train1['country'].map(country_multipliers)\ntest1['country_multiplier'] = test1['country'].map(country_multipliers)\n# One-hot encoding para efectos fijos\n#train1 = pd.get_dummies(train1, columns=['country'], prefix='country')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  EFECTOS FIJOS POR TIENDA\n# Multiplicadores por tienda (Kaggle Learn = base)\nstore_multipliers = {\n    'Kaggle Learn': 1.0,      # Base\n    'Kaggle Store': 1.54,     # 1.54x base\n    'Kagglazon': 5.67         # 5.67x base  \n}\ntrain1['store_multiplier'] = train1['store'].map(store_multipliers)\ntest1['store_multiplier'] = test1['store'].map(store_multipliers)\n# One-hot encoding para tiendas\n#train1 = pd.get_dummies(train1, columns=['store'], prefix='store')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  EFECTOS FIJOS POR PRODUCTO\n# Ranking de productos (mismo orden en todos los países)\nproduct_ranking = {\n    'Using LLMs to Improve Your Coding': 1,\n    'Using LLMs to Train More LLMs': 2, \n    'Using LLMs to Win More Kaggle Competitions': 3,\n    'Using LLMs to Write Better': 4,\n    'Using LLMs to Win Friends and Influence People': 5\n}\ntrain1['product_rank'] = train1['product'].map(product_ranking)\ntest1['product_rank'] = test1['product'].map(product_ranking)\n# Valores base por producto (usando Argentina + Kaggle Learn como referencia)\nproduct_base_values = {\n    'Using LLMs to Improve Your Coding': 82.3,\n    'Using LLMs to Train More LLMs': 81.2,\n    'Using LLMs to Win More Kaggle Competitions': 68.8, \n    'Using LLMs to Write Better': 62.6,\n    'Using LLMs to Win Friends and Influence People': 12.4\n}\ntrain1['product_base_value'] = train1['product'].map(product_base_values)\ntrain1['product_base_value'] = train1['product'].map(product_base_values)\ntest1['product_base_value'] = test1['product'].map(product_base_values)\ntest1['product_base_value'] = test1['product'].map(product_base_values)\n# One-hot encoding\n#train1 = pd.get_dummies(train1, columns=['product'], prefix='product')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURES TEMPORALES\n# Tendencias temporales\ntrain1['year_trend'] = train1['Year'] - 2017  # 0,1,2,3,4\ntrain1['days_since_start'] = (train1['date'] - train1['date'].min()).dt.days\ntest1['year_trend'] = test1['Year'] - 2017\ntrain_min_date = pd.to_datetime('2017-01-01')\ntest1['days_since_start'] = (test1['date'] - train_min_date).dt.days\n\n# Seasonality dentro del año\ntrain1['month_sin'] = np.sin(2 * np.pi * train1['Month'] / 12)\ntrain1['month_cos'] = np.cos(2 * np.pi * train1['Month'] / 12)\ntrain1['day_of_year'] = train1['date'].dt.dayofyear\ntrain1['quarter_sin'] = np.sin(2 * np.pi * train1['Quarter'] / 4)\ntrain1['quarter_cos'] = np.cos(2 * np.pi * train1['Quarter'] / 4)\n\ntest1['month_sin'] = np.sin(2 * np.pi * test1['Month'] / 12)\ntest1['month_cos'] = np.cos(2 * np.pi * test1['Month'] / 12)\ntest1['day_of_year'] = test1['date'].dt.dayofyear\ntest1['quarter_sin'] = np.sin(2 * np.pi * test1['Quarter'] / 4)\ntest1['quarter_cos'] = np.cos(2 * np.pi * test1['Quarter'] / 4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#FEATURES DE INTERACCIÓN\n# Interacciones clave identificadas\ntrain1['country_store'] = train1['country_multiplier'] * train1['store_multiplier']\ntrain1['country_weekend'] = train1['country_multiplier'] * train1['weekend_factor']\ntrain1['store_weekend'] = train1['store_multiplier'] * train1['weekend_factor']\ntrain1['product_weekend'] = train1['product_base_value'] * train1['weekend_factor']\ntrain1['product_store'] = train1['product_base_value'] * train1['store_multiplier']\n\ntest1['country_store'] = test1['country_multiplier'] * test1['store_multiplier']\ntest1['country_weekend'] = test1['country_multiplier'] * test1['weekend_factor']\ntest1['store_weekend'] = test1['store_multiplier'] * test1['weekend_factor']\ntest1['product_weekend'] = test1['product_base_value'] * train1['weekend_factor']\ntest1['product_store'] = test1['product_base_value'] * test1['store_multiplier']\n\n\n\n\n# Interacción triple principal\ntrain1['country_store_product'] = (train1['country_multiplier'] * \n                                  train1['store_multiplier'] * \n                                  train1['product_base_value'])\n\ntest1['country_store_product'] = (test1['country_multiplier'] * \n                                  test1['store_multiplier'] * \n                                  test1['product_base_value'])\n\n\n# Interacción año-producto (por si hay trends específicos)\nfor year in [2017, 2018, 2019, 2020, 2021]:\n    train1[f'product_rank_year_{year}'] = (train1['product_rank'] * \n                                          (train1['Year'] == year).astype(int))\n\nfor year in [2017, 2018, 2019, 2020, 2021, 2022]:\n        test1[f'product_rank_year_{year}'] = (test1['product_rank'] * \n                                                      (test1['Year'] == year).astype(int))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LAG FEATURES\n# Ordenar por entidad y fecha\ntrain1 = train1.sort_values(['country', 'store', 'product', 'date'])\n\n# Lags por entidad (país-tienda-producto)\nfor lag in [1, 7, 30]:\n    train1[f'num_sold_lag_{lag}'] = (train1.groupby(['country', 'store', 'product'])\n                                    ['num_sold'].shift(lag))\n\n# Rolling statistics\nfor window in [7, 30]:\n    train1[f'num_sold_rolling_mean_{window}'] = (\n        train1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).mean())\n    )\n\n    train1[f'num_sold_rolling_std_{window}'] = (\n        train1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).std())\n    )\n\ntest1 = test1.sort_values(['country', 'store', 'product', 'date'])\n\n# Lags por entidad (país-tienda-producto)\nfor lag in [1, 7, 30]:\n    test1[f'num_sold_lag_{lag}'] = (test1.groupby(['country', 'store', 'product'])\n                                    ['num_sold'].shift(lag))\n\n# Rolling statistics\nfor window in [7, 30]:\n    test1[f'num_sold_rolling_mean_{window}'] = (\n        test1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).mean())\n    )\n\n    test1[f'num_sold_rolling_std_{window}'] = (\n        test1.groupby(['country', 'store', 'product'])['num_sold']\n        .transform(lambda x: x.rolling(window).std())\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURES TARGET TRANSFORMADAS\n# Para el régimen normal (alta demanda)\ntrain1['log_num_sold'] = np.log1p(train1['num_sold']) \n#train1['sqrt_num_sold'] = np.sqrt(train1['num_sold']) no lo uso x ahora\n\n#one hot al final\n#train1 = pd.get_dummies(train1, columns=['country'], prefix='country')\n#train1 = pd.get_dummies(train1, columns=['store'], prefix='store')\n#train1 = pd.get_dummies(train1, columns=['product'], prefix='product')\n\n#test1 = pd.get_dummies(test1, columns=['country'], prefix='country')\n#test1 = pd.get_dummies(test1, columns=['store'], prefix='store')\n#test1 = pd.get_dummies(test1, columns=['product'], prefix='product')\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURES FINALES PARA MODELO\n# Lista de features para usar en el modelo\nbase_features = [\n    # Efectos fijos multiplicativos\n    'country_multiplier', 'store_multiplier', 'product_base_value', 'product_rank',\n    \n    # Estacionalidad \n    'weekend_factor', 'is_weekend', 'is_friday',\n    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n    \n    # Temporales\n    'year_trend', 'days_since_start',\n    \n    # Interacciones clave\n    'country_store', 'country_weekend', 'store_weekend', \n    'product_weekend', 'product_store', 'country_store_product',\n    \n    # Base esperada y desviaciones\n    'expected_base', 'deviation_from_expected',\n    \n    # Lags (si disponibles)\n    'num_sold_lag_1', 'num_sold_lag_7', 'num_sold_rolling_mean_7'\n]\n\n# Features categóricas (one-hot encoded)\ncategorical_features = [col for col in train1.columns \n                       if col.startswith(('country_', 'product_', 'store_'))]\n\n# Features finales\nmodel_features = base_features + categorical_features\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test1.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#eliminar los NaN/ reemplzar por 0\n# no saque la columna ID que no aporta poder predictivo\n\nfeatures_con_nans = [col for col in train1.columns if 'lag' in col or 'rolling' in col]\ntrain1[features_con_nans] = train1[features_con_nans].fillna(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nmodel_features = [\n    'country_multiplier', 'store_multiplier', 'product_base_value', 'product_rank',\n    'weekend_factor', 'is_weekend', 'is_friday',\n    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n    'year_trend', 'days_since_start',\n    'country_store', 'country_weekend', 'store_weekend', \n    'product_weekend', 'product_store', 'country_store_product',\n    'num_sold_lag_1', 'num_sold_lag_7', 'num_sold_lag_30',\n    'num_sold_rolling_mean_7', 'num_sold_rolling_std_7',\n    'num_sold_rolling_mean_30', 'num_sold_rolling_std_30'\n]\n\nX = train1[model_features]\ny = train1['log_num_sold']\n\nX_test = test1[model_features]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  fecha de corte voy a usar 3 meses solamente de validacion\nfecha_corte = '2021-09-30'\nindices_train = train1[train1['date'] < fecha_corte].index\nindices_val = train1[train1['date'] >= fecha_corte].index\n\n# DATA LEAKEAGE , despues termine viendo que las variables de lag y rolling producieron data LEAKEAGE\n# asi que lo que probe en el test de validacion esta mal ,\n# Separar X e y\nX_train, X_val = X.loc[indices_train], X.loc[indices_val]\ny_train, y_val = y.loc[indices_train], y.loc[indices_val]\n\nprint(f\"Datos de entrenamiento: {len(X_train)} filas\")\nprint(f\"Datos de validación: {len(X_val)} filas\")\n\"\"\"\n# Valores prediciendo los ultimos 3 meses del 2021\n\n[LightGBM] [Info] Total Bins 2271\n[LightGBM] [Info] Number of data points in the train set: 129975, number of used features: 26\n[LightGBM] [Info] Start training from score 4.534541\nError del modelo (RMSE) en el conjunto de validación: 21.39\nError del modelo (SMAPE): 5.41%\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_reg = lgb.LGBMRegressor(random_state=1722, n_jobs=-1)\nlgbm_reg.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# Hacer predicciones sobre el conjunto de validación\nlog_predicciones = lgbm_reg.predict(X_val)\n\n# Convertir las predicciones de vuelta a la escala original\n# Si tu target 'y' era log_num_sold, usá np.expm1()\npredicciones_finales = np.expm1(log_predicciones)\n\n# Obtener los valores reales también en la escala original\nvalores_reales = np.expm1(y_val)\n\n\nrmse = np.sqrt(mean_squared_error(valores_reales, predicciones_finales))\nprint(f\"\\nError del modelo (RMSE) en el conjunto de validación: {rmse:.2f}\")\n\n\nplt.figure(figsize=(15, 6))\nplt.plot(train1.loc[indices_val, 'date'], valores_reales, label='Valores Reales', alpha=0.7)\nplt.plot(train1.loc[indices_val, 'date'], predicciones_finales, label='Predicciones del Modelo', linestyle='--')\nplt.title('Comparación de Valores Reales vs. Predicciones')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0  \n    return np.mean(diff) * 100\n# Calcular SMAPE\nsmape_error = smape(valores_reales, predicciones_finales)\nprint(f\"Error del modelo (SMAPE): {smape_error:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Modelo para enviar a kaggle\n# voy a tener que ir prediciendo de a 1 mes por la forma en que arme mis variables\n\nlgbm_1 = lgb.LGBMRegressor(random_state=1722, n_jobs=-1)\nlgbm_1.fit(X, y)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dividir X_test por meses usando days_since_start\nmonthly_tests = {}\ninicio_2022 = 1795  # tu valor mínimo\ndias_acumulados = 0\ndias_por_mes = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\nfor month in range(1, 13):\n   inicio_mes = inicio_2022 + dias_acumulados\n   fin_mes = inicio_2022 + dias_acumulados + dias_por_mes[month-1] - 1\n   \n   mask = (X_test['days_since_start'] >= inicio_mes) & (X_test['days_since_start'] <= fin_mes)\n   monthly_tests[f'month_{month}'] = X_test[mask]\n   \n   dias_acumulados += dias_por_mes[month-1]\n\n# Inicializar historial con datos reales hasta diciembre 2021\nhistorial = train1.copy()\npredicciones_finales = []\n\n# Loop iterativo\nfor month in range(1, 13):\n    print(f\"=== MES {month} ===\")\n    \n    # Predecir mes actual\n    X_mes_actual = monthly_tests[f'month_{month}']\n    print(f\"Filas a predecir: {len(X_mes_actual)}\")\n    \n    pred_mes = lgbm_1.predict(X_mes_actual)\n    print(f\"Predicciones generadas: {len(pred_mes)}\")\n    print(f\"Ejemplo predicciones: {pred_mes[:5]}\")  # primeras 5\n    \n    # Agregar a lista final\n    predicciones_finales.extend(pred_mes)\n    \n    # Agregar predicciones al historial para próximos lags\n    # (actualizar historial con pred_mes) - POR IMPLEMENTAR\n    \n    # Recalcular lags para próximo mes  \n    # (calcular nuevos lags usando historial actualizado) - POR IMPLEMENTAR\n\nprint(f\"\\n=== RESUMEN FINAL ===\")  \nprint(f\"Total predicciones: {len(predicciones_finales)}\")\nprint(f\"Primeras 10 predicciones: {predicciones_finales[:10]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Aplicar exponencial a las predicciones\npredicciones_finales_exp = [np.exp(pred) for pred in predicciones_finales]\n\n# 2. Crear DataFrame con las predicciones\npred_df = pd.DataFrame({\n    'num_sold': predicciones_finales_exp\n})\n\n# 3. Agregar las columnas originales desde test1\npred_df_completo = pd.concat([\n    test1[['date', 'country', 'store', 'product']].reset_index(drop=True),\n    pred_df\n], axis=1)\n\n# 4. Convertir fechas al mismo formato antes del merge\ntest_merge = test[['id', 'date', 'country', 'store', 'product']].copy()\ntest_merge['date'] = pd.to_datetime(test_merge['date'])\npred_df_completo['date'] = pd.to_datetime(pred_df_completo['date'])\n\n# 5. Hacer merge\nsubmission = test_merge.merge(\n    pred_df_completo,\n    on=['date', 'country', 'store', 'product'],\n    how='left'\n)\n\n# 6. Submission final\nsubmission_final = submission[['id', 'num_sold']]\nprint(f\"Submission shape: {submission_final.shape}\")\nprint(submission_final.head())\n\n# 7. Guardar para Kaggle\nsubmission_final.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {submission_final.shape}\")\nprint(f\"Columnas: {submission_final.columns.tolist()}\")\nprint(f\"Tipos: {submission_final.dtypes}\")\nprint(f\"Valores nulos: {submission_final.isnull().sum()}\")\nprint(submission_final.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#podria hacer ensemble/DNN y hacer que prediga 75 variables entonces es analizar cada pais/tienda/producto y ya sea hacer una variable aleatoria de c/u\n# osea ejemplo ,para ARG/TIENDA1/Producto 1 , y esto es una variable conocida o combinacion o hago que la red aprenda como esta distribuido en los \n# cuartiles y asi la forme , pero la pag hoy no esta andando bien y no deja ver como quedo en relacion al leaderboard(fijo)\n# tmb en este mismo notebook quedaron muchas cosas por probar , con las variables one-hot y mas","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}