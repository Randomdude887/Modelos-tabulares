{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:24:57.209232Z","iopub.execute_input":"2025-08-30T18:24:57.209622Z","iopub.status.idle":"2025-08-30T18:24:57.698876Z","shell.execute_reply.started":"2025-08-30T18:24:57.209594Z","shell.execute_reply":"2025-08-30T18:24:57.697390Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/public_timeseries_testing_util.py\n/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/county_id_to_name_map.json\n/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/sample_submission.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CORE DATA MANIPULATION\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import pearsonr, spearmanr\nimport math\n\n# MACHINE LEARNING - SCIKIT-LEARN\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, roc_curve, auc\nfrom sklearn.pipeline import Pipeline\n\n# ADVANCED ML LIBRARIES\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# VISUALIZATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\n\n# CONFIGURATION FOR PLOTS\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n\n# DEEP LEARNING \ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n    print(\"TensorFlow disponible\")\nexcept ImportError:\n    print(\"TensorFlow no disponible en este entorno\")\n\n\n# UTILITIES\nimport os\nimport sys\nimport warnings\nimport itertools\nfrom datetime import datetime, timedelta\nimport time\nfrom collections import Counter\nimport pickle\nimport joblib\n\n# JUPYTER SPECIFIC\nfrom IPython.display import display, HTML\nfrom tqdm.notebook import tqdm\n\n# SUPPRESS WARNINGS\nwarnings.filterwarnings('ignore')\n\n# PANDAS CONFIGURATION\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n# NUMPY CONFIGURATION\nnp.random.seed(42)\n\n# PLOTLY CONFIGURATION\npyo.init_notebook_mode(connected=True)\n\nprint(\"✅ Todas las librerías importadas correctamente!\")\nprint(f\"📊 Pandas version: {pd.__version__}\")\nprint(f\"🔢 NumPy version: {np.__version__}\")\nprint(f\"🤖 Scikit-learn version: {__import__('sklearn').__version__}\")\nprint(f\"📈 Matplotlib version: {__import__('matplotlib').__version__}\")\nprint(f\"🎨 Seaborn version: {sns.__version__}\")\n\n# ============================================================================\n# FUNCIONES ÚTILES ADICIONALES\n# ============================================================================\n\ndef quick_info(df):\n    \"\"\"Información rápida del dataset\"\"\"\n    print(f\"📊 Dataset Shape: {df.shape}\")\n    print(f\"🔢 Columnas numéricas: {df.select_dtypes(include=[np.number]).columns.tolist()}\")\n    print(f\"📝 Columnas categóricas: {df.select_dtypes(include=['object']).columns.tolist()}\")\n    print(f\"❌ Valores nulos por columna:\")\n    print(df.isnull().sum()[df.isnull().sum() > 0])\n\ndef plot_missing_values(df):\n    \"\"\"Visualizar valores faltantes\"\"\"\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    if len(missing) > 0:\n        plt.figure(figsize=(10, 6))\n        missing.plot(kind='bar')\n        plt.title('Valores Faltantes por Columna')\n        plt.ylabel('Cantidad')\n        plt.xticks(rotation=45)\n        plt.show()\n    else:\n        print(\"✅ No hay valores faltantes en el dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:24:57.702558Z","iopub.execute_input":"2025-08-30T18:24:57.703643Z","iopub.status.idle":"2025-08-30T18:25:29.957678Z","shell.execute_reply.started":"2025-08-30T18:24:57.703606Z","shell.execute_reply":"2025-08-30T18:25:29.956620Z"}},"outputs":[{"name":"stderr","text":"2025-08-30 18:25:11.464770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756578311.743838      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756578311.836830      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow disponible\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"name":"stdout","text":"✅ Todas las librerías importadas correctamente!\n📊 Pandas version: 2.2.3\n🔢 NumPy version: 1.26.4\n🤖 Scikit-learn version: 1.2.2\n📈 Matplotlib version: 3.7.2\n🎨 Seaborn version: 0.12.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#competencia https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/overview\n\n# Files\n# train.csv\n\n# county - An ID code for the county.\n# is_business - Boolean for whether or not the prosumer is a business.\n# product_type - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n# target - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n# is_consumption - Boolean for whether or not this row's target is consumption or production.\n# datetime - The Estonian time in EET (UTC+2) / EEST (UTC+3). It describes the start of the 1-hour period on which target is given.\n# data_block_id - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n# row_id - A unique identifier for the row.\n# prediction_unit_id - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n# gas_prices.csv\n\n# origin_date - The date when the day-ahead prices became available.\n# forecast_date - The date when the forecast prices should be relevant.\n# [lowest/highest]_price_per_mwh - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n# data_block_id\n# client.csv\n\n# product_type\n# county - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n# eic_count - The aggregated number of consumption points (EICs - European Identifier Code).\n# installed_capacity - Installed photovoltaic solar panel capacity in kilowatts.\n# is_business - Boolean for whether or not the prosumer is a business.\n# date\n# data_block_id\n# electricity_prices.csv\n\n# origin_date\n# forecast_date - Represents the start of the 1-hour period when the price is valid\n# euros_per_mwh - The price of electricity on the day ahead markets in euros per megawatt hour.\n# data_block_id\n# forecast_weather.csv Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n\n# [latitude/longitude] - The coordinates of the weather forecast.\n# origin_datetime - The timestamp of when the forecast was generated.\n# hours_ahead - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n# temperature - The air temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# dewpoint - The dew point temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# cloudcover_[low/mid/high/total] - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total. Estimated for the end of the 1-hour period.\n# 10_metre_[u/v]_wind_component - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second. Estimated for the end of the 1-hour period.\n# data_block_id\n# forecast_datetime - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead. This represents the start of the 1-hour period for which weather data are forecasted.\n# direct_solar_radiation - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the hour, in watt-hours per square meter.\n# surface_solar_radiation_downwards - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, accumulated during the hour, in watt-hours per square meter.\n# snowfall - Snowfall over hour in units of meters of water equivalent.\n# total_precipitation - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the described hour, in units of meters.\n# historical_weather.csv Historic weather data.\n\n# datetime - This represents the start of the 1-hour period for which weather data are measured.\n# temperature - Measured at the end of the 1-hour period.\n# dewpoint - Measured at the end of the 1-hour period.\n# rain - Different from the forecast conventions. The rain from large scale weather systems of the hour in millimeters.\n# snowfall - Different from the forecast conventions. Snowfall over the hour in centimeters.\n# surface_pressure - The air pressure at surface in hectopascals.\n# cloudcover_[low/mid/high/total] - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n# windspeed_10m - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n# winddirection_10m - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n# shortwave_radiation - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n# direct_solar_radiation\n# diffuse_radiation - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n# [latitude/longitude] - The coordinates of the weather station.\n# data_block_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:29.958869Z","iopub.execute_input":"2025-08-30T18:25:29.959280Z","iopub.status.idle":"2025-08-30T18:25:29.966517Z","shell.execute_reply.started":"2025-08-30T18:25:29.959248Z","shell.execute_reply":"2025-08-30T18:25:29.965471Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#modelos a correr: NN por ser forecasting y algun xgb o lgb\n#son 4 dias a predecir el comportamiento de los prosumers\n#predecir con rolling de a 1 dia\n\n\n#analizar y preparar\n# como siempre primero ver los faltantes y los NaN y en cada caso decidir que hacer\n# atento con valores expresados para el dia actual o la hora actual / la hora anterior / el dia de mañana , ver de no malinterpretar\n# ver la correlacion de precios sea gas y/o electricidad para ver como se comparta con business y no business\n# lo mismo con el clima para business y no business\n# ver si aplica el supuesto de que los business si operan bajo esos valores (costos de electricidad/gas) y los no business no es tan relevante\n# clima historical vs forecast , adaptar columnas que usan distintas proporciones , medir la certeza del forecast\n# analizar por fuera como funcionan los paneles y como miden lo que miden ( para mejor entendimiento del df)\n\n#columnas a agregar\n#Datetime dias lu-1/ma-2/mi-3/ju-4/vi-5/sa-6/do-7 \n#Date time horas y capaz minutos\n#horario laboral ejemplo 7 a 17hs (analizarlo) sea en el dataset y por fuera\n#horario business , para los que son business ver un pseudo horario de apertura y cierre como afecta y si se puede\n#Dia 6hs a 18hs / noche de 18hs a 6hs ver si lo adapto con 4 columnas por las estaciones\n#hora de dormir ejemplo 22hs a 6hs\n#dias festivos y vacaciones , columnas binarias en ambos\n#estaciones del año 4 columnas binarias\n#periodogram para ver los lags y sea mensuales/quincenales/diarios/hora\n#ver si es relevante agregar alguna columna con la poblacion mensual y usar un ratio (info a buscar afuera)\n\n#sugerencias de features\n\n##Energéticas:\n\n# Ratio production/consumption por prediction_unit_id (histórico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n##Temporales:\n\n# Lag de consumo/producción del mismo prediction_unit_id (24h, 48h, 168h)\n# Media móvil de target por prediction_unit_id\n# Cambios día a día (delta vs día anterior)\n\n##Weather engineering:\n\n# Índice de confort térmico (combinando temp + humidity)\n# Potencial solar real (direct + diffuse radiation ajustado por cloudcover)\n# Diferencia forecast vs historical weather (para medir accuracy del forecast)\n\n##Segmentación:\n\n# Clustering de prediction_units por comportamiento similar\n# Ratios county-level (agregaciones por county vs individual)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:29.968895Z","iopub.execute_input":"2025-08-30T18:25:29.969298Z","iopub.status.idle":"2025-08-30T18:25:30.005402Z","shell.execute_reply.started":"2025-08-30T18:25:29.969270Z","shell.execute_reply":"2025-08-30T18:25:30.004071Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#county_id_to_name_map.json\n# {\n#   \"0\": \"HARJUMAA\",\n#   \"1\": \"HIIUMAA\",\n#   \"2\": \"IDA-VIRUMAA\",\n#   \"3\": \"JÄRVAMAA\",\n#   \"4\": \"JÕGEVAMAA\",\n#   \"5\": \"LÄÄNE-VIRUMAA\",\n#   \"6\": \"LÄÄNEMAA\",\n#   \"7\": \"PÄRNUMAA\",\n#   \"8\": \"PÕLVAMAA\",\n#   \"9\": \"RAPLAMAA\",\n#   \"10\": \"SAAREMAA\",\n#   \"11\": \"TARTUMAA\",\n#   \"12\": \"UNKNOWN\",\n#   \"13\": \"VALGAMAA\",\n#   \"14\": \"VILJANDIMAA\",\n#   \"15\": \"VÕRUMAA\"\n# }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:30.006725Z","iopub.execute_input":"2025-08-30T18:25:30.007129Z","iopub.status.idle":"2025-08-30T18:25:30.038668Z","shell.execute_reply.started":"2025-08-30T18:25:30.007104Z","shell.execute_reply":"2025-08-30T18:25:30.037474Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train                 = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\nclient                = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\nelectricity_prices    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\nforecast_weather      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\ngas_prices            = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\nhistorical_weather    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\nweather_station       = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv')\n\ntest                  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv')\nclient_t              = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv')\nelectricity_prices_t  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv')\nforecast_weather_t    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv')\ngas_prices_t          = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv')\nrevealed_targets      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:30.039653Z","iopub.execute_input":"2025-08-30T18:25:30.039986Z","iopub.status.idle":"2025-08-30T18:25:55.471283Z","shell.execute_reply.started":"2025-08-30T18:25:30.039963Z","shell.execute_reply":"2025-08-30T18:25:55.470301Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# pruebo pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:55.472737Z","iopub.execute_input":"2025-08-30T18:25:55.473151Z","iopub.status.idle":"2025-08-30T18:25:55.478610Z","shell.execute_reply.started":"2025-08-30T18:25:55.473119Z","shell.execute_reply":"2025-08-30T18:25:55.477431Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport unicodedata\nfrom sklearn.neighbors import NearestNeighbors\nimport gc\nfrom collections import defaultdict\nfrom pathlib import Path\n\nclass DataPreprocessingPipeline:\n    \"\"\"Pipeline modular para preprocessing de datos de train y test\"\"\"\n    \n    def __init__(self):\n        self.county_id_to_name_map = {\n            0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"JÄRVAMAA\",\n            4: \"JÕGEVAMAA\", 5: \"LÄÄNE-VIRUMAA\", 6: \"LÄÄNEMAA\", 7: \"PÄRNUMAA\",\n            8: \"PÕLVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n            12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"VÕRUMAA\"\n        }\n        self.county_name_to_id_map = {v: k for k, v in self.county_id_to_name_map.items()}\n        \n        # Diccionario de sinónimos para normalización\n        self.synonyms_to_canonical = {\n            \"HARJU\": \"HARJUMAA\", \"HIIU\": \"HIIUMAA\", \"IDA-VIRU\": \"IDA-VIRUMAA\",\n            \"JARVA\": \"JÄRVAMAA\", \"JÄRVA\": \"JÄRVAMAA\", \"JOGEVA\": \"JÕGEVAMAA\", \n            \"JÕGEVA\": \"JÕGEVAMAA\", \"LAANE-VIRU\": \"LÄÄNE-VIRUMAA\", \n            \"LÄÄNE-VIRU\": \"LÄÄNE-VIRUMAA\", \"LAANE\": \"LÄÄNEMAA\", \"LÄÄNE\": \"LÄÄNEMAA\",\n            \"PARNU\": \"PÄRNUMAA\", \"PÄRNU\": \"PÄRNUMAA\", \"POLVA\": \"PÕLVAMAA\", \n            \"PÕLVA\": \"PÕLVAMAA\", \"RAPLA\": \"RAPLAMAA\", \"SAARE\": \"SAAREMAA\",\n            \"TARTU\": \"TARTUMAA\", \"VALGA\": \"VALGAMAA\", \"VILJANDI\": \"VILJANDIMAA\",\n            \"VORU\": \"VÕRUMAA\", \"VÕRU\": \"VÕRUMAA\",\n            # versiones ya canónicas\n            **{name: name for name in self.county_id_to_name_map.values()}\n        }\n    \n    def strip_accents(self, s: str) -> str:\n        \"\"\"Eliminar acentos de string\"\"\"\n        return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n    \n    def normalize_county_name(self, x):\n        \"\"\"Normalizar nombre de county\"\"\"\n        if pd.isna(x):\n            return np.nan\n        s = str(x).upper().strip()\n        # eliminar sufijos frecuentes\n        for suf in [\" COUNTY\", \" MAAKOND\"]:\n            if s.endswith(suf):\n                s = s[: -len(suf)]\n        s = s.replace(\"  \", \" \").replace(\"–\", \"-\")\n        s_noacc = self.strip_accents(s)\n        s_noacc = s_noacc.replace(\"  \", \" \")\n        \n        token = s_noacc.split()[0]\n        if \"-\" in s_noacc:\n            token = s_noacc\n        \n        # intentos de match\n        if s_noacc in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s_noacc]\n        if token in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[token]\n        if s in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s]\n        \n        return np.nan\n    \n    def process_weather_stations(self, weather_station):\n        \"\"\"Procesar estaciones meteorológicas para asignar counties\"\"\"\n        weather_station = weather_station.copy()\n        \n        # Normalizar nombres de county\n        weather_station['county_name_norm'] = weather_station['county_name'].apply(self.normalize_county_name)\n        \n        # Si tengo 'county' numérico pero falta nombre, lo inferimos del ID\n        mask = weather_station['county_name_norm'].isna() & weather_station['county'].notna()\n        weather_station.loc[mask, 'county_name_norm'] = (\n            weather_station.loc[mask, 'county'].astype(int).map(self.county_id_to_name_map)\n        )\n        \n        # Completar faltantes por k-NN\n        fcols = ['latitude', 'longitude']\n        known = weather_station.dropna(subset=['county_name_norm']).copy()\n        unknown = weather_station[weather_station['county_name_norm'].isna()].copy()\n        \n        if not unknown.empty and not known.empty:\n            known_rad = np.radians(known[fcols].values)\n            unknown_rad = np.radians(unknown[fcols].values)\n            \n            nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n            nbrs.fit(known_rad)\n            distances, idxs = nbrs.kneighbors(unknown_rad)\n            \n            nn_names = known['county_name_norm'].values\n            filled = []\n            for neigh_idx in idxs:\n                cands = nn_names[neigh_idx]\n                vals, counts = np.unique(cands, return_counts=True)\n                filled.append(vals[np.argmax(counts)])\n            \n            weather_station.loc[unknown.index, 'county_name_norm'] = filled\n        \n        # Sincronizar columnas finales\n        weather_station['county_name'] = weather_station['county_name_norm']\n        weather_station['county'] = weather_station['county_name'].map(self.county_name_to_id_map).astype('Int64')\n        \n        # Cualquier remanente a UNKNOWN (12)\n        weather_station.loc[weather_station['county_name'].isna(), 'county'] = 12\n        weather_station['county'] = weather_station['county'].astype('Int64')\n        \n        weather_station.drop(columns=['county_name_norm'], inplace=True)\n        \n        return weather_station\n    \n    def optimize_dtypes(self, df, is_train=True):\n        \"\"\"Optimizar tipos de datos para reducir memoria\"\"\"\n        df = df.copy()\n        \n        # Tipos comunes\n        if 'county' in df.columns:\n            df['county'] = df['county'].astype('uint8')\n        if 'is_business' in df.columns:\n            df['is_business'] = df['is_business'].astype('uint8')\n        if 'product_type' in df.columns:\n            df['product_type'] = df['product_type'].astype('uint8')\n        if 'is_consumption' in df.columns:\n            df['is_consumption'] = df['is_consumption'].astype('uint8')\n        if 'hour' in df.columns:\n            df['hour'] = df['hour'].astype('uint8')\n        \n        # IDs\n        if 'data_block_id' in df.columns:\n            df['data_block_id'] = df['data_block_id'].astype('uint16')\n        if 'row_id' in df.columns:\n            df['row_id'] = df['row_id'].astype('uint32')\n        if 'prediction_unit_id' in df.columns:\n            df['prediction_unit_id'] = df['prediction_unit_id'].astype('uint8')\n        \n        # Coordenadas y variables meteorológicas - float32\n        float_cols = ['latitude', 'longitude', 'target', 'lowest_price_per_mwh', \n                     'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', \n                     'installed_capacity']\n        \n        # Variables meteorológicas\n        weather_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                       'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                       'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                       'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                       'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                       'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                       'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                       'direct_solar_radiation', 'diffuse_radiation']\n        \n        # Aplicar float32 a columnas que existen\n        for col in float_cols + weather_cols:\n            if col in df.columns:\n                df[col] = df[col].astype('float32')\n        \n        # Weather forecast hour\n        if 'weather_forecast_hour' in df.columns:\n            df['weather_forecast_hour'] = df['weather_forecast_hour'].astype('uint8')\n        \n        return df\n\n\nclass TrainPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline específico para datos de entrenamiento\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.temp_dir = Path(\"temp_chunks\")\n        self.temp_dir.mkdir(exist_ok=True)\n    \n    def part1_prepare_base_merges(self, train, gas_prices, electricity_prices, client):\n        \"\"\"Parte 1: Preparar datos base y hacer primeros merges\"\"\"\n        print(\"=== PARTE 1: Preparación y merges base ===\")\n        \n        # Preparar copias\n        train1 = train.dropna().copy()\n        gas_prices1 = gas_prices.copy()\n        electricity_prices1 = electricity_prices.copy()\n        client1 = client.copy()\n        \n        # Procesar train\n        train1['datetime'] = pd.to_datetime(train1['datetime'])\n        train1['hour'] = train1['datetime'].dt.hour\n        train1['forecast_date'] = train1['datetime']\n        \n        # Merge 1: Gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        gas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n        \n        # Expandir gas a nivel horario\n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices1_hourly = pd.DataFrame(gas_hourly)\n        train2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')\n        \n        # Forward fill gas prices\n        train2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\n        train2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge 2: Electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        elec_columns = ['forecast_date', 'euros_per_mwh']\n        train3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')\n        train3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge 3: Client data\n        print(\"Procesando client data...\")\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        train4 = train3.merge(client1.drop('data_block_id', axis=1),\n                             left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                             right_on=['product_type', 'county', 'is_business', 'date'],\n                             how='left')\n        \n        train4 = train4.drop('date', axis=1)\n        train4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        print(\"Completando datos de clientes...\")\n        train4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        train4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        train4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        train4['eic_count'] = train4['eic_count'].fillna(train4.groupby('county')['eic_count'].transform('mean'))\n        train4['installed_capacity'] = train4['installed_capacity'].fillna(train4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Optimizar tipos\n        train4 = self.optimize_dtypes(train4, is_train=True)\n        \n        print(f\"Parte 1 completada. Dataset: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")\n        return train4\n    \n    def part2_prepare_weather_merge(self, train4, forecast_weather, historical_weather, weather_station):\n        \"\"\"Parte 2: Preparar datos meteorológicos y hacer merge\"\"\"\n        print(\"=== PARTE 2: Preparación datos meteorológicos ===\")\n        \n        # Procesar weather stations\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        forecast_weather1 = forecast_weather.dropna().copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Merge con historical weather\n        print(\"Mergeando forecast con historical weather...\")\n        historical_weather1 = historical_weather.copy()\n        historical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n        \n        forecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n        \n        merged_weather = pd.merge(\n            forecast_weather2,\n            historical_weather1,\n            left_on=['latitude', 'longitude', 'forecast_datetime'],\n            right_on=['latitude', 'longitude', 'datetime'],\n            how='inner'\n        )\n        \n        # Limpiar y optimizar\n        merged_weather = merged_weather.drop(['f_data_block_id', 'forecast_datetime'], axis=1)\n        merged_weather = self.optimize_dtypes(merged_weather, is_train=True)\n        \n        print(f\"Weather data preparado: {merged_weather.shape[0]:,} filas, {merged_weather.shape[1]} columnas\")\n        return train4, merged_weather\n    \n    def part3_final_merge_and_cleanup(self, train4, merged_weather, chunk_size=1_000_000):\n        \"\"\"Parte 3: Merge final y limpieza\"\"\"\n        print(\"=== PARTE 3: Merge final y limpieza ===\")\n        \n        # Definir períodos para procesar por chunks\n        periods = [\n            ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), \n            ('2021-12-01', '2022-01-01'), ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), \n            ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'), ('2022-05-01', '2022-06-01'), \n            ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n            ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), \n            ('2022-12-01', '2023-01-01'), ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), \n            ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'), ('2023-05-01', '2023-05-31')\n        ]\n        \n        # Procesar por chunks temporales\n        chunk_files = []\n        for i, (start_date, end_date) in enumerate(periods):\n            print(f\"Procesando período {i+1}/{len(periods)}: {start_date} a {end_date}\")\n            \n            # Filtrar por período\n            mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n            train4_chunk = train4[mask_train].copy()\n            \n            mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n            weather_chunk = merged_weather[mask_weather].copy()\n            \n            if len(train4_chunk) > 0 and len(weather_chunk) > 0:\n                # Merge chunk\n                chunk_result = pd.merge(train4_chunk, weather_chunk, \n                               on=['county', 'datetime'], \n                               how='left')\n                \n                # Guardar chunk temporal\n                chunk_file = self.temp_dir / f'train5_chunk_{i+1}.parquet'\n                chunk_result.to_parquet(chunk_file, index=False)\n                chunk_files.append(chunk_file)\n                \n                print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n                \n                # Liberar memoria\n                del train4_chunk, weather_chunk, chunk_result\n                gc.collect()\n        \n        # Concatenar chunks\n        print(\"Concatenando chunks...\")\n        final_chunks = []\n        for chunk_file in chunk_files:\n            chunk_data = pd.read_parquet(chunk_file)\n            final_chunks.append(chunk_data)\n        \n        train5 = pd.concat(final_chunks, ignore_index=True)\n        \n        # Limpieza final\n        print(\"Aplicando limpieza final...\")\n        train5 = self._cleanup_merged_data(train5, chunk_size)\n        \n        # Limpiar archivos temporales\n        for chunk_file in chunk_files:\n            chunk_file.unlink()\n        \n        return train5\n    \n    def _cleanup_merged_data(self, train5, chunk_size=1_000_000):\n        \"\"\"Limpiar datos después del merge\"\"\"\n        # Identificar columnas a eliminar y renombrar\n        sample = train5.head(1000)\n        cols_to_drop = [col for col in sample.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\n        \n        print(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n        \n        # Procesar por chunks\n        chunks_processed = []\n        total_rows = len(train5)\n        n_chunks = (total_rows // chunk_size) + 1\n        \n        for i in range(n_chunks):\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, total_rows)\n            \n            print(f\"Procesando chunk limpieza {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n            \n            chunk = train5.iloc[start_idx:end_idx].copy()\n            \n            # Filtrar county != 12\n            chunk = chunk[chunk['county'] != 12]\n            \n            if len(chunk) > 0:\n                # Eliminar columnas _y y renombrar _x\n                chunk = chunk.drop(columns=cols_to_drop)\n                chunk.rename(columns=rename_dict, inplace=True)\n                chunks_processed.append(chunk)\n            \n            del chunk\n            gc.collect()\n        \n        # Concatenar final\n        train5_clean = pd.concat(chunks_processed, ignore_index=True)\n        del chunks_processed\n        gc.collect()\n        \n        # Eliminar datos de clima faltantes\n        weather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n        initial_rows = len(train5_clean)\n        train5_clean = train5_clean.dropna(subset=weather_key_cols)\n        print(f\"Filas eliminadas por clima faltante: {initial_rows - len(train5_clean):,}\")\n        \n        # Reordenar columnas\n        train5_clean = self._reorder_columns(train5_clean)\n        \n        return train5_clean\n    \n    def _reorder_columns(self, df):\n        \"\"\"Reordenar columnas en el orden especificado\"\"\"\n        main_cols = [\n            'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = main_cols.copy()\n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n    \n    def run_full_pipeline(self, train, gas_prices, electricity_prices, client, \n                         forecast_weather, historical_weather, weather_station):\n        \"\"\"Ejecutar pipeline completo de entrenamiento\"\"\"\n        print(\"Iniciando pipeline completo de entrenamiento...\")\n        \n        # Parte 1: Merges base\n        train4 = self.part1_prepare_base_merges(train, gas_prices, electricity_prices, client)\n        \n        # Parte 2: Preparar datos meteorológicos\n        train4, merged_weather = self.part2_prepare_weather_merge(\n            train4, forecast_weather, historical_weather, weather_station\n        )\n        \n        # Parte 3: Merge final y limpieza\n        train5 = self.part3_final_merge_and_cleanup(train4, merged_weather)\n        \n        print(f\"Pipeline completado! Dataset final: {train5.shape[0]:,} filas, {train5.shape[1]} columnas\")\n        return train5\n\n\nclass TestPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline simplificado para datos de test\"\"\"\n    \n    def run_test_pipeline(self, test, gas_prices_t, electricity_prices_t, client_t, \n                         forecast_weather_t, weather_station):\n        \"\"\"Pipeline completo para test set (más simple, sin historical weather)\"\"\"\n        print(\"=== PIPELINE TEST ===\")\n        \n        # Preparar datos base\n        test1 = test.copy()\n        test1['datetime'] = pd.to_datetime(test1['datetime'])\n        test1['hour'] = test1['datetime'].dt.hour\n        test1['forecast_date'] = test1['datetime']\n        \n        # Merge gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1 = gas_prices_t.copy()\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        \n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices_hourly = pd.DataFrame(gas_hourly)\n        test2 = pd.merge(test1, gas_prices_hourly, on='forecast_date', how='left')\n        test2['lowest_price_per_mwh'] = test2['lowest_price_per_mwh'].fillna(method='ffill')\n        test2['highest_price_per_mwh'] = test2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1 = electricity_prices_t.copy()\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        test3 = pd.merge(test2, electricity_prices1[['forecast_date', 'euros_per_mwh']], \n                        on='forecast_date', how='left')\n        test3['euros_per_mwh'] = test3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge client data\n        print(\"Procesando client data...\")\n        client1 = client_t.copy()\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        test4 = test3.merge(client1.drop('data_block_id', axis=1),\n                           left_on=['product_type', 'county', 'is_business', test3['datetime'].dt.date],\n                           right_on=['product_type', 'county', 'is_business', 'date'],\n                           how='left')\n        \n        test4 = test4.drop('date', axis=1)\n        test4 = test4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        test4 = test4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        test4['eic_count'] = test4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        test4['installed_capacity'] = test4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        test4['eic_count'] = test4['eic_count'].fillna(test4.groupby('county')['eic_count'].transform('mean'))\n        test4['installed_capacity'] = test4['installed_capacity'].fillna(test4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        forecast_weather1 = forecast_weather_t.copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Para test, no tenemos historical weather, así que creamos columnas dummy o usamos solo forecast\n        # Opción 1: Solo usar forecast weather (renombrar las f columns)\n        # Opción 2: Crear columnas históricas como NaN y llenar después\n        \n        # Vamos con opción 1: usar forecast como histórico también (aproximación)\n        historical_cols_mapping = {\n            'ftemperature': 'temperature',\n            'fdewpoint': 'dewpoint', \n            'fcloudcover_high': 'cloudcover_high',\n            'fcloudcover_low': 'cloudcover_low',\n            'fcloudcover_mid': 'cloudcover_mid', \n            'fcloudcover_total': 'cloudcover_total',\n            'fdirect_solar_radiation': 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards': 'shortwave_radiation',\n            'fsnowfall': 'snowfall',\n            'ftotal_precipitation': 'rain'\n        }\n        \n        # Crear versiones históricas basadas en forecast\n        for fcol, hcol in historical_cols_mapping.items():\n            if fcol in forecast_weather2.columns:\n                forecast_weather2[hcol] = forecast_weather2[fcol]\n        \n        # Agregar columnas que solo existen en historical\n        forecast_weather2['surface_pressure'] = 1013.25  # valor típico\n        forecast_weather2['windspeed_10m'] = np.sqrt(\n            forecast_weather2['f10_metre_u_wind_component']**2 + \n            forecast_weather2['f10_metre_v_wind_component']**2\n        ) if 'f10_metre_u_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['winddirection_10m'] = np.arctan2(\n            forecast_weather2['f10_metre_v_wind_component'], \n            forecast_weather2['f10_metre_u_wind_component']\n        ) * 180 / np.pi if 'f10_metre_v_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['diffuse_radiation'] = forecast_weather2.get('fsurface_solar_radiation_downwards', 0) - forecast_weather2.get('fdirect_solar_radiation', 0)\n        forecast_weather2['diffuse_radiation'] = forecast_weather2['diffuse_radiation'].clip(lower=0)\n        \n        # Limpiar y optimizar\n        forecast_weather2 = forecast_weather2.drop(columns=['data_block_id'], errors='ignore')\n        forecast_weather2 = self.optimize_dtypes(forecast_weather2, is_train=False)\n        \n        # Merge final con weather\n        print(\"Merge final con datos meteorológicos...\")\n        test5 = pd.merge(test4, forecast_weather2, \n                        left_on=['datetime', 'county'], \n                        right_on=['forecast_datetime', 'county'], \n                        how='left')\n        \n        # Limpiar columnas duplicadas y renombrar\n        cols_to_drop = [col for col in test5.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in test5.columns if col.endswith('_x')}\n        \n        test5 = test5.drop(columns=cols_to_drop + ['forecast_datetime'], errors='ignore')\n        test5.rename(columns=rename_dict, inplace=True)\n        \n        # Optimizar tipos finales\n        test5 = self.optimize_dtypes(test5, is_train=False)\n        \n        # Reordenar columnas igual que train\n        test5 = self._reorder_columns_test(test5)\n        \n        print(f\"Pipeline test completado! Dataset: {test5.shape[0]:,} filas, {test5.shape[1]} columnas\")\n        return test5\n    \n    def _reorder_columns_test(self, df):\n        \"\"\"Reordenar columnas para test (mismo orden que train)\"\"\"\n        main_cols = [\n            'row_id', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = []\n        # Solo agregar columnas que existen\n        for col in main_cols:\n            if col in df.columns:\n                final_order.append(col)\n        \n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n\n\n# Funciones de utilidad para usar el pipeline\ndef run_train_pipeline(train, gas_prices, electricity_prices, client, \n                      forecast_weather, historical_weather, weather_station):\n    \"\"\"\n    Función principal para ejecutar pipeline de entrenamiento\n    \n    Returns:\n        pd.DataFrame: Dataset procesado train5\n    \"\"\"\n    pipeline = TrainPipeline()\n    return pipeline.run_full_pipeline(\n        train, gas_prices, electricity_prices, client, \n        forecast_weather, historical_weather, weather_station\n    )\n\ndef run_test_pipeline(test, gas_prices_t, electricity_prices_t, client_t, \n                     forecast_weather_t, weather_station):\n    \"\"\"\n    Función principal para ejecutar pipeline de test\n    \n    Returns:\n        pd.DataFrame: Dataset procesado test5\n    \"\"\"\n    pipeline = TestPipeline()\n    return pipeline.run_test_pipeline(\n        test, gas_prices_t, electricity_prices_t, client_t, \n        forecast_weather_t, weather_station\n    )\n\n# Ejemplo de uso:\n\"\"\"\n# Para train:\ntrain5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)\n\n# Para test:\ntest5 = run_test_pipeline(\n    test, gas_prices_t, electricity_prices_t, client_t, \n    forecast_weather_t, weather_station\n)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:55.480440Z","iopub.execute_input":"2025-08-30T18:25:55.480803Z","iopub.status.idle":"2025-08-30T18:25:55.603577Z","shell.execute_reply.started":"2025-08-30T18:25:55.480778Z","shell.execute_reply":"2025-08-30T18:25:55.601950Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n# Para train:\\ntrain5 = run_train_pipeline(\\n    train, gas_prices, electricity_prices, client, \\n    forecast_weather, historical_weather, weather_station\\n)\\n\\n# Para test:\\ntest5 = run_test_pipeline(\\n    test, gas_prices_t, electricity_prices_t, client_t, \\n    forecast_weather_t, weather_station\\n)\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:55.605608Z","iopub.execute_input":"2025-08-30T18:25:55.606146Z","iopub.status.idle":"2025-08-30T18:28:10.797356Z","shell.execute_reply.started":"2025-08-30T18:25:55.606113Z","shell.execute_reply":"2025-08-30T18:28:10.796217Z"}},"outputs":[{"name":"stdout","text":"Iniciando pipeline completo de entrenamiento...\n=== PARTE 1: Preparación y merges base ===\nProcesando gas prices...\nProcesando electricity prices...\nProcesando client data...\nCompletando datos de clientes...\nParte 1 completada. Dataset: 2,017,824 filas, 16 columnas\n=== PARTE 2: Preparación datos meteorológicos ===\nProcesando forecast weather...\nCompletando 2140318 counties faltantes con k-NN...\nMergeando forecast con historical weather...\nWeather data preparado: 3,418,242 filas, 32 columnas\n=== PARTE 3: Merge final y limpieza ===\nProcesando período 1/21: 2021-09-01 a 2021-10-01\nChunk 1 guardado: (1423080, 46)\nProcesando período 2/21: 2021-10-01 a 2021-11-01\nChunk 2 guardado: (1532066, 46)\nProcesando período 3/21: 2021-11-01 a 2021-12-01\nChunk 3 guardado: (1484640, 46)\nProcesando período 4/21: 2021-12-01 a 2022-01-01\nChunk 4 guardado: (1543056, 46)\nProcesando período 5/21: 2022-01-01 a 2022-02-01\nChunk 5 guardado: (1590672, 46)\nProcesando período 6/21: 2022-02-01 a 2022-03-01\nChunk 6 guardado: (1441632, 46)\nProcesando período 7/21: 2022-03-01 a 2022-04-01\nChunk 7 guardado: (1597678, 46)\nProcesando período 8/21: 2022-04-01 a 2022-05-01\nChunk 8 guardado: (1630080, 46)\nProcesando período 9/21: 2022-05-01 a 2022-06-01\nChunk 9 guardado: (1644240, 46)\nProcesando período 10/21: 2022-06-01 a 2022-07-01\nChunk 10 guardado: (1573920, 46)\nProcesando período 11/21: 2022-07-01 a 2022-08-01\nChunk 11 guardado: (1581744, 46)\nProcesando período 12/21: 2022-08-01 a 2022-09-01\nChunk 12 guardado: (1581716, 46)\nProcesando período 13/21: 2022-09-01 a 2022-10-01\nChunk 13 guardado: (1573920, 46)\nProcesando período 14/21: 2022-10-01 a 2022-11-01\nChunk 14 guardado: (1686610, 46)\nProcesando período 15/21: 2022-11-01 a 2022-12-01\nChunk 15 guardado: (1643084, 46)\nProcesando período 16/21: 2022-12-01 a 2023-01-01\nChunk 16 guardado: (1669008, 46)\nProcesando período 17/21: 2023-01-01 a 2023-02-01\nChunk 17 guardado: (1654608, 46)\nProcesando período 18/21: 2023-02-01 a 2023-03-01\nChunk 18 guardado: (1494048, 46)\nProcesando período 19/21: 2023-03-01 a 2023-04-01\nChunk 19 guardado: (1604926, 46)\nProcesando período 20/21: 2023-04-01 a 2023-05-01\nChunk 20 guardado: (1514016, 46)\nProcesando período 21/21: 2023-05-01 a 2023-05-31\nChunk 21 guardado: (1530872, 46)\nConcatenando chunks...\nAplicando limpieza final...\nEliminando 1 columnas, renombrando 1\nProcesando chunk limpieza 1/33: filas 0 a 1,000,000\nProcesando chunk limpieza 2/33: filas 1,000,000 a 2,000,000\nProcesando chunk limpieza 3/33: filas 2,000,000 a 3,000,000\nProcesando chunk limpieza 4/33: filas 3,000,000 a 4,000,000\nProcesando chunk limpieza 5/33: filas 4,000,000 a 5,000,000\nProcesando chunk limpieza 6/33: filas 5,000,000 a 6,000,000\nProcesando chunk limpieza 7/33: filas 6,000,000 a 7,000,000\nProcesando chunk limpieza 8/33: filas 7,000,000 a 8,000,000\nProcesando chunk limpieza 9/33: filas 8,000,000 a 9,000,000\nProcesando chunk limpieza 10/33: filas 9,000,000 a 10,000,000\nProcesando chunk limpieza 11/33: filas 10,000,000 a 11,000,000\nProcesando chunk limpieza 12/33: filas 11,000,000 a 12,000,000\nProcesando chunk limpieza 13/33: filas 12,000,000 a 13,000,000\nProcesando chunk limpieza 14/33: filas 13,000,000 a 14,000,000\nProcesando chunk limpieza 15/33: filas 14,000,000 a 15,000,000\nProcesando chunk limpieza 16/33: filas 15,000,000 a 16,000,000\nProcesando chunk limpieza 17/33: filas 16,000,000 a 17,000,000\nProcesando chunk limpieza 18/33: filas 17,000,000 a 18,000,000\nProcesando chunk limpieza 19/33: filas 18,000,000 a 19,000,000\nProcesando chunk limpieza 20/33: filas 19,000,000 a 20,000,000\nProcesando chunk limpieza 21/33: filas 20,000,000 a 21,000,000\nProcesando chunk limpieza 22/33: filas 21,000,000 a 22,000,000\nProcesando chunk limpieza 23/33: filas 22,000,000 a 23,000,000\nProcesando chunk limpieza 24/33: filas 23,000,000 a 24,000,000\nProcesando chunk limpieza 25/33: filas 24,000,000 a 25,000,000\nProcesando chunk limpieza 26/33: filas 25,000,000 a 26,000,000\nProcesando chunk limpieza 27/33: filas 26,000,000 a 27,000,000\nProcesando chunk limpieza 28/33: filas 27,000,000 a 28,000,000\nProcesando chunk limpieza 29/33: filas 28,000,000 a 29,000,000\nProcesando chunk limpieza 30/33: filas 29,000,000 a 30,000,000\nProcesando chunk limpieza 31/33: filas 30,000,000 a 31,000,000\nProcesando chunk limpieza 32/33: filas 31,000,000 a 32,000,000\nProcesando chunk limpieza 33/33: filas 32,000,000 a 32,995,616\nFilas eliminadas por clima faltante: 2,024\nPipeline completado! Dataset final: 32,963,024 filas, 45 columnas\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:10.802414Z","iopub.execute_input":"2025-08-30T18:28:10.803007Z","iopub.status.idle":"2025-08-30T18:28:10.862130Z","shell.execute_reply.started":"2025-08-30T18:28:10.802973Z","shell.execute_reply":"2025-08-30T18:28:10.860551Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n6      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n7      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n8      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n9      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n10     366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n\n    data_block_id  prediction_unit_id  is_business  product_type  county  \\\n6               0                   0            0             1       0   \n7               0                   0            0             1       0   \n8               0                   0            0             1       0   \n9               0                   0            0             1       0   \n10              0                   0            0             1       0   \n\n     latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n6   59.099998  24.200001                 45.23                  46.32   \n7   59.099998  25.200001                 45.23                  46.32   \n8   59.400002  22.700001                 45.23                  46.32   \n9   59.400002  23.200001                 45.23                  46.32   \n10  59.400002  23.700001                 45.23                  46.32   \n\n    euros_per_mwh  eic_count  installed_capacity  ftemperature  temperature  \\\n6       86.879997      108.0          952.890015     12.681543         12.4   \n7       86.879997      108.0          952.890015     12.868921         12.3   \n8       86.879997      108.0          952.890015     15.041773         15.2   \n9       86.879997      108.0          952.890015     14.632105         14.9   \n10      86.879997      108.0          952.890015     14.480005         12.8   \n\n    fdewpoint  dewpoint  fcloudcover_high  cloudcover_high  fcloudcover_low  \\\n6    9.783228       9.7          0.023590             11.0         0.002380   \n7    9.498316       9.6          0.431854              6.0         0.211182   \n8   11.860376      11.8          0.134674             18.0         0.202515   \n9   11.773584      11.5          0.255188             19.0         0.036774   \n10  11.581568      10.4          0.338074             12.0         0.037109   \n\n    cloudcover_low  fcloudcover_mid  cloudcover_mid  fcloudcover_total  \\\n6             10.0         0.001251             0.0           0.026398   \n7             23.0         0.006790             1.0           0.548508   \n8              7.0         0.003906             0.0           0.308930   \n9              6.0         0.026245             0.0           0.286194   \n10             4.0         0.016510             1.0           0.368134   \n\n    cloudcover_total  f10_metre_u_wind_component  windspeed_10m  \\\n6               12.0                    1.840991       4.222222   \n7               23.0                    1.505298       4.027778   \n8               12.0                    3.185351       9.055555   \n9               11.0                    3.474780       8.361111   \n10               8.0                    3.211841       5.416667   \n\n    f10_metre_v_wind_component  winddirection_10m  fdirect_solar_radiation  \\\n6                    -3.857846              338.0                      0.0   \n7                    -3.590024              337.0                      0.0   \n8                    -8.173276              338.0                      0.0   \n9                    -8.008969              335.0                      0.0   \n10                   -7.426206              337.0                      0.0   \n\n    direct_solar_radiation  fsurface_solar_radiation_downwards  \\\n6                      0.0                                 0.0   \n7                      0.0                                 0.0   \n8                      0.0                                 0.0   \n9                      0.0                                 0.0   \n10                     0.0                                 0.0   \n\n    shortwave_radiation  diffuse_radiation  fsnowfall  snowfall  \\\n6                   0.0                0.0        0.0       0.0   \n7                   0.0                0.0        0.0       0.0   \n8                   0.0                0.0        0.0       0.0   \n9                   0.0                0.0        0.0       0.0   \n10                  0.0                0.0        0.0       0.0   \n\n    ftotal_precipitation  rain  surface_pressure  weather_forecast_hour  \\\n6                    0.0   0.0       1009.200012                    1.0   \n7                    0.0   0.0       1004.200012                    1.0   \n8                    0.0   0.0       1015.000000                    1.0   \n9                    0.0   0.0       1014.500000                    1.0   \n10                   0.0   0.0       1014.000000                    1.0   \n\n    is_consumption  \n6                0  \n7                0  \n8                0  \n9                0  \n10               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.681543</td>\n      <td>12.4</td>\n      <td>9.783228</td>\n      <td>9.7</td>\n      <td>0.023590</td>\n      <td>11.0</td>\n      <td>0.002380</td>\n      <td>10.0</td>\n      <td>0.001251</td>\n      <td>0.0</td>\n      <td>0.026398</td>\n      <td>12.0</td>\n      <td>1.840991</td>\n      <td>4.222222</td>\n      <td>-3.857846</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1009.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.868921</td>\n      <td>12.3</td>\n      <td>9.498316</td>\n      <td>9.6</td>\n      <td>0.431854</td>\n      <td>6.0</td>\n      <td>0.211182</td>\n      <td>23.0</td>\n      <td>0.006790</td>\n      <td>1.0</td>\n      <td>0.548508</td>\n      <td>23.0</td>\n      <td>1.505298</td>\n      <td>4.027778</td>\n      <td>-3.590024</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1004.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>15.041773</td>\n      <td>15.2</td>\n      <td>11.860376</td>\n      <td>11.8</td>\n      <td>0.134674</td>\n      <td>18.0</td>\n      <td>0.202515</td>\n      <td>7.0</td>\n      <td>0.003906</td>\n      <td>0.0</td>\n      <td>0.308930</td>\n      <td>12.0</td>\n      <td>3.185351</td>\n      <td>9.055555</td>\n      <td>-8.173276</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1015.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.632105</td>\n      <td>14.9</td>\n      <td>11.773584</td>\n      <td>11.5</td>\n      <td>0.255188</td>\n      <td>19.0</td>\n      <td>0.036774</td>\n      <td>6.0</td>\n      <td>0.026245</td>\n      <td>0.0</td>\n      <td>0.286194</td>\n      <td>11.0</td>\n      <td>3.474780</td>\n      <td>8.361111</td>\n      <td>-8.008969</td>\n      <td>335.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.500000</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.480005</td>\n      <td>12.8</td>\n      <td>11.581568</td>\n      <td>10.4</td>\n      <td>0.338074</td>\n      <td>12.0</td>\n      <td>0.037109</td>\n      <td>4.0</td>\n      <td>0.016510</td>\n      <td>1.0</td>\n      <td>0.368134</td>\n      <td>8.0</td>\n      <td>3.211841</td>\n      <td>5.416667</td>\n      <td>-7.426206</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"train5.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:30:09.640640Z","iopub.execute_input":"2025-08-30T18:30:09.641758Z","iopub.status.idle":"2025-08-30T18:30:13.606600Z","shell.execute_reply.started":"2025-08-30T18:30:09.641718Z","shell.execute_reply":"2025-08-30T18:30:13.605408Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"row_id                                0\ntarget                                0\ndatetime                              0\nforecast_date                         0\nhour                                  0\ndata_block_id                         0\nprediction_unit_id                    0\nis_business                           0\nproduct_type                          0\ncounty                                0\nlatitude                              0\nlongitude                             0\nlowest_price_per_mwh                  0\nhighest_price_per_mwh                 0\neuros_per_mwh                         0\neic_count                             0\ninstalled_capacity                    0\nftemperature                          0\ntemperature                           0\nfdewpoint                             0\ndewpoint                              0\nfcloudcover_high                      0\ncloudcover_high                       0\nfcloudcover_low                       0\ncloudcover_low                        0\nfcloudcover_mid                       0\ncloudcover_mid                        0\nfcloudcover_total                     0\ncloudcover_total                      0\nf10_metre_u_wind_component            0\nwindspeed_10m                         0\nf10_metre_v_wind_component            0\nwinddirection_10m                     0\nfdirect_solar_radiation               0\ndirect_solar_radiation                0\nfsurface_solar_radiation_downwards    0\nshortwave_radiation                   0\ndiffuse_radiation                     0\nfsnowfall                             0\nsnowfall                              0\nftotal_precipitation                  0\nrain                                  0\nsurface_pressure                      0\nweather_forecast_hour                 0\nis_consumption                        0\ndtype: int64"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:10.862956Z","iopub.execute_input":"2025-08-30T18:28:10.863338Z","iopub.status.idle":"2025-08-30T18:28:11.312532Z","shell.execute_reply.started":"2025-08-30T18:28:10.863307Z","shell.execute_reply":"2025-08-30T18:28:11.304991Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2167009006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"],"ename":"NameError","evalue":"name 'a' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"#target 528 nulos de mas de 2kk , los omito\n#fecha inicio 2021-09-01\n#Dataset Shape: (2018352, 9)\n#🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n#📝 Columnas categóricas: ['datetime']\n#quick_info(train)\n#train.head()\n#train.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.313362Z","iopub.status.idle":"2025-08-30T18:28:11.313823Z","shell.execute_reply.started":"2025-08-30T18:28:11.313659Z","shell.execute_reply":"2025-08-30T18:28:11.313676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1 = train.dropna()\n#quick_info(train1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.315867Z","iopub.status.idle":"2025-08-30T18:28:11.316415Z","shell.execute_reply.started":"2025-08-30T18:28:11.316198Z","shell.execute_reply":"2025-08-30T18:28:11.316219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 0 nulos\n#fecha inicio 2021-09-01\n# Dataset Shape: (41919, 7)\n#🔢 Columnas numéricas: ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'data_block_id']\n#📝 Columnas categóricas: ['date']\n#quick_info(client)\n#client.head()\n# client.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.317403Z","iopub.status.idle":"2025-08-30T18:28:11.317730Z","shell.execute_reply.started":"2025-08-30T18:28:11.317571Z","shell.execute_reply":"2025-08-30T18:28:11.317584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 0 nulos\n#fecha inicio 2021-08-31\n#📊 Dataset Shape: (15286, 4)\n#🔢 Columnas numéricas: ['euros_per_mwh', 'data_block_id']\n#📝 Columnas categóricas: ['forecast_date', 'origin_date']\n#quick_info(electricity_prices)\n#electricity_prices.head()\n#electricity_prices.describe(include=\"all\")\n# euros_per_mwh porque el minimo es -10.06 ?\n# un supuesto seria que es lo que paga la empresa si aportas electricidad , pero siendo el max 4000 , hace ruido pensar eso","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.319175Z","iopub.status.idle":"2025-08-30T18:28:11.319530Z","shell.execute_reply.started":"2025-08-30T18:28:11.319384Z","shell.execute_reply":"2025-08-30T18:28:11.319397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2 nulos en 'surface_solar_radiation_downwards' , los omito\n#fecha inicio 2021-09-01\n#📊 Dataset Shape: (3424512, 18)\n#🔢 Columnas numéricas: ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid',\n#'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'data_block_id', 'direct_solar_radiation',\n#'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n#📝 Columnas categóricas: ['origin_datetime', 'forecast_datetime']\n#quick_info(forecast_weather)\n#forecast_weather.head()\n#forecast_weather.describe(include=\"all\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.321617Z","iopub.status.idle":"2025-08-30T18:28:11.322450Z","shell.execute_reply.started":"2025-08-30T18:28:11.322169Z","shell.execute_reply":"2025-08-30T18:28:11.322204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1 = forecast_weather.dropna()\n#quick_info(forecast_weather1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.324167Z","iopub.status.idle":"2025-08-30T18:28:11.324481Z","shell.execute_reply.started":"2025-08-30T18:28:11.324340Z","shell.execute_reply":"2025-08-30T18:28:11.324353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#0 nulos\n#fecha inicio 2021-09-01\n#📊 Dataset Shape: (637, 5)\n#🔢 Columnas numéricas: ['lowest_price_per_mwh', 'highest_price_per_mwh', 'data_block_id']\n#📝 Columnas categóricas: ['forecast_date', 'origin_date']\n#quick_info(gas_prices)\n#gas_prices.head()\n#gas_prices.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.327393Z","iopub.status.idle":"2025-08-30T18:28:11.328550Z","shell.execute_reply.started":"2025-08-30T18:28:11.328177Z","shell.execute_reply":"2025-08-30T18:28:11.328205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#0 nulos\n#fecha inicio 2021-09-01\n#📊 Dataset Shape: (1710802, 18)\n#🔢 Columnas numéricas: ['temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n#'cloudcover_mid', 'cloudcover_high', 'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n#'diffuse_radiation', 'latitude', 'longitude', 'data_block_id']\n#📝 Columnas categóricas: ['datetime']\n#quick_info(historical_weather)\n#historical_weather.head()\n#historical_weather.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.329682Z","iopub.status.idle":"2025-08-30T18:28:11.330141Z","shell.execute_reply.started":"2025-08-30T18:28:11.329954Z","shell.execute_reply":"2025-08-30T18:28:11.329985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ver si completo la info por fuera de los county_name\n# county_name 63 nulos / county 63 nulos\n# 📊 Dataset Shape: (112, 4)\n# 🔢 Columnas numéricas: ['longitude', 'latitude', 'county']\n# 📝 Columnas categóricas: ['county_name']\n#quick_info(weather_station)\n#weather_station.head(50)\n#weather_station.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.331494Z","iopub.status.idle":"2025-08-30T18:28:11.331844Z","shell.execute_reply.started":"2025-08-30T18:28:11.331664Z","shell.execute_reply":"2025-08-30T18:28:11.331677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weather_station1 = weather_station.copy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.333504Z","iopub.status.idle":"2025-08-30T18:28:11.333981Z","shell.execute_reply.started":"2025-08-30T18:28:11.333742Z","shell.execute_reply":"2025-08-30T18:28:11.333761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unicodedata\nfrom sklearn.neighbors import NearestNeighbors\n\n# === 1) Diccionarios canonicos ===\ncounty_id_to_name_map = {\n    0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"JÄRVAMAA\",\n    4: \"JÕGEVAMAA\", 5: \"LÄÄNE-VIRUMAA\", 6: \"LÄÄNEMAA\", 7: \"PÄRNUMAA\",\n    8: \"PÕLVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n    12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"VÕRUMAA\"\n}\ncounty_name_to_id_map = {v: k for k, v in county_id_to_name_map.items()}\n\n# Para normalizar entradas (sin/ con acentos, \"County\"/\"maakond\", etc.)\nsynonyms_to_canonical = {\n    # clave \"base\" (sin acentos/variantes) -> canon con acentos\n    \"HARJU\": \"HARJUMAA\",\n    \"HIIU\": \"HIIUMAA\",\n    \"IDA-VIRU\": \"IDA-VIRUMAA\",\n    \"JARVA\": \"JÄRVAMAA\", \"JÄRVA\": \"JÄRVAMAA\",\n    \"JOGEVA\": \"JÕGEVAMAA\", \"JÕGEVA\": \"JÕGEVAMAA\",\n    \"LAANE-VIRU\": \"LÄÄNE-VIRUMAA\", \"LÄÄNE-VIRU\": \"LÄÄNE-VIRUMAA\",\n    \"LAANE\": \"LÄÄNEMAA\", \"LÄÄNE\": \"LÄÄNEMAA\",\n    \"PARNU\": \"PÄRNUMAA\", \"PÄRNU\": \"PÄRNUMAA\",\n    \"POLVA\": \"PÕLVAMAA\", \"PÕLVA\": \"PÕLVAMAA\",\n    \"RAPLA\": \"RAPLAMAA\",\n    \"SAARE\": \"SAAREMAA\",\n    \"TARTU\": \"TARTUMAA\",\n    \"VALGA\": \"VALGAMAA\",\n    \"VILJANDI\": \"VILJANDIMAA\",\n    \"VORU\": \"VÕRUMAA\", \"VÕRU\": \"VÕRUMAA\",\n\n    # versiones ya canónicas\n    \"HARJUMAA\": \"HARJUMAA\",\n    \"HIIUMAA\": \"HIIUMAA\",\n    \"IDA-VIRUMAA\": \"IDA-VIRUMAA\",\n    \"JÄRVAMAA\": \"JÄRVAMAA\",\n    \"JÕGEVAMAA\": \"JÕGEVAMAA\",\n    \"LÄÄNE-VIRUMAA\": \"LÄÄNE-VIRUMAA\",\n    \"LÄÄNEMAA\": \"LÄÄNEMAA\",\n    \"PÄRNUMAA\": \"PÄRNUMAA\",\n    \"PÕLVAMAA\": \"PÕLVAMAA\",\n    \"RAPLAMAA\": \"RAPLAMAA\",\n    \"SAAREMAA\": \"SAAREMAA\",\n    \"TARTUMAA\": \"TARTUMAA\",\n    \"VALGAMAA\": \"VALGAMAA\",\n    \"VILJANDIMAA\": \"VILJANDIMAA\",\n    \"VÕRUMAA\": \"VÕRUMAA\",\n}\n\ndef strip_accents(s: str) -> str:\n    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n\ndef normalize_county_name(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).upper().strip()\n    # eliminar sufijos frecuentes\n    for suf in [\" COUNTY\", \" MAAKOND\"]:\n        if s.endswith(suf):\n            s = s[: -len(suf)]\n    s = s.replace(\"  \", \" \").replace(\"–\", \"-\")  # guion raro -> normal\n    s_noacc = strip_accents(s)\n    s_noacc = s_noacc.replace(\"  \", \" \")\n    # prueba de match exacto o por prefijo lógico\n    # ej. \"HARJU\", \"HARJU MAAKOND\" -> \"HARJU\"\n    token = s_noacc.split()[0]  # primer palabra alcanza en estos casos\n    # casos con guion (IDA-VIRU, LAA NE-VIRU, etc.)\n    if \"-\" in s_noacc:\n        token = s_noacc  # mantener completo en compuestos\n\n    # intento 1: match directo con cadena completa sin acentos\n    if s_noacc in synonyms_to_canonical:\n        return synonyms_to_canonical[s_noacc]\n    # intento 2: match por token/prefijo conocido\n    if token in synonyms_to_canonical:\n        return synonyms_to_canonical[token]\n    # intento 3: si ya vino canónico exacto (con acentos)\n    if s in synonyms_to_canonical:\n        return synonyms_to_canonical[s]\n\n    return np.nan  # no lo reconozco\n\n# === 2) Normalizar lo existente sin romper los NaN ===\n# (NO convertir NaN a \"NAN\")\nweather_station1['county_name_norm'] = weather_station1['county_name'].apply(normalize_county_name)\n\n# Si tengo 'county' numérico pero falta nombre, lo inferimos del ID\nmask = weather_station1['county_name_norm'].isna() & weather_station1['county'].notna()\nweather_station1.loc[mask, 'county_name_norm'] = (\n    weather_station1.loc[mask, 'county'].astype(int).map(county_id_to_name_map)\n)\n\n# === 3) Completar faltantes por k-NN (lat/lon) ===\nfcols = ['latitude', 'longitude']\nknown = weather_station1.dropna(subset=['county_name_norm']).copy()\nunknown = weather_station1[weather_station1['county_name_norm'].isna()].copy()\n\nif not unknown.empty and not known.empty:\n    # k-NN en (lat, lon) usando Haversine (necesita radianes)\n    # ojo: haversine en sklearn requiere [lat, lon] en radianes\n    known_rad = np.radians(known[fcols].values)\n    unknown_rad = np.radians(unknown[fcols].values)\n\n    nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n    nbrs.fit(known_rad)\n    distances, idxs = nbrs.kneighbors(unknown_rad)\n\n    # votación simple de los 3 vecinos (majority vote)\n    nn_names = known['county_name_norm'].values\n    filled = []\n    for neigh_idx in idxs:\n        cands = nn_names[neigh_idx]\n        # mayoría; en empate, tomar el 1er vecino\n        vals, counts = np.unique(cands, return_counts=True)\n        filled.append(vals[np.argmax(counts)])\n\n    weather_station1.loc[unknown.index, 'county_name_norm'] = filled\n\n# === 4) Sincronizar columnas finales ===\nweather_station1['county_name'] = weather_station1['county_name_norm']\nweather_station1['county'] = weather_station1['county_name'].map(county_name_to_id_map).astype('Int64')\n\n# Cualquier remanente a UNKNOWN (12)\nweather_station1.loc[weather_station1['county_name'].isna(), 'county'] = 12\nweather_station1['county'] = weather_station1['county'].astype('Int64')\n\n# Limpieza\nweather_station1.drop(columns=['county_name_norm'], inplace=True)\n\n# Chequeo rápido\nprint(weather_station1[['county_name','longitude','latitude','county']].head(25))\nprint(\"\\nFaltantes aún:\", weather_station1['county_name'].isna().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.335823Z","iopub.status.idle":"2025-08-30T18:28:11.336287Z","shell.execute_reply.started":"2025-08-30T18:28:11.336084Z","shell.execute_reply":"2025-08-30T18:28:11.336103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#weather_station1[\"county\"].value_counts()\n#aplicando de esta forma no esta el 12 que es \"UNKNOWN\"\n#supongo que aplica y funciona bien","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.337660Z","iopub.status.idle":"2025-08-30T18:28:11.337972Z","shell.execute_reply.started":"2025-08-30T18:28:11.337803Z","shell.execute_reply":"2025-08-30T18:28:11.337815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0 nulos\n# 📊 Dataset Shape: (12480, 9)\n# 🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n# 📝 Columnas categóricas: ['prediction_datetime']\n# fecha inicio 2023-05-28\n#quick_info(test)\n#test.head()\n#test.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.339280Z","iopub.status.idle":"2025-08-30T18:28:11.340447Z","shell.execute_reply.started":"2025-08-30T18:28:11.340262Z","shell.execute_reply":"2025-08-30T18:28:11.340281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0 nulos # como identifico a este cliente ?\n# 📊 Dataset Shape: (262, 7)\n# 🔢 Columnas numéricas: ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'data_block_id']\n# 📝 Columnas categóricas: ['date']\n#quick_info(client_t)\n#client_t.head()\n#client_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.341527Z","iopub.status.idle":"2025-08-30T18:28:11.341821Z","shell.execute_reply.started":"2025-08-30T18:28:11.341683Z","shell.execute_reply":"2025-08-30T18:28:11.341697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -10 el minimo como el de train , ver que hacer con eso\n# 📊 Dataset Shape: (96, 4)\n# 🔢 Columnas numéricas: ['euros_per_mwh', 'data_block_id']\n# 📝 Columnas categóricas: ['forecast_date', 'origin_date']\n#fecha inicio 2023-05-26\n#quick_info(electricity_prices_t)\n#electricity_prices_t.head()\n#electricity_prices_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.343427Z","iopub.status.idle":"2025-08-30T18:28:11.343858Z","shell.execute_reply.started":"2025-08-30T18:28:11.343652Z","shell.execute_reply":"2025-08-30T18:28:11.343669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 📊 Dataset Shape: (21504, 18)\n# 🔢 Columnas numéricas: ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low',\n#                        'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'data_block_id',\n#                        'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n# 📝 Columnas categóricas: ['origin_datetime', 'forecast_datetime']\n# fecha inicio 2023-05-27\n# quick_info(forecast_weather_t)\n# forecast_weather_t.head()\n#forecast_weather_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.346119Z","iopub.status.idle":"2025-08-30T18:28:11.346452Z","shell.execute_reply.started":"2025-08-30T18:28:11.346314Z","shell.execute_reply":"2025-08-30T18:28:11.346327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 📊 Dataset Shape: (4, 5)\n# 🔢 Columnas numéricas: ['lowest_price_per_mwh', 'highest_price_per_mwh', 'data_block_id']\n# 📝 Columnas categóricas: ['forecast_date', 'origin_date']\n# fecha inicio 2023-5-26\n# quick_info(gas_prices_t)\n# gas_prices_t.head()\n# #gas_prices_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.349639Z","iopub.status.idle":"2025-08-30T18:28:11.350144Z","shell.execute_reply.started":"2025-08-30T18:28:11.349831Z","shell.execute_reply":"2025-08-30T18:28:11.349845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 📊 Dataset Shape: (12576, 9)\n# 🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n# 📝 Columnas categóricas: ['datetime']\n# fecha inicio 2023-05-26\n# quick_info(revealed_targets)\n#revealed_targets.head()\n#revealed_targets.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.351812Z","iopub.status.idle":"2025-08-30T18:28:11.352391Z","shell.execute_reply.started":"2025-08-30T18:28:11.352084Z","shell.execute_reply":"2025-08-30T18:28:11.352103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lo mejor seria ver como ir combinando todo en 1 dataset ,hago copia y pongo los que estan listos para usar\n\ntrain1 = train.dropna()# train                 \nclient1 = client.copy() # client               \nelectricity_prices1 = electricity_prices.copy() # electricity_prices  \nforecast_weather1 = forecast_weather.dropna() # forecast_weather    \ngas_prices1 = gas_prices.copy() # gas_prices                        \nhistorical_weather1 = historical_weather.copy() # historical_weather    \nweather_station2 = weather_station1.copy()      \n\ntest1 = test.copy() # test                 \nclient_t = client_t.copy() # client_t              \nelectricity_prices_t1 = electricity_prices_t.copy() # electricity_prices_t  \nforecast_weather_t1 = forecast_weather_t.copy() # forecast_weather_t    \ngas_prices_t1 = gas_prices_t.copy() # gas_prices_t          \nrevealed_targets1 = revealed_targets.copy() # revealed_targets    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.353524Z","iopub.status.idle":"2025-08-30T18:28:11.353968Z","shell.execute_reply.started":"2025-08-30T18:28:11.353746Z","shell.execute_reply":"2025-08-30T18:28:11.353764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.355460Z","iopub.status.idle":"2025-08-30T18:28:11.355766Z","shell.execute_reply.started":"2025-08-30T18:28:11.355623Z","shell.execute_reply":"2025-08-30T18:28:11.355636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train1)\n\n# 📊 Dataset Shape: (2017824, 9)\n# 🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n# 📝 Columnas categóricas: ['datetime']\n# ❌ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.356865Z","iopub.status.idle":"2025-08-30T18:28:11.357301Z","shell.execute_reply.started":"2025-08-30T18:28:11.357095Z","shell.execute_reply":"2025-08-30T18:28:11.357113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#gas_prices1.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.359190Z","iopub.status.idle":"2025-08-30T18:28:11.359590Z","shell.execute_reply.started":"2025-08-30T18:28:11.359404Z","shell.execute_reply":"2025-08-30T18:28:11.359418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(gas_prices1)\n\n# 📊 Dataset Shape: (637, 5)\n# 🔢 Columnas numéricas: ['lowest_price_per_mwh', 'highest_price_per_mwh', 'data_block_id']\n# 📝 Columnas categóricas: ['forecast_date', 'origin_date']\n# ❌ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.362080Z","iopub.status.idle":"2025-08-30T18:28:11.362758Z","shell.execute_reply.started":"2025-08-30T18:28:11.362295Z","shell.execute_reply":"2025-08-30T18:28:11.362329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# el forecast de gas es de 24 horas\n\n# Convertir datetime y crear columnas separadas en train1\ntrain1['datetime'] = pd.to_datetime(train1['datetime'])\ntrain1['hour'] = train1['datetime'].dt.hour\ntrain1['forecast_date'] = train1['datetime']\n\n#\ngas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\ngas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\ngas_prices1['hour'] = gas_prices1['forecast_date'].dt.hour\ngas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.364084Z","iopub.status.idle":"2025-08-30T18:28:11.364464Z","shell.execute_reply.started":"2025-08-30T18:28:11.364314Z","shell.execute_reply":"2025-08-30T18:28:11.364328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.367865Z","iopub.status.idle":"2025-08-30T18:28:11.368332Z","shell.execute_reply.started":"2025-08-30T18:28:11.368143Z","shell.execute_reply":"2025-08-30T18:28:11.368164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#gas_prices1.tail(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.372917Z","iopub.status.idle":"2025-08-30T18:28:11.373652Z","shell.execute_reply.started":"2025-08-30T18:28:11.373400Z","shell.execute_reply":"2025-08-30T18:28:11.373425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Expandir gas_prices1 a nivel horario - solo columnas necesarias\ngas_hourly = []\nfor _, row in gas_prices1.iterrows():\n    for hour in range(24):\n        new_row = {\n            'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n            'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n            'highest_price_per_mwh': row['highest_price_per_mwh'],\n            'gas_origin_date': row['gas_origin_date']\n        }\n        gas_hourly.append(new_row)\n\ngas_prices1_hourly = pd.DataFrame(gas_hourly)\n\n# Merge train1 con gas_prices1_hourly\ntrain2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.376789Z","iopub.status.idle":"2025-08-30T18:28:11.377337Z","shell.execute_reply.started":"2025-08-30T18:28:11.377094Z","shell.execute_reply":"2025-08-30T18:28:11.377114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train2.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.379912Z","iopub.status.idle":"2025-08-30T18:28:11.380436Z","shell.execute_reply.started":"2025-08-30T18:28:11.380214Z","shell.execute_reply":"2025-08-30T18:28:11.380235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train2)\n\n# 📊 Dataset Shape: (2017824, 14)\n# 🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n# 📝 Columnas categóricas: []\n# ❌ Valores nulos por columna:\n# lowest_price_per_mwh     3120\n# highest_price_per_mwh    3120\n# gas_origin_date          3120\n# dtype: int64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.382878Z","iopub.status.idle":"2025-08-30T18:28:11.383404Z","shell.execute_reply.started":"2025-08-30T18:28:11.383163Z","shell.execute_reply":"2025-08-30T18:28:11.383185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\ntrain2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n\n# Eliminar columna gas_origin_date\ntrain2 = train2.drop(columns=['gas_origin_date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.384890Z","iopub.status.idle":"2025-08-30T18:28:11.385278Z","shell.execute_reply.started":"2025-08-30T18:28:11.385131Z","shell.execute_reply":"2025-08-30T18:28:11.385145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.386799Z","iopub.status.idle":"2025-08-30T18:28:11.387206Z","shell.execute_reply.started":"2025-08-30T18:28:11.387012Z","shell.execute_reply":"2025-08-30T18:28:11.387025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train2.tail()\n# 📊 Dataset Shape: (2017824, 16)\n# 🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'minute', 'second', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n# 📝 Columnas categóricas: []\n# ❌ Valores nulos por columna:\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.388832Z","iopub.status.idle":"2025-08-30T18:28:11.389154Z","shell.execute_reply.started":"2025-08-30T18:28:11.389018Z","shell.execute_reply":"2025-08-30T18:28:11.389029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#electricity_prices1.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.390624Z","iopub.status.idle":"2025-08-30T18:28:11.391171Z","shell.execute_reply.started":"2025-08-30T18:28:11.390854Z","shell.execute_reply":"2025-08-30T18:28:11.390869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(electricity_prices1)\n\n# 📊 Dataset Shape: (15286, 4)\n# 🔢 Columnas numéricas: ['euros_per_mwh', 'data_block_id']\n# 📝 Columnas categóricas: ['forecast_date', 'origin_date']\n# ❌ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.393571Z","iopub.status.idle":"2025-08-30T18:28:11.393919Z","shell.execute_reply.started":"2025-08-30T18:28:11.393766Z","shell.execute_reply":"2025-08-30T18:28:11.393780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#electricity_prices1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.395276Z","iopub.status.idle":"2025-08-30T18:28:11.395643Z","shell.execute_reply.started":"2025-08-30T18:28:11.395497Z","shell.execute_reply":"2025-08-30T18:28:11.395511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparar electricity_prices1\nelectricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\nelectricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n\n# Extraer componentes de tiempo\nelectricity_prices1['hour'] = electricity_prices1['forecast_date'].dt.hour\n\n\n# Merge\nelec_columns = ['forecast_date', 'euros_per_mwh']\ntrain3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.397224Z","iopub.status.idle":"2025-08-30T18:28:11.397700Z","shell.execute_reply.started":"2025-08-30T18:28:11.397481Z","shell.execute_reply":"2025-08-30T18:28:11.397501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.401100Z","iopub.status.idle":"2025-08-30T18:28:11.401488Z","shell.execute_reply.started":"2025-08-30T18:28:11.401330Z","shell.execute_reply":"2025-08-30T18:28:11.401344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train3.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.403618Z","iopub.status.idle":"2025-08-30T18:28:11.404040Z","shell.execute_reply.started":"2025-08-30T18:28:11.403820Z","shell.execute_reply":"2025-08-30T18:28:11.403839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train3)\n\n# 📊 Dataset Shape: (2017824, 18)\n# 🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'minute', 'second', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh']\n# 📝 Columnas categóricas: []\n# ❌ Valores nulos por columna:\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.405746Z","iopub.status.idle":"2025-08-30T18:28:11.406154Z","shell.execute_reply.started":"2025-08-30T18:28:11.405956Z","shell.execute_reply":"2025-08-30T18:28:11.405973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train3.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.408074Z","iopub.status.idle":"2025-08-30T18:28:11.408494Z","shell.execute_reply.started":"2025-08-30T18:28:11.408328Z","shell.execute_reply":"2025-08-30T18:28:11.408343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#client1.tail(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.409807Z","iopub.status.idle":"2025-08-30T18:28:11.410479Z","shell.execute_reply.started":"2025-08-30T18:28:11.410263Z","shell.execute_reply":"2025-08-30T18:28:11.410292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quick_info(client1)\n\n# 📊 Dataset Shape: (41919, 7)\n# 🔢 Columnas numéricas: ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'data_block_id']\n# 📝 Columnas categóricas: ['date']\n# ❌ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.412016Z","iopub.status.idle":"2025-08-30T18:28:11.412446Z","shell.execute_reply.started":"2025-08-30T18:28:11.412241Z","shell.execute_reply":"2025-08-30T18:28:11.412259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train3.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.413244Z","iopub.status.idle":"2025-08-30T18:28:11.413531Z","shell.execute_reply.started":"2025-08-30T18:28:11.413392Z","shell.execute_reply":"2025-08-30T18:28:11.413405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#client1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.415184Z","iopub.status.idle":"2025-08-30T18:28:11.415621Z","shell.execute_reply.started":"2025-08-30T18:28:11.415428Z","shell.execute_reply":"2025-08-30T18:28:11.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparar client1\nclient1['date'] = pd.to_datetime(client1['date']).dt.date\n\n# Merge con client1 usando datetime de train3\ntrain4 = train3.merge(client1.drop('data_block_id', axis=1),\n                      left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                      right_on=['product_type', 'county', 'is_business', 'date'],\n                      how='left')\n\n# Limpiar\ntrain4 = train4.drop('date', axis=1)\ntrain4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.418453Z","iopub.status.idle":"2025-08-30T18:28:11.418997Z","shell.execute_reply.started":"2025-08-30T18:28:11.418745Z","shell.execute_reply":"2025-08-30T18:28:11.418767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.420204Z","iopub.status.idle":"2025-08-30T18:28:11.420644Z","shell.execute_reply.started":"2025-08-30T18:28:11.420422Z","shell.execute_reply":"2025-08-30T18:28:11.420440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train4)\n\n\n# 📊 Dataset Shape: (2017824, 20)\n# 🔢 Columnas numéricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'minute', 'second', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', 'installed_capacity']\n# 📝 Columnas categóricas: []\n# ❌ Valores nulos por columna:\n# eic_count                6240\n# installed_capacity       6240\n# dtype: int64\n\n\n#falta corregir esto todavia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.422479Z","iopub.status.idle":"2025-08-30T18:28:11.422783Z","shell.execute_reply.started":"2025-08-30T18:28:11.422647Z","shell.execute_reply":"2025-08-30T18:28:11.422659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# decido completar asi los ultimos 2 dias que faltan\n# Verificar el problema\nprint(\"NaN en datos de clientes:\")\nprint(f\"eic_count: {train4['eic_count'].isna().sum():,}\")\nprint(f\"installed_capacity: {train4['installed_capacity'].isna().sum():,}\")\n\n# Ver las fechas con problemas\nnan_dates = train4[train4['eic_count'].isna()]['datetime'].dt.date.unique()\nprint(f\"Fechas con NaN: {sorted(nan_dates)}\")\n\n# Ver último día con datos\nlast_valid_date = train4[train4['eic_count'].notna()]['datetime'].dt.date.max()\nprint(f\"Último día con datos: {last_valid_date}\")\n\n# Forward fill por grupo (county + is_business + product_type)\n# Esto mantiene los últimos valores conocidos para cada combinación\nprint(\"\\nCompletando con forward fill por grupo...\")\n\n# Ordenar por datetime para asegurar orden correcto\ntrain4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n\n# Forward fill por grupo\ntrain4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\ntrain4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n\n# Verificar resultado\nprint(f\"\\nDespués del forward fill:\")\nprint(f\"eic_count NaN: {train4['eic_count'].isna().sum()}\")\nprint(f\"installed_capacity NaN: {train4['installed_capacity'].isna().sum()}\")\n\n# Si aún quedan NaN, son combinaciones nuevas - usar 0 o promedio\nremaining_nan = train4['eic_count'].isna().sum()\nif remaining_nan > 0:\n    print(f\"\\n{remaining_nan} NaN restantes (combinaciones nuevas)\")\n    print(\"Opciones:\")\n    print(\"1. Rellenar con 0\")\n    print(\"2. Rellenar with promedio del county\")\n    \n    # Opción recomendada: promedio por county\n    train4['eic_count'] = train4['eic_count'].fillna(\n        train4.groupby('county')['eic_count'].transform('mean')\n    )\n    train4['installed_capacity'] = train4['installed_capacity'].fillna(\n        train4.groupby('county')['installed_capacity'].transform('mean')\n    )\n    \n    print(f\"NaN finales - eic_count: {train4['eic_count'].isna().sum()}\")\n    print(f\"NaN finales - installed_capacity: {train4['installed_capacity'].isna().sum()}\")\n\nprint(f\"\\nDataset train4: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.424489Z","iopub.status.idle":"2025-08-30T18:28:11.424847Z","shell.execute_reply.started":"2025-08-30T18:28:11.424684Z","shell.execute_reply":"2025-08-30T18:28:11.424696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.425724Z","iopub.status.idle":"2025-08-30T18:28:11.426084Z","shell.execute_reply.started":"2025-08-30T18:28:11.425883Z","shell.execute_reply":"2025-08-30T18:28:11.425895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.428292Z","iopub.status.idle":"2025-08-30T18:28:11.429833Z","shell.execute_reply.started":"2025-08-30T18:28:11.429604Z","shell.execute_reply":"2025-08-30T18:28:11.429632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.430697Z","iopub.status.idle":"2025-08-30T18:28:11.431394Z","shell.execute_reply.started":"2025-08-30T18:28:11.431127Z","shell.execute_reply":"2025-08-30T18:28:11.431147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weather_station3 = weather_station2.copy()\nweather_station3.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.433067Z","iopub.status.idle":"2025-08-30T18:28:11.433404Z","shell.execute_reply.started":"2025-08-30T18:28:11.433254Z","shell.execute_reply":"2025-08-30T18:28:11.433268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge forecast_weather1 con weather_station1 por coordenadas\nforecast_weather1 = forecast_weather1.merge(weather_station3[['latitude', 'longitude', 'county', 'county_name']], \n                                          on=['latitude', 'longitude'], \n                                          how='left')\n\nprint(f\"Filas con county asignado: {forecast_weather1['county'].notna().sum()}\")\nprint(f\"Total filas: {len(forecast_weather1)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.434606Z","iopub.status.idle":"2025-08-30T18:28:11.434910Z","shell.execute_reply.started":"2025-08-30T18:28:11.434769Z","shell.execute_reply":"2025-08-30T18:28:11.434782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.436149Z","iopub.status.idle":"2025-08-30T18:28:11.436471Z","shell.execute_reply.started":"2025-08-30T18:28:11.436333Z","shell.execute_reply":"2025-08-30T18:28:11.436346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.438445Z","iopub.status.idle":"2025-08-30T18:28:11.438799Z","shell.execute_reply.started":"2025-08-30T18:28:11.438635Z","shell.execute_reply":"2025-08-30T18:28:11.438648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(forecast_weather1)\n\n# 📊 Dataset Shape: (3424510, 20)\n# 🔢 Columnas numéricas: ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'data_block_id', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation', 'county']\n# 📝 Columnas categóricas: ['origin_datetime', 'forecast_datetime', 'county_name']\n# ❌ Valores nulos por columna:\n# county         2140318\n# county_name    2140318\n# dtype: int64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.440968Z","iopub.status.idle":"2025-08-30T18:28:11.441550Z","shell.execute_reply.started":"2025-08-30T18:28:11.441266Z","shell.execute_reply":"2025-08-30T18:28:11.441293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nimport numpy as np\n\n# Identificar filas sin county asignado\nmissing_county = forecast_weather1['county'].isna()\nprint(f\"Filas sin county: {missing_county.sum()}\")\n\nif missing_county.sum() > 0:\n   # Datos conocidos (con county)\n   known = weather_station1[['latitude', 'longitude', 'county', 'county_name']].copy()\n   \n   # Datos faltantes\n   missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n   \n   # K-NN con distancia euclidiana (para coordenadas pequeñas como Estonia)\n   nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n   nbrs.fit(known[['latitude', 'longitude']].values)\n   \n   # Encontrar vecino más cercano\n   distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n   \n   # Asignar county y county_name del vecino más cercano\n   nearest_counties = known.iloc[indices.flatten()]\n   \n   forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n   forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n\nprint(f\"Filas con county después del K-NN: {forecast_weather1['county'].notna().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.442861Z","iopub.status.idle":"2025-08-30T18:28:11.443343Z","shell.execute_reply.started":"2025-08-30T18:28:11.443191Z","shell.execute_reply":"2025-08-30T18:28:11.443206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(forecast_weather1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.445263Z","iopub.status.idle":"2025-08-30T18:28:11.446120Z","shell.execute_reply.started":"2025-08-30T18:28:11.445804Z","shell.execute_reply":"2025-08-30T18:28:11.445823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.tail()\n# unir por latitu/longitude y forecast_datetime# esto por ahora no\n# sacar county_name\n# sacar origin_datetime\n# renombrar las variables de forecast con una f adelante (excepto county/hours_ahead/latitude/longitude)\n# eliminar origin_datetime\n# agregar horas/minutos/segundos y dejar 1 columna con año/mes/dia cuando transforme forecast_datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.447224Z","iopub.status.idle":"2025-08-30T18:28:11.447740Z","shell.execute_reply.started":"2025-08-30T18:28:11.447528Z","shell.execute_reply":"2025-08-30T18:28:11.447547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forecast_weather2 = forecast_weather1.copy()\n\n# Eliminar columnas innecesarias\nforecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n\n\n# Convertir forecast_datetime a datetime\nforecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n\n# Crear columnas temporales\nforecast_weather2['forecast_date'] = forecast_weather2['forecast_datetime'].dt.date\nforecast_weather2['hour'] = forecast_weather2['forecast_datetime'].dt.hour\n\n# Renombrar variables con 'f' adelante (excepto las especificadas)\nrename_dict = {}\nfor col in forecast_weather2.columns:\n   if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                  'forecast_datetime', 'forecast_date', 'hour']:\n       rename_dict[col] = 'f' + col\n\nforecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n\nprint(\"Columnas después de transformaciones:\")\nprint(forecast_weather2.columns.tolist())\n\nforecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\":\"weather_forecast_hour\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.449572Z","iopub.status.idle":"2025-08-30T18:28:11.449915Z","shell.execute_reply.started":"2025-08-30T18:28:11.449744Z","shell.execute_reply":"2025-08-30T18:28:11.449757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather2.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.451997Z","iopub.status.idle":"2025-08-30T18:28:11.452749Z","shell.execute_reply.started":"2025-08-30T18:28:11.452390Z","shell.execute_reply":"2025-08-30T18:28:11.452482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#historical_weather1.tail()\n#agregarle horas/minutos/segundos y cambiar datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.454612Z","iopub.status.idle":"2025-08-30T18:28:11.455114Z","shell.execute_reply.started":"2025-08-30T18:28:11.454841Z","shell.execute_reply":"2025-08-30T18:28:11.454856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(historical_weather1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.456388Z","iopub.status.idle":"2025-08-30T18:28:11.457268Z","shell.execute_reply.started":"2025-08-30T18:28:11.456772Z","shell.execute_reply":"2025-08-30T18:28:11.456792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"historical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n\n# Renombrar y merge\nforecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n\nmerged_weather = pd.merge(\n    forecast_weather2,\n    historical_weather1,\n    left_on=['latitude', 'longitude', 'forecast_datetime'],\n    right_on=['latitude', 'longitude', 'datetime'],\n    how='inner'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.459437Z","iopub.status.idle":"2025-08-30T18:28:11.459800Z","shell.execute_reply.started":"2025-08-30T18:28:11.459649Z","shell.execute_reply":"2025-08-30T18:28:11.459664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merged_weather.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.460791Z","iopub.status.idle":"2025-08-30T18:28:11.461103Z","shell.execute_reply.started":"2025-08-30T18:28:11.460962Z","shell.execute_reply":"2025-08-30T18:28:11.460974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merged_weather.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.462968Z","iopub.status.idle":"2025-08-30T18:28:11.463479Z","shell.execute_reply.started":"2025-08-30T18:28:11.463197Z","shell.execute_reply":"2025-08-30T18:28:11.463215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(merged_weather)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.465801Z","iopub.status.idle":"2025-08-30T18:28:11.466244Z","shell.execute_reply.started":"2025-08-30T18:28:11.466033Z","shell.execute_reply":"2025-08-30T18:28:11.466065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.467409Z","iopub.status.idle":"2025-08-30T18:28:11.467788Z","shell.execute_reply.started":"2025-08-30T18:28:11.467607Z","shell.execute_reply":"2025-08-30T18:28:11.467620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizar tipos de datos\ntrain4['county'] = train4['county'].astype('uint8')  # 0-15\ntrain4['is_business'] = train4['is_business'].astype('uint8')  # 0-1\ntrain4['product_type'] = train4['product_type'].astype('uint8')  # 0-3\ntrain4['is_consumption'] = train4['is_consumption'].astype('uint8')  # 0-1\ntrain4['hour'] = train4['hour'].astype('uint8')  # 0-23\n\n# IDs optimizados\ntrain4['data_block_id'] = train4['data_block_id'].astype('uint16')  # 0-700\ntrain4['row_id'] = train4['row_id'].astype('uint32')  # 0-3M\ntrain4['prediction_unit_id'] = train4['prediction_unit_id'].astype('uint8')  # 0-68\n\n# Flotantes a float32 (suficiente precision para los rangos dados)\ntrain4['target'] = train4['target'].astype('float32')\ntrain4['lowest_price_per_mwh'] = train4['lowest_price_per_mwh'].astype('float32')\ntrain4['highest_price_per_mwh'] = train4['highest_price_per_mwh'].astype('float32')\ntrain4['euros_per_mwh'] = train4['euros_per_mwh'].astype('float32')\ntrain4['eic_count'] = train4['eic_count'].astype('float32')\ntrain4['installed_capacity'] = train4['installed_capacity'].astype('float32')\n\nprint(\"Optimización completada\")\nprint(train4.info(memory_usage='deep'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.469154Z","iopub.status.idle":"2025-08-30T18:28:11.469485Z","shell.execute_reply.started":"2025-08-30T18:28:11.469331Z","shell.execute_reply":"2025-08-30T18:28:11.469344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Eliminar columnas innecesarias\nmerged_weather = merged_weather.drop('f_data_block_id', axis=1)\n\n# Coordenadas - float32 suficiente para la precisión geográfica\nmerged_weather['latitude'] = merged_weather['latitude'].astype('float32')  # 57.6-59.7\nmerged_weather['longitude'] = merged_weather['longitude'].astype('float32')  # 21.7-28.2\n\n# Enteros pequeños\nmerged_weather['weather_forecast_hour'] = merged_weather['weather_forecast_hour'].astype('uint8')  # 1-48\nmerged_weather['county'] = merged_weather['county'].astype('uint8')  # 0-15\nmerged_weather['hour'] = merged_weather['hour'].astype('uint8')  # 0-23\n\n# IDs\nmerged_weather['data_block_id'] = merged_weather['data_block_id'].astype('uint16')  # 1-637\n\n# Variables meteorológicas - float32 es suficiente\nweather_float_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                     'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                     'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                     'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                     'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                     'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                     'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                     'direct_solar_radiation', 'diffuse_radiation']\n\nfor col in weather_float_cols:\n    merged_weather[col] = merged_weather[col].astype('float32')\n\nprint(\"Optimización de merged_weather completada\")\nprint(merged_weather.info(memory_usage='deep'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.470837Z","iopub.status.idle":"2025-08-30T18:28:11.471315Z","shell.execute_reply.started":"2025-08-30T18:28:11.471116Z","shell.execute_reply":"2025-08-30T18:28:11.471134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(train4.info(memory_usage='deep'))\n\n#ANTES\n# <class 'pandas.core.frame.DataFrame'>\n# RangeIndex: 2017824 entries, 0 to 2017823\n# Data columns (total 22 columns):\n#  #   Column                 Dtype         \n# ---  ------                 -----         \n#  0   county                 int64         \n#  1   is_business            int64         \n#  2   product_type           int64         \n#  3   target                 float64       \n#  4   is_consumption         int64         \n#  5   datetime               datetime64[ns]\n#  6   data_block_id          int64         \n#  7   row_id                 int64         \n#  8   prediction_unit_id     int64         \n#  9   hour                   int32         \n#  10  minute                 int32         \n#  11  second                 int32         \n#  12  forecast_date          datetime64[ns]\n#  13  lowest_price_per_mwh   float64       \n#  14  highest_price_per_mwh  float64       \n#  15  gas_origin_date        datetime64[ns]\n#  16  euros_per_mwh          float64       \n#  17  elec_origin_date       datetime64[ns]\n#  18  eic_count              float64       \n#  19  installed_capacity     float64       \n#  20  elec_forecast_hour     int64         \n#  21  gas_forecast_hour      int64         \n# dtypes: datetime64[ns](4), float64(6), int32(3), int64(9)\n# memory usage: 315.6 MB\n# None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.472182Z","iopub.status.idle":"2025-08-30T18:28:11.472592Z","shell.execute_reply.started":"2025-08-30T18:28:11.472391Z","shell.execute_reply":"2025-08-30T18:28:11.472409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(merged_weather.info(memory_usage='deep'))\n\n# Antes\n# <class 'pandas.core.frame.DataFrame'>\n# RangeIndex: 3418242 entries, 0 to 3418241\n# Data columns (total 38 columns):\n#  #   Column                              Dtype         \n# ---  ------                              -----         \n#  0   latitude                            float64     # 2 digitos y 1 decimal  \n#  1   longitude                           float64     # 2 digitos y 1 decimal   \n#  2   weather_forecast_hour               int64       # 0 a 4 digitos \n#  3   ftemperature                        float64     #  \n#  4   fdewpoint                           float64       \n#  5   fcloudcover_high                    float64       \n#  6   fcloudcover_low                     float64       \n#  7   fcloudcover_mid                     float64       \n#  8   fcloudcover_total                   float64       \n#  9   f10_metre_u_wind_component          float64       \n#  10  f10_metre_v_wind_component          float64       \n#  11  f_data_block_id                     int64         \n#  12  forecast_datetime                   datetime64[ns]\n#  13  fdirect_solar_radiation             float64       \n#  14  fsurface_solar_radiation_downwards  float64       \n#  15  fsnowfall                           float64       \n#  16  ftotal_precipitation                float64       \n#  17  county                              Int64         \n#  18  forecast_date                       object        \n#  19  hour                                int32         \n#  20  minute                              int32         \n#  21  second                              int32         \n#  22  datetime                            datetime64[ns]\n#  23  temperature                         float64       \n#  24  dewpoint                            float64       \n#  25  rain                                float64       \n#  26  snowfall                            float64       \n#  27  surface_pressure                    float64       \n#  28  cloudcover_total                    int64         \n#  29  cloudcover_low                      int64         \n#  30  cloudcover_mid                      int64         \n#  31  cloudcover_high                     int64         \n#  32  windspeed_10m                       float64       \n#  33  winddirection_10m                   int64         \n#  34  shortwave_radiation                 float64       \n#  35  direct_solar_radiation              float64       \n#  36  diffuse_radiation                   float64       \n#  37  data_block_id                       float64       \n# dtypes: Int64(1), datetime64[ns](2), float64(24), int32(3), int64(7), object(1)\n# memory usage: 1.0 GB\n# None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.475130Z","iopub.status.idle":"2025-08-30T18:28:11.475593Z","shell.execute_reply.started":"2025-08-30T18:28:11.475375Z","shell.execute_reply":"2025-08-30T18:28:11.475394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train3, train2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.477363Z","iopub.status.idle":"2025-08-30T18:28:11.477743Z","shell.execute_reply.started":"2025-08-30T18:28:11.477565Z","shell.execute_reply":"2025-08-30T18:28:11.477578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_weather = merged_weather.drop('forecast_datetime', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.479583Z","iopub.status.idle":"2025-08-30T18:28:11.480095Z","shell.execute_reply.started":"2025-08-30T18:28:11.479840Z","shell.execute_reply":"2025-08-30T18:28:11.479861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.481651Z","iopub.status.idle":"2025-08-30T18:28:11.482047Z","shell.execute_reply.started":"2025-08-30T18:28:11.481858Z","shell.execute_reply":"2025-08-30T18:28:11.481871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merged_weather.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.483169Z","iopub.status.idle":"2025-08-30T18:28:11.483481Z","shell.execute_reply.started":"2025-08-30T18:28:11.483344Z","shell.execute_reply":"2025-08-30T18:28:11.483356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train, client, electricity_prices, forecast_weather, gas_prices, historical_weather, weather_station","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.485599Z","iopub.status.idle":"2025-08-30T18:28:11.485891Z","shell.execute_reply.started":"2025-08-30T18:28:11.485759Z","shell.execute_reply":"2025-08-30T18:28:11.485772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merge por county y datetime\ndef process_weather_chunks_corrected(start_chunk, end_chunk):\n    periods = [\n        ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), ('2021-12-01', '2022-01-01'),\n        ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'),\n        ('2022-05-01', '2022-06-01'), ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n        ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), ('2022-12-01', '2023-01-01'),\n        ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'),\n        ('2023-05-01', '2023-05-31')\n    ]\n    \n    for i in range(start_chunk, end_chunk):\n        start_date, end_date = periods[i]\n        print(f\"Procesando período {i+1}: {start_date} a {end_date}\")\n        \n        # Filtrar por período\n        mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n        train4_chunk = train4[mask_train].copy()\n        \n        mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n        weather_chunk = merged_weather[mask_weather].copy()\n        \n        # Merge CORREGIDO - por county y datetime\n        chunk_result = pd.merge(train4_chunk, weather_chunk, \n                               on=['county', 'datetime'], \n                               how='left')\n        \n        chunk_result.to_parquet(f'train5_corrected_chunk_{i+1}.parquet', index=False)\n        print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n        \n        # Liberar memoria\n        del train4_chunk, weather_chunk, chunk_result\n\n# Probar con un chunk pequeño primero\nprocess_weather_chunks_corrected(0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.487236Z","iopub.status.idle":"2025-08-30T18:28:11.487571Z","shell.execute_reply.started":"2025-08-30T18:28:11.487430Z","shell.execute_reply":"2025-08-30T18:28:11.487443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(0, 3)\n","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-08-30T18:28:11.489732Z","shell.execute_reply.started":"2025-08-30T18:28:11.489518Z","shell.execute_reply":"2025-08-30T18:28:11.489537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(3, 9)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.492324Z","iopub.status.idle":"2025-08-30T18:28:11.492646Z","shell.execute_reply.started":"2025-08-30T18:28:11.492491Z","shell.execute_reply":"2025-08-30T18:28:11.492521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(9, 15)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.494079Z","iopub.status.idle":"2025-08-30T18:28:11.494610Z","shell.execute_reply.started":"2025-08-30T18:28:11.494385Z","shell.execute_reply":"2025-08-30T18:28:11.494407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(15, 21)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.495821Z","iopub.status.idle":"2025-08-30T18:28:11.496337Z","shell.execute_reply.started":"2025-08-30T18:28:11.496055Z","shell.execute_reply":"2025-08-30T18:28:11.496085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenar de a grupos para evitar saturar memoria\ndef concat_chunks_by_groups(group_size=1):\n    group_files = []\n    \n    # Crear grupos intermedios\n    for group_start in range(1, 22, group_size):\n        group_end = min(group_start + group_size, 22)\n        print(f\"Procesando grupo: chunks {group_start} a {group_end-1}\")\n        \n        # Leer chunks del grupo\n        group_chunks = []\n        for i in range(group_start, group_end):\n            try:\n                chunk = pd.read_parquet(f'train5_corrected_chunk_{i}.parquet')  # CORREGIDO\n                group_chunks.append(chunk)\n            except FileNotFoundError:\n                print(f\"Archivo train5_corrected_chunk_{i}.parquet no encontrado\")\n        \n        # Concatenar grupo y guardar\n        if group_chunks:\n            group_df = pd.concat(group_chunks, ignore_index=True)\n            group_filename = f'train5_group_{group_start}_{group_end-1}.parquet'\n            group_df.to_parquet(group_filename, index=False)\n            group_files.append(group_filename)\n            print(f\"Grupo guardado: {group_filename} - Shape: {group_df.shape}\")\n            \n            # Liberar memoria\n            del group_chunks, group_df\n    \n    # Ahora concatenar los grupos (serán muchos menos archivos)\n    print(\"Concatenando grupos finales...\")\n    final_chunks = []\n    for group_file in group_files:\n        group_data = pd.read_parquet(group_file)\n        final_chunks.append(group_data)\n    \n    train5 = pd.concat(final_chunks, ignore_index=True)\n    return train5\n\n# Ejecutar\ntrain5 = concat_chunks_by_groups(1)\nprint(f\"Dataset final: {train5.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.499142Z","iopub.status.idle":"2025-08-30T18:28:11.499530Z","shell.execute_reply.started":"2025-08-30T18:28:11.499377Z","shell.execute_reply":"2025-08-30T18:28:11.499392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.501454Z","iopub.status.idle":"2025-08-30T18:28:11.501859Z","shell.execute_reply.started":"2025-08-30T18:28:11.501686Z","shell.execute_reply":"2025-08-30T18:28:11.501700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.503551Z","iopub.status.idle":"2025-08-30T18:28:11.503852Z","shell.execute_reply.started":"2025-08-30T18:28:11.503718Z","shell.execute_reply":"2025-08-30T18:28:11.503731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\n\n# 1. Identificar columnas a eliminar y renombrar SIN cargar datos\nsample = train5.head(1000)  # Solo una muestra\ncols_to_drop = [col for col in sample.columns if col.endswith('_y')]\nrename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\nprint(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n\n# 2. Procesar por chunks de 1M filas\nchunk_size = 1_000_000\nchunks_processed = []\n\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Procesando chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx].copy()\n    \n    # Filtrar county != 12\n    chunk = chunk[chunk['county'] != 12]\n    \n    if len(chunk) > 0:  # Solo si quedan filas\n        # Eliminar columnas _y\n        chunk = chunk.drop(columns=cols_to_drop)\n        \n        # Renombrar _x\n        chunk.rename(columns=rename_dict, inplace=True)\n        \n        chunks_processed.append(chunk)\n    \n    # Limpiar memoria\n    del chunk\n    gc.collect()\n    \n    print(f\"Memoria liberada, chunks guardados: {len(chunks_processed)}\")\n\n# 3. Concatenar chunks finales\nprint(\"Concatenando chunks finales...\")\ntrain5_clean = pd.concat(chunks_processed, ignore_index=True)\n\n# 4. Limpiar\ndel chunks_processed, train5\ngc.collect()\n\nprint(f\"Dataset final: {train5_clean.shape[0]:,} filas y {train5_clean.shape[1]} columnas\")\n\n# Renombrar para usar el mismo nombre\ntrain5 = train5_clean\ndel train5_clean\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.504804Z","iopub.status.idle":"2025-08-30T18:28:11.505162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.506301Z","iopub.status.idle":"2025-08-30T18:28:11.507136Z","shell.execute_reply.started":"2025-08-30T18:28:11.506898Z","shell.execute_reply":"2025-08-30T18:28:11.506917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.508115Z","iopub.status.idle":"2025-08-30T18:28:11.508506Z","shell.execute_reply.started":"2025-08-30T18:28:11.508336Z","shell.execute_reply":"2025-08-30T18:28:11.508354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Orden específico que solicitaste\nmain_cols = [\n    'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n    'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n    'longitude', 'lowest_price_per_mwh', \n    'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', 'installed_capacity'\n]\n\n# Pares de clima: pronóstico (f) vs histórico - lado a lado para comparar\nweather_pairs = [\n    # Temperatura\n    'ftemperature', 'temperature',\n    \n    # Punto de rocío  \n    'fdewpoint', 'dewpoint',\n    \n    # Cobertura de nubes\n    'fcloudcover_high', 'cloudcover_high',\n    'fcloudcover_low', 'cloudcover_low', \n    'fcloudcover_mid', 'cloudcover_mid',\n    'fcloudcover_total', 'cloudcover_total',\n    \n    # Viento\n    'f10_metre_u_wind_component', 'windspeed_10m',\n    'f10_metre_v_wind_component', 'winddirection_10m',\n    \n    # Radiación solar\n    'fdirect_solar_radiation', 'direct_solar_radiation',\n    'fsurface_solar_radiation_downwards', 'shortwave_radiation',\n    'diffuse_radiation',  # solo histórico\n    \n    # Precipitación\n    'fsnowfall', 'snowfall',\n    'ftotal_precipitation', 'rain',\n    \n    # Otras variables climáticas\n    'surface_pressure',  # solo histórico\n    'weather_forecast_hour'\n]\n\n# Construir orden final\nfinal_order = main_cols.copy()\n\n# Agregar solo las columnas que existen\nfor col in weather_pairs:\n    if col in train5.columns:\n        final_order.append(col)\n\n# Agregar cualquier columna restante\nremaining_cols = [col for col in train5.columns if col not in final_order]\nfinal_order.extend(remaining_cols)\n\nprint(f\"Reordenando {len(final_order)} columnas...\")\nprint(\"Pares clima encontrados:\")\nweather_cols_found = [col for col in weather_pairs if col in train5.columns]\nfor i in range(0, len(weather_cols_found), 2):\n    pair = weather_cols_found[i:i+2]\n    print(f\"  {' vs '.join(pair)}\")\n\n# Reordenar\ntrain5 = train5[final_order]\n\nprint(\"¡Columnas reordenadas!\")\nprint(f\"Primeras 10: {list(train5.columns[:10])}\")\nprint(f\"Clima inicia en posición: {final_order.index(weather_cols_found[0]) if weather_cols_found else 'N/A'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.510760Z","iopub.status.idle":"2025-08-30T18:28:11.511140Z","shell.execute_reply.started":"2025-08-30T18:28:11.510978Z","shell.execute_reply":"2025-08-30T18:28:11.510994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.tail(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.512271Z","iopub.status.idle":"2025-08-30T18:28:11.512594Z","shell.execute_reply.started":"2025-08-30T18:28:11.512445Z","shell.execute_reply":"2025-08-30T18:28:11.512457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hacer scatterplot con \n# train3.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) el alpha para ver con mas claridad las zonas mas poblada\n# podria agregar colores para diferencia temperatura o exposicion a rayos solares o nubes y poder encarar distintos el dataset\n# sino funciona probar %matplotlib inline\n# train3.hist(bins=50, figsize=(20,15)) ver tmb los histograma de los datos cuando este todo junto# cambiar los bins a valores mas chicos\n# plt.show() agregar si hace falta\n# corr_matrix = train3.corr()\n# corr_matrix[\"target\"].sort_values(ascending=False) ver si aplica pero creo que no , recordad probar en columnas numericas si hay otra relacion\n# ejemplo crear columna con raiz/potencia/log y ver como apartan esos valores en correlacion\n# o usar scatter_matrix() de pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.514045Z","iopub.status.idle":"2025-08-30T18:28:11.514537Z","shell.execute_reply.started":"2025-08-30T18:28:11.514385Z","shell.execute_reply":"2025-08-30T18:28:11.514402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# Procesar por chunks de 1M filas\nchunk_size = 1_000_000\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\n# Variables para acumular resultados\nmissing_dates_count = defaultdict(int)\nmissing_by_county = defaultdict(int)\nfirst_missing = None\nlast_complete = None\ntotal_missing = 0\n\nprint(f\"Procesando {total_rows:,} filas en {n_chunks} chunks...\")\n\nweather_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx][['datetime', 'county'] + weather_cols]\n    \n    # Encontrar filas con NaN en clima\n    missing_mask = chunk[weather_cols].isna().any(axis=1)\n    missing_chunk = chunk[missing_mask]\n    \n    if len(missing_chunk) > 0:\n        # Acumular conteos por fecha\n        dates = missing_chunk['datetime'].dt.date.value_counts()\n        for date, count in dates.items():\n            missing_dates_count[date] += count\n        \n        # Acumular por county\n        counties = missing_chunk['county'].value_counts()\n        for county, count in counties.items():\n            missing_by_county[county] += count\n        \n        # Tracking de primera fecha faltante\n        chunk_first_missing = missing_chunk['datetime'].min()\n        if first_missing is None or chunk_first_missing < first_missing:\n            first_missing = chunk_first_missing\n        \n        total_missing += len(missing_chunk)\n    \n    # Tracking de última fecha completa\n    complete_chunk = chunk[chunk['latitude'].notna()]\n    if len(complete_chunk) > 0:\n        chunk_last_complete = complete_chunk['datetime'].max()\n        if last_complete is None or chunk_last_complete > last_complete:\n            last_complete = chunk_last_complete\n    \n    # Limpiar memoria\n    del chunk, missing_chunk\n    gc.collect()\n    \n    if i % 5 == 0:  # cada 5 chunks\n        print(f\"  Faltantes encontrados hasta ahora: {total_missing:,}\")\n\n# Mostrar resultados\nprint(f\"\\n=== RESULTADOS ===\")\nprint(f\"Total filas con clima faltante: {total_missing:,}\")\nprint(f\"Total fechas afectadas: {len(missing_dates_count)}\")\n\nif missing_dates_count:\n    # Convertir a Series para ordenar\n    missing_dates_series = pd.Series(missing_dates_count).sort_index()\n    \n    print(f\"\\nPrimeras 10 fechas con más faltantes:\")\n    print(missing_dates_series.head(10))\n    \n    print(f\"\\nRango de fechas problemáticas:\")\n    print(f\"Desde: {missing_dates_series.index.min()}\")\n    print(f\"Hasta: {missing_dates_series.index.max()}\")\n    \n    print(f\"\\nTop 5 counties con más faltantes:\")\n    county_series = pd.Series(missing_by_county).sort_values(ascending=False)\n    print(county_series.head())\n    \n    print(f\"\\nTimeline:\")\n    print(f\"Última fecha completa: {last_complete}\")\n    print(f\"Primera fecha faltante: {first_missing}\")\n    if last_complete and first_missing:\n        print(f\"Gap de tiempo: {first_missing - last_complete}\")\nelse:\n    print(\"No se encontraron datos de clima faltantes!\")\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.516674Z","iopub.status.idle":"2025-08-30T18:28:11.517534Z","shell.execute_reply.started":"2025-08-30T18:28:11.517256Z","shell.execute_reply":"2025-08-30T18:28:11.517283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hago drop de los faltantes\n\n# Eliminar filas con datos de clima faltantes\nprint(f\"Dataset original: {len(train5):,} filas\")\n\n# Drop filas con NaN en clima\nweather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\ntrain5 = train5.dropna(subset=weather_key_cols)\n\nprint(f\"Dataset limpio: {len(train5):,} filas\")\nprint(f\"Filas eliminadas: {32_965_048 - len(train5):,}\")\n\n# Verificar que no quedan NaN en clima\nremaining_weather_nan = train5[weather_key_cols].isna().sum().sum()\nprint(f\"NaN restantes en clima: {remaining_weather_nan}\")\n\n# Verificar fechas disponibles para el período de predicción\ntest_period = train5[\n    (train5['datetime'].dt.date >= pd.to_datetime('2023-05-20').date()) &\n    (train5['datetime'].dt.date <= pd.to_datetime('2023-05-31').date())\n]['datetime'].dt.date.unique()\n\nprint(f\"\\nFechas disponibles cerca del período de test:\")\nprint(sorted(test_period))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.518830Z","iopub.status.idle":"2025-08-30T18:28:11.519295Z","shell.execute_reply.started":"2025-08-30T18:28:11.519014Z","shell.execute_reply":"2025-08-30T18:28:11.519037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.521229Z","iopub.status.idle":"2025-08-30T18:28:11.521747Z","shell.execute_reply.started":"2025-08-30T18:28:11.521536Z","shell.execute_reply":"2025-08-30T18:28:11.521554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.523106Z","iopub.status.idle":"2025-08-30T18:28:11.523803Z","shell.execute_reply.started":"2025-08-30T18:28:11.523342Z","shell.execute_reply":"2025-08-30T18:28:11.523361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.526271Z","iopub.status.idle":"2025-08-30T18:28:11.526737Z","shell.execute_reply.started":"2025-08-30T18:28:11.526520Z","shell.execute_reply":"2025-08-30T18:28:11.526539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Inicio de guia para hacer Pipeline\n#Voy pegando los codigos de lo que fui aplicando asi podes usarlo de guia\n#quiero poder aplicarlo a Train y a Test ,\n#si hay diferencias con test y queres hacer Un Pipeline para Train y otro Pipeline para Test para que sea mas simple , me da igual\n#esto termina teniendo 33kk de filas en el final ,asi que tene en cuenta opciones que vayan haciendo de a 1kk para no saturar la memoria , o ir borrando\n\n#fui poniendo los pasos que fui haciendo , obviamente hay que optimizarlo y hay cosas que no son necesarias\n#me parece que para train podria ser en 3 partes , primero preparar todo hasta antes del ultimo merge , luego el merge y tercero encargarse de los problemas del merge\n#se que para trabajar luego deberia haber otro pipeline para usar standarcaler y otras cosas mas , pero quiero dejarlo hasta aca para ver por mi mismo \n# de que forma continuar\n\n\n\n\n#1er paso el weather to county que utilizo en train y luego voy a utilizar en test\n\nimport unicodedata\nfrom sklearn.neighbors import NearestNeighbors\n\n# === 1) Diccionarios canonicos ===\ncounty_id_to_name_map = {\n    0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"JÄRVAMAA\",\n    4: \"JÕGEVAMAA\", 5: \"LÄÄNE-VIRUMAA\", 6: \"LÄÄNEMAA\", 7: \"PÄRNUMAA\",\n    8: \"PÕLVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n    12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"VÕRUMAA\"\n}\ncounty_name_to_id_map = {v: k for k, v in county_id_to_name_map.items()}\n\n# Para normalizar entradas (sin/ con acentos, \"County\"/\"maakond\", etc.)\nsynonyms_to_canonical = {\n    # clave \"base\" (sin acentos/variantes) -> canon con acentos\n    \"HARJU\": \"HARJUMAA\",\n    \"HIIU\": \"HIIUMAA\",\n    \"IDA-VIRU\": \"IDA-VIRUMAA\",\n    \"JARVA\": \"JÄRVAMAA\", \"JÄRVA\": \"JÄRVAMAA\",\n    \"JOGEVA\": \"JÕGEVAMAA\", \"JÕGEVA\": \"JÕGEVAMAA\",\n    \"LAANE-VIRU\": \"LÄÄNE-VIRUMAA\", \"LÄÄNE-VIRU\": \"LÄÄNE-VIRUMAA\",\n    \"LAANE\": \"LÄÄNEMAA\", \"LÄÄNE\": \"LÄÄNEMAA\",\n    \"PARNU\": \"PÄRNUMAA\", \"PÄRNU\": \"PÄRNUMAA\",\n    \"POLVA\": \"PÕLVAMAA\", \"PÕLVA\": \"PÕLVAMAA\",\n    \"RAPLA\": \"RAPLAMAA\",\n    \"SAARE\": \"SAAREMAA\",\n    \"TARTU\": \"TARTUMAA\",\n    \"VALGA\": \"VALGAMAA\",\n    \"VILJANDI\": \"VILJANDIMAA\",\n    \"VORU\": \"VÕRUMAA\", \"VÕRU\": \"VÕRUMAA\",\n\n    # versiones ya canónicas\n    \"HARJUMAA\": \"HARJUMAA\",\n    \"HIIUMAA\": \"HIIUMAA\",\n    \"IDA-VIRUMAA\": \"IDA-VIRUMAA\",\n    \"JÄRVAMAA\": \"JÄRVAMAA\",\n    \"JÕGEVAMAA\": \"JÕGEVAMAA\",\n    \"LÄÄNE-VIRUMAA\": \"LÄÄNE-VIRUMAA\",\n    \"LÄÄNEMAA\": \"LÄÄNEMAA\",\n    \"PÄRNUMAA\": \"PÄRNUMAA\",\n    \"PÕLVAMAA\": \"PÕLVAMAA\",\n    \"RAPLAMAA\": \"RAPLAMAA\",\n    \"SAAREMAA\": \"SAAREMAA\",\n    \"TARTUMAA\": \"TARTUMAA\",\n    \"VALGAMAA\": \"VALGAMAA\",\n    \"VILJANDIMAA\": \"VILJANDIMAA\",\n    \"VÕRUMAA\": \"VÕRUMAA\",\n}\n\ndef strip_accents(s: str) -> str:\n    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n\ndef normalize_county_name(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).upper().strip()\n    # eliminar sufijos frecuentes\n    for suf in [\" COUNTY\", \" MAAKOND\"]:\n        if s.endswith(suf):\n            s = s[: -len(suf)]\n    s = s.replace(\"  \", \" \").replace(\"–\", \"-\")  # guion raro -> normal\n    s_noacc = strip_accents(s)\n    s_noacc = s_noacc.replace(\"  \", \" \")\n    # prueba de match exacto o por prefijo lógico\n    # ej. \"HARJU\", \"HARJU MAAKOND\" -> \"HARJU\"\n    token = s_noacc.split()[0]  # primer palabra alcanza en estos casos\n    # casos con guion (IDA-VIRU, LAA NE-VIRU, etc.)\n    if \"-\" in s_noacc:\n        token = s_noacc  # mantener completo en compuestos\n\n    # intento 1: match directo con cadena completa sin acentos\n    if s_noacc in synonyms_to_canonical:\n        return synonyms_to_canonical[s_noacc]\n    # intento 2: match por token/prefijo conocido\n    if token in synonyms_to_canonical:\n        return synonyms_to_canonical[token]\n    # intento 3: si ya vino canónico exacto (con acentos)\n    if s in synonyms_to_canonical:\n        return synonyms_to_canonical[s]\n\n    return np.nan  # no lo reconozco\n\n# === 2) Normalizar lo existente sin romper los NaN ===\n# (NO convertir NaN a \"NAN\")\nweather_station1['county_name_norm'] = weather_station1['county_name'].apply(normalize_county_name)\n\n# Si tengo 'county' numérico pero falta nombre, lo inferimos del ID\nmask = weather_station1['county_name_norm'].isna() & weather_station1['county'].notna()\nweather_station1.loc[mask, 'county_name_norm'] = (\n    weather_station1.loc[mask, 'county'].astype(int).map(county_id_to_name_map)\n)\n\n# === 3) Completar faltantes por k-NN (lat/lon) ===\nfcols = ['latitude', 'longitude']\nknown = weather_station1.dropna(subset=['county_name_norm']).copy()\nunknown = weather_station1[weather_station1['county_name_norm'].isna()].copy()\n\nif not unknown.empty and not known.empty:\n    # k-NN en (lat, lon) usando Haversine (necesita radianes)\n    # ojo: haversine en sklearn requiere [lat, lon] en radianes\n    known_rad = np.radians(known[fcols].values)\n    unknown_rad = np.radians(unknown[fcols].values)\n\n    nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n    nbrs.fit(known_rad)\n    distances, idxs = nbrs.kneighbors(unknown_rad)\n\n    # votación simple de los 3 vecinos (majority vote)\n    nn_names = known['county_name_norm'].values\n    filled = []\n    for neigh_idx in idxs:\n        cands = nn_names[neigh_idx]\n        # mayoría; en empate, tomar el 1er vecino\n        vals, counts = np.unique(cands, return_counts=True)\n        filled.append(vals[np.argmax(counts)])\n\n    weather_station1.loc[unknown.index, 'county_name_norm'] = filled\n\n# === 4) Sincronizar columnas finales ===\nweather_station1['county_name'] = weather_station1['county_name_norm']\nweather_station1['county'] = weather_station1['county_name'].map(county_name_to_id_map).astype('Int64')\n\n# Cualquier remanente a UNKNOWN (12)\nweather_station1.loc[weather_station1['county_name'].isna(), 'county'] = 12\nweather_station1['county'] = weather_station1['county'].astype('Int64')\n\n# Limpieza\nweather_station1.drop(columns=['county_name_norm'], inplace=True)\n\n# Chequeo rápido\nprint(weather_station1[['county_name','longitude','latitude','county']].head(25))\nprint(\"\\nFaltantes aún:\", weather_station1['county_name'].isna().sum())\n\n\n#2do paso crear copias antes de empezar a juntar/merge los df , las cuales elimino al final para liberar memoria\n# en algunos caso elimino los na con drop , en otros no hace falta respetar como esta aca \n# en test no hay historical porque estoy prediciendo a futuro \n# weather_station2 es asi por lo que hice arriba\n\ntrain1 = train.dropna()# train                 \nclient1 = client.copy() # client               \nelectricity_prices1 = electricity_prices.copy() # electricity_prices  \nforecast_weather1 = forecast_weather.dropna() # forecast_weather    \ngas_prices1 = gas_prices.copy() # gas_prices                        \nhistorical_weather1 = historical_weather.copy() # historical_weather    \nweather_station2 = weather_station1.copy()      \n\ntest1 = test.copy() # test                 \nclient_t = client_t.copy() # client_t              \nelectricity_prices_t1 = electricity_prices_t.copy() # electricity_prices_t  \nforecast_weather_t1 = forecast_weather_t.copy() # forecast_weather_t    \ngas_prices_t1 = gas_prices_t.copy() # gas_prices_t          \nrevealed_targets1 = revealed_targets.copy() # revealed_targets\n\n#3er paso trabajo con train1 y gas_prices1 para hacer el primer merge\n\n# el forecast de gas es de 24 horas\n\n# Convertir datetime y crear columnas separadas en train1\ntrain1['datetime'] = pd.to_datetime(train1['datetime'])\ntrain1['hour'] = train1['datetime'].dt.hour\ntrain1['forecast_date'] = train1['datetime']\n\n#\ngas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\ngas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\ngas_prices1['hour'] = gas_prices1['forecast_date'].dt.hour\ngas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n\n# Expandir gas_prices1 a nivel horario - solo columnas necesarias\ngas_hourly = []\nfor _, row in gas_prices1.iterrows():\n    for hour in range(24):\n        new_row = {\n            'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n            'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n            'highest_price_per_mwh': row['highest_price_per_mwh'],\n            'gas_origin_date': row['gas_origin_date']\n        }\n        gas_hourly.append(new_row)\n\ngas_prices1_hourly = pd.DataFrame(gas_hourly)\n\n# Merge train1 con gas_prices1_hourly\ntrain2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')\n# completo los datos faltantes antes de continuar\ntrain2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\ntrain2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n\n# Eliminar columna gas_origin_date\ntrain2 = train2.drop(columns=['gas_origin_date'])\n\n# ahora se llama train2 que es train1+gas_prices1\n# a \"gas_origin_date\" despues lo elimino , si lees todo el pipeline y cuando lo corrijas ves que es mejor eliminarlo ahora , hacelo ahora \n\n\n\n#3er paso\n\n\n# Preparar electricity_prices1 para el merge con train2\nelectricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\nelectricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n\n# Extraer componentes de tiempo\nelectricity_prices1['hour'] = electricity_prices1['forecast_date'].dt.hour\n\n\n# Merge\nelec_columns = ['forecast_date', 'euros_per_mwh']\ntrain3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')\n\n# me ocupo de los valores faltantes antes de continuar\n\ntrain3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n\n\n#4to paso , preparo client1 para unirlo con train3\n\n# Preparar client1\nclient1['date'] = pd.to_datetime(client1['date']).dt.date\n\n# Merge con client1 usando datetime de train3\ntrain4 = train3.merge(client1.drop('data_block_id', axis=1),\n                      left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                      right_on=['product_type', 'county', 'is_business', 'date'],\n                      how='left')\n\n# Limpiar\ntrain4 = train4.drop('date', axis=1)\ntrain4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})\n\n# me ocupo de los valores faltantes antes de continuar\n# decido completar asi los ultimos 2 dias que faltan\n# Verificar el problema\nprint(\"NaN en datos de clientes:\")\nprint(f\"eic_count: {train4['eic_count'].isna().sum():,}\")\nprint(f\"installed_capacity: {train4['installed_capacity'].isna().sum():,}\")\n\n# Ver las fechas con problemas\nnan_dates = train4[train4['eic_count'].isna()]['datetime'].dt.date.unique()\nprint(f\"Fechas con NaN: {sorted(nan_dates)}\")\n\n# Ver último día con datos\nlast_valid_date = train4[train4['eic_count'].notna()]['datetime'].dt.date.max()\nprint(f\"Último día con datos: {last_valid_date}\")\n\n# Forward fill por grupo (county + is_business + product_type)\n# Esto mantiene los últimos valores conocidos para cada combinación\nprint(\"\\nCompletando con forward fill por grupo...\")\n\n# Ordenar por datetime para asegurar orden correcto\ntrain4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n\n# Forward fill por grupo\ntrain4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\ntrain4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n\n# Verificar resultado\nprint(f\"\\nDespués del forward fill:\")\nprint(f\"eic_count NaN: {train4['eic_count'].isna().sum()}\")\nprint(f\"installed_capacity NaN: {train4['installed_capacity'].isna().sum()}\")\n\n# Si aún quedan NaN, son combinaciones nuevas - usar 0 o promedio\nremaining_nan = train4['eic_count'].isna().sum()\nif remaining_nan > 0:\n    print(f\"\\n{remaining_nan} NaN restantes (combinaciones nuevas)\")\n    print(\"Opciones:\")\n    print(\"1. Rellenar con 0\")\n    print(\"2. Rellenar with promedio del county\")\n    \n    # Opción recomendada: promedio por county\n    train4['eic_count'] = train4['eic_count'].fillna(\n        train4.groupby('county')['eic_count'].transform('mean')\n    )\n    train4['installed_capacity'] = train4['installed_capacity'].fillna(\n        train4.groupby('county')['installed_capacity'].transform('mean')\n    )\n    \n    print(f\"NaN finales - eic_count: {train4['eic_count'].isna().sum()}\")\n    print(f\"NaN finales - installed_capacity: {train4['installed_capacity'].isna().sum()}\")\n\nprint(f\"\\nDataset train4: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")\n\n# 5to paso antes de continuar con train4 sigo con los df relacionados al clima , junto forecast_weather1 con weather station1 ,\n\n# Merge forecast_weather1 con weather_station1 por coordenadas\nforecast_weather1 = forecast_weather1.merge(weather_station3[['latitude', 'longitude', 'county', 'county_name']], \n                                          on=['latitude', 'longitude'], \n                                          how='left')\n\nprint(f\"Filas con county asignado: {forecast_weather1['county'].notna().sum()}\")\nprint(f\"Total filas: {len(forecast_weather1)}\")\n\n# completo los valores faltantes\n\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\n\n# Identificar filas sin county asignado\nmissing_county = forecast_weather1['county'].isna()\nprint(f\"Filas sin county: {missing_county.sum()}\")\n\nif missing_county.sum() > 0:\n   # Datos conocidos (con county)\n   known = weather_station1[['latitude', 'longitude', 'county', 'county_name']].copy()\n   \n   # Datos faltantes\n   missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n   \n   # K-NN con distancia euclidiana (para coordenadas pequeñas como Estonia)\n   nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n   nbrs.fit(known[['latitude', 'longitude']].values)\n   \n   # Encontrar vecino más cercano\n   distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n   \n   # Asignar county y county_name del vecino más cercano\n   nearest_counties = known.iloc[indices.flatten()]\n   \n   forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n   forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n\nprint(f\"Filas con county después del K-NN: {forecast_weather1['county'].notna().sum()}\")\n\n#6to paso trabajo sobre forecast_weather \n\nforecast_weather2 = forecast_weather1.copy()\n\n# Eliminar columnas innecesarias\nforecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n\n\n# Convertir forecast_datetime a datetime\nforecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n\n# Crear columnas temporales\nforecast_weather2['forecast_date'] = forecast_weather2['forecast_datetime'].dt.date\nforecast_weather2['hour'] = forecast_weather2['forecast_datetime'].dt.hour\n\n# Renombrar variables con 'f' adelante (excepto las especificadas)\nrename_dict = {}\nfor col in forecast_weather2.columns:\n   if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                  'forecast_datetime', 'forecast_date', 'hour']:\n       rename_dict[col] = 'f' + col\n\nforecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n\nprint(\"Columnas después de transformaciones:\")\nprint(forecast_weather2.columns.tolist())\n\nforecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\":\"weather_forecast_hour\"})\n\n#7mo paso preparo y hago merge con historical weather\n\nhistorical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n\n# Renombrar y merge\nforecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n\nmerged_weather = pd.merge(\n    forecast_weather2,\n    historical_weather1,\n    left_on=['latitude', 'longitude', 'forecast_datetime'],\n    right_on=['latitude', 'longitude', 'datetime'],\n    how='inner'\n)\n\n#8vo paso , optimizo datos por un tema de memoria antes de continuar\n\n# Optimizar tipos de datos\ntrain4['county'] = train4['county'].astype('uint8')  # 0-15\ntrain4['is_business'] = train4['is_business'].astype('uint8')  # 0-1\ntrain4['product_type'] = train4['product_type'].astype('uint8')  # 0-3\ntrain4['is_consumption'] = train4['is_consumption'].astype('uint8')  # 0-1\ntrain4['hour'] = train4['hour'].astype('uint8')  # 0-23\n\n# IDs optimizados\ntrain4['data_block_id'] = train4['data_block_id'].astype('uint16')  # 0-700\ntrain4['row_id'] = train4['row_id'].astype('uint32')  # 0-3M\ntrain4['prediction_unit_id'] = train4['prediction_unit_id'].astype('uint8')  # 0-68\n\n# Flotantes a float32 (suficiente precision para los rangos dados)\ntrain4['target'] = train4['target'].astype('float32')\ntrain4['lowest_price_per_mwh'] = train4['lowest_price_per_mwh'].astype('float32')\ntrain4['highest_price_per_mwh'] = train4['highest_price_per_mwh'].astype('float32')\ntrain4['euros_per_mwh'] = train4['euros_per_mwh'].astype('float32')\ntrain4['eic_count'] = train4['eic_count'].astype('float32')\ntrain4['installed_capacity'] = train4['installed_capacity'].astype('float32')\n\nprint(\"Optimización completada\")\nprint(train4.info(memory_usage='deep'))\n\n# optimizo y preparo para merge con train4 \n\n# Eliminar columnas innecesarias\nmerged_weather = merged_weather.drop('f_data_block_id', axis=1)\n\n# Coordenadas - float32 suficiente para la precisión geográfica\nmerged_weather['latitude'] = merged_weather['latitude'].astype('float32')  # 57.6-59.7\nmerged_weather['longitude'] = merged_weather['longitude'].astype('float32')  # 21.7-28.2\n\n# Enteros pequeños\nmerged_weather['weather_forecast_hour'] = merged_weather['weather_forecast_hour'].astype('uint8')  # 1-48\nmerged_weather['county'] = merged_weather['county'].astype('uint8')  # 0-15\nmerged_weather['hour'] = merged_weather['hour'].astype('uint8')  # 0-23\n\n# IDs\nmerged_weather['data_block_id'] = merged_weather['data_block_id'].astype('uint16')  # 1-637\n\n# Variables meteorológicas - float32 es suficiente\nweather_float_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                     'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                     'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                     'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                     'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                     'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                     'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                     'direct_solar_radiation', 'diffuse_radiation']\n\nfor col in weather_float_cols:\n    merged_weather[col] = merged_weather[col].astype('float32')\n\nprint(\"Optimización de merged_weather completada\")\nprint(merged_weather.info(memory_usage='deep'))\n\nmerged_weather = merged_weather.drop('forecast_datetime', axis=1)\n\n\n#9no paso , defino funciones y luego merge de a chunks por un tema de memoria\n\ndef process_weather_chunks(start_chunk, end_chunk):\n    periods = [\n        ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), ('2021-12-01', '2022-01-01'),\n        ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'),\n        ('2022-05-01', '2022-06-01'), ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n        ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), ('2022-12-01', '2023-01-01'),\n        ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'),\n        ('2023-05-01', '2023-05-31')\n    ]\n    \n    for i in range(start_chunk, end_chunk):\n        start_date, end_date = periods[i]\n        print(f\"Procesando período {i+1}: {start_date} a {end_date}\")\n        \n        # Filtrar por período\n        mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n        train4_chunk = train4[mask_train].copy()\n        \n        mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n        weather_chunk = merged_weather[mask_weather].copy()\n        \n        # Merge y guardar\n        chunk_result = pd.merge(train4_chunk, weather_chunk, on='datetime', how='left')\n        chunk_result.to_parquet(f'train5_chunk_{i+1}.parquet', index=False)\n        print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n        \n        # Liberar memoria\n        del train4_chunk, weather_chunk, chunk_result\n\n# lo hice asi para no saturar\nprocess_weather_chunks_corrected(0, 3)\n\nprocess_weather_chunks_corrected(3, 9)\n\nprocess_weather_chunks_corrected(9, 15)\n\nprocess_weather_chunks_corrected(15, 21)\n\n\n#junto todo\n\n# Concatenar de a grupos para evitar saturar memoria\ndef concat_chunks_by_groups(group_size=1):\n    group_files = []\n    \n    # Crear grupos intermedios\n    for group_start in range(1, 22, group_size):\n        group_end = min(group_start + group_size, 22)\n        print(f\"Procesando grupo: chunks {group_start} a {group_end-1}\")\n        \n        # Leer chunks del grupo\n        group_chunks = []\n        for i in range(group_start, group_end):\n            try:\n                chunk = pd.read_parquet(f'train5_corrected_chunk_{i}.parquet')  # CORREGIDO\n                group_chunks.append(chunk)\n            except FileNotFoundError:\n                print(f\"Archivo train5_corrected_chunk_{i}.parquet no encontrado\")\n        \n        # Concatenar grupo y guardar\n        if group_chunks:\n            group_df = pd.concat(group_chunks, ignore_index=True)\n            group_filename = f'train5_group_{group_start}_{group_end-1}.parquet'\n            group_df.to_parquet(group_filename, index=False)\n            group_files.append(group_filename)\n            print(f\"Grupo guardado: {group_filename} - Shape: {group_df.shape}\")\n            \n            # Liberar memoria\n            del group_chunks, group_df\n    \n    # Ahora concatenar los grupos (serán muchos menos archivos)\n    print(\"Concatenando grupos finales...\")\n    final_chunks = []\n    for group_file in group_files:\n        group_data = pd.read_parquet(group_file)\n        final_chunks.append(group_data)\n    \n    train5 = pd.concat(final_chunks, ignore_index=True)\n    return train5\n\n# Ejecutar\ntrain5 = concat_chunks_by_groups(1)\nprint(f\"Dataset final: {train5.shape}\")\n\n#10mo paso , trabajo los problemas generados del merge\n\nimport gc\n\n# 1. Identificar columnas a eliminar y renombrar SIN cargar datos\nsample = train5.head(1000)  # Solo una muestra\ncols_to_drop = [col for col in sample.columns if col.endswith('_y')]\nrename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\nprint(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n\n# 2. Procesar por chunks de 1M filas\nchunk_size = 1_000_000\nchunks_processed = []\n\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Procesando chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx].copy()\n    \n    # Filtrar county != 12\n    chunk = chunk[chunk['county'] != 12]\n    \n    if len(chunk) > 0:  # Solo si quedan filas\n        # Eliminar columnas _y\n        chunk = chunk.drop(columns=cols_to_drop)\n        \n        # Renombrar _x\n        chunk.rename(columns=rename_dict, inplace=True)\n        \n        chunks_processed.append(chunk)\n    \n    # Limpiar memoria\n    del chunk\n    gc.collect()\n    \n    print(f\"Memoria liberada, chunks guardados: {len(chunks_processed)}\")\n\n# 3. Concatenar chunks finales\nprint(\"Concatenando chunks finales...\")\ntrain5_clean = pd.concat(chunks_processed, ignore_index=True)\n\n# 4. Limpiar\ndel chunks_processed, train5\ngc.collect()\n\nprint(f\"Dataset final: {train5_clean.shape[0]:,} filas y {train5_clean.shape[1]} columnas\")\n\n# Renombrar para usar el mismo nombre\ntrain5 = train5_clean\ndel train5_clean\ngc.collect()\n\n# reordeno y veo que problemas siguen estando\n\n# Orden específico que solicitaste\nmain_cols = [\n    'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n    'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n    'longitude', 'lowest_price_per_mwh', \n    'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', 'installed_capacity'\n]\n\n# Pares de clima: pronóstico (f) vs histórico - lado a lado para comparar\nweather_pairs = [\n    # Temperatura\n    'ftemperature', 'temperature',\n    \n    # Punto de rocío  \n    'fdewpoint', 'dewpoint',\n    \n    # Cobertura de nubes\n    'fcloudcover_high', 'cloudcover_high',\n    'fcloudcover_low', 'cloudcover_low', \n    'fcloudcover_mid', 'cloudcover_mid',\n    'fcloudcover_total', 'cloudcover_total',\n    \n    # Viento\n    'f10_metre_u_wind_component', 'windspeed_10m',\n    'f10_metre_v_wind_component', 'winddirection_10m',\n    \n    # Radiación solar\n    'fdirect_solar_radiation', 'direct_solar_radiation',\n    'fsurface_solar_radiation_downwards', 'shortwave_radiation',\n    'diffuse_radiation',  # solo histórico\n    \n    # Precipitación\n    'fsnowfall', 'snowfall',\n    'ftotal_precipitation', 'rain',\n    \n    # Otras variables climáticas\n    'surface_pressure',  # solo histórico\n    'weather_forecast_hour'\n]\n\n# Construir orden final\nfinal_order = main_cols.copy()\n\n# Agregar solo las columnas que existen\nfor col in weather_pairs:\n    if col in train5.columns:\n        final_order.append(col)\n\n# Agregar cualquier columna restante\nremaining_cols = [col for col in train5.columns if col not in final_order]\nfinal_order.extend(remaining_cols)\n\nprint(f\"Reordenando {len(final_order)} columnas...\")\nprint(\"Pares clima encontrados:\")\nweather_cols_found = [col for col in weather_pairs if col in train5.columns]\nfor i in range(0, len(weather_cols_found), 2):\n    pair = weather_cols_found[i:i+2]\n    print(f\"  {' vs '.join(pair)}\")\n\n# Reordenar\ntrain5 = train5[final_order]\n\nprint(\"¡Columnas reordenadas!\")\nprint(f\"Primeras 10: {list(train5.columns[:10])}\")\nprint(f\"Clima inicia en posición: {final_order.index(weather_cols_found[0]) if weather_cols_found else 'N/A'}\")\n\n# re check de los na y lo hago de a chunks\nfrom collections import defaultdict\n\n# Procesar por chunks de 1M filas\nchunk_size = 1_000_000\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\n# Variables para acumular resultados\nmissing_dates_count = defaultdict(int)\nmissing_by_county = defaultdict(int)\nfirst_missing = None\nlast_complete = None\ntotal_missing = 0\n\nprint(f\"Procesando {total_rows:,} filas en {n_chunks} chunks...\")\n\nweather_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx][['datetime', 'county'] + weather_cols]\n    \n    # Encontrar filas con NaN en clima\n    missing_mask = chunk[weather_cols].isna().any(axis=1)\n    missing_chunk = chunk[missing_mask]\n    \n    if len(missing_chunk) > 0:\n        # Acumular conteos por fecha\n        dates = missing_chunk['datetime'].dt.date.value_counts()\n        for date, count in dates.items():\n            missing_dates_count[date] += count\n        \n        # Acumular por county\n        counties = missing_chunk['county'].value_counts()\n        for county, count in counties.items():\n            missing_by_county[county] += count\n        \n        # Tracking de primera fecha faltante\n        chunk_first_missing = missing_chunk['datetime'].min()\n        if first_missing is None or chunk_first_missing < first_missing:\n            first_missing = chunk_first_missing\n        \n        total_missing += len(missing_chunk)\n    \n    # Tracking de última fecha completa\n    complete_chunk = chunk[chunk['latitude'].notna()]\n    if len(complete_chunk) > 0:\n        chunk_last_complete = complete_chunk['datetime'].max()\n        if last_complete is None or chunk_last_complete > last_complete:\n            last_complete = chunk_last_complete\n    \n    # Limpiar memoria\n    del chunk, missing_chunk\n    gc.collect()\n    \n    if i % 5 == 0:  # cada 5 chunks\n        print(f\"  Faltantes encontrados hasta ahora: {total_missing:,}\")\n\n# Mostrar resultados\nprint(f\"\\n=== RESULTADOS ===\")\nprint(f\"Total filas con clima faltante: {total_missing:,}\")\nprint(f\"Total fechas afectadas: {len(missing_dates_count)}\")\n\nif missing_dates_count:\n    # Convertir a Series para ordenar\n    missing_dates_series = pd.Series(missing_dates_count).sort_index()\n    \n    print(f\"\\nPrimeras 10 fechas con más faltantes:\")\n    print(missing_dates_series.head(10))\n    \n    print(f\"\\nRango de fechas problemáticas:\")\n    print(f\"Desde: {missing_dates_series.index.min()}\")\n    print(f\"Hasta: {missing_dates_series.index.max()}\")\n    \n    print(f\"\\nTop 5 counties con más faltantes:\")\n    county_series = pd.Series(missing_by_county).sort_values(ascending=False)\n    print(county_series.head())\n    \n    print(f\"\\nTimeline:\")\n    print(f\"Última fecha completa: {last_complete}\")\n    print(f\"Primera fecha faltante: {first_missing}\")\n    if last_complete and first_missing:\n        print(f\"Gap de tiempo: {first_missing - last_complete}\")\nelse:\n    print(\"No se encontraron datos de clima faltantes!\")\n\ngc.collect()\n\n## hago drop de los faltantes\n\n# Eliminar filas con datos de clima faltantes\nprint(f\"Dataset original: {len(train5):,} filas\")\n\n# Drop filas con NaN en clima\nweather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\ntrain5 = train5.dropna(subset=weather_key_cols)\n\nprint(f\"Dataset limpio: {len(train5):,} filas\")\nprint(f\"Filas eliminadas: {32_965_048 - len(train5):,}\")\n\n# Verificar que no quedan NaN en clima\nremaining_weather_nan = train5[weather_key_cols].isna().sum().sum()\nprint(f\"NaN restantes en clima: {remaining_weather_nan}\")\n\n# Verificar fechas disponibles para el período de predicción\ntest_period = train5[\n    (train5['datetime'].dt.date >= pd.to_datetime('2023-05-20').date()) &\n    (train5['datetime'].dt.date <= pd.to_datetime('2023-05-31').date())\n]['datetime'].dt.date.unique()\n\nprint(f\"\\nFechas disponibles cerca del período de test:\")\nprint(sorted(test_period))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.528535Z","iopub.status.idle":"2025-08-30T18:28:11.529011Z","shell.execute_reply.started":"2025-08-30T18:28:11.528780Z","shell.execute_reply":"2025-08-30T18:28:11.528798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.531462Z","iopub.status.idle":"2025-08-30T18:28:11.531902Z","shell.execute_reply.started":"2025-08-30T18:28:11.531699Z","shell.execute_reply":"2025-08-30T18:28:11.531718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.539857Z","iopub.status.idle":"2025-08-30T18:28:11.540343Z","shell.execute_reply.started":"2025-08-30T18:28:11.540150Z","shell.execute_reply":"2025-08-30T18:28:11.540173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.542204Z","iopub.status.idle":"2025-08-30T18:28:11.542918Z","shell.execute_reply.started":"2025-08-30T18:28:11.542687Z","shell.execute_reply":"2025-08-30T18:28:11.542705Z"}},"outputs":[],"execution_count":null}]}