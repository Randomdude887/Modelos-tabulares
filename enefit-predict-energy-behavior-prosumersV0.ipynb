{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:24:57.209232Z","iopub.execute_input":"2025-08-30T18:24:57.209622Z","iopub.status.idle":"2025-08-30T18:24:57.698876Z","shell.execute_reply.started":"2025-08-30T18:24:57.209594Z","shell.execute_reply":"2025-08-30T18:24:57.697390Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/public_timeseries_testing_util.py\n/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/county_id_to_name_map.json\n/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/sample_submission.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CORE DATA MANIPULATION\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import pearsonr, spearmanr\nimport math\n\n# MACHINE LEARNING - SCIKIT-LEARN\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, roc_curve, auc\nfrom sklearn.pipeline import Pipeline\n\n# ADVANCED ML LIBRARIES\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# VISUALIZATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\n\n# CONFIGURATION FOR PLOTS\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n\n# DEEP LEARNING \ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n    print(\"TensorFlow disponible\")\nexcept ImportError:\n    print(\"TensorFlow no disponible en este entorno\")\n\n\n# UTILITIES\nimport os\nimport sys\nimport warnings\nimport itertools\nfrom datetime import datetime, timedelta\nimport time\nfrom collections import Counter\nimport pickle\nimport joblib\n\n# JUPYTER SPECIFIC\nfrom IPython.display import display, HTML\nfrom tqdm.notebook import tqdm\n\n# SUPPRESS WARNINGS\nwarnings.filterwarnings('ignore')\n\n# PANDAS CONFIGURATION\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n# NUMPY CONFIGURATION\nnp.random.seed(42)\n\n# PLOTLY CONFIGURATION\npyo.init_notebook_mode(connected=True)\n\nprint(\"âœ… Todas las librerÃ­as importadas correctamente!\")\nprint(f\"ðŸ“Š Pandas version: {pd.__version__}\")\nprint(f\"ðŸ”¢ NumPy version: {np.__version__}\")\nprint(f\"ðŸ¤– Scikit-learn version: {__import__('sklearn').__version__}\")\nprint(f\"ðŸ“ˆ Matplotlib version: {__import__('matplotlib').__version__}\")\nprint(f\"ðŸŽ¨ Seaborn version: {sns.__version__}\")\n\n# ============================================================================\n# FUNCIONES ÃšTILES ADICIONALES\n# ============================================================================\n\ndef quick_info(df):\n    \"\"\"InformaciÃ³n rÃ¡pida del dataset\"\"\"\n    print(f\"ðŸ“Š Dataset Shape: {df.shape}\")\n    print(f\"ðŸ”¢ Columnas numÃ©ricas: {df.select_dtypes(include=[np.number]).columns.tolist()}\")\n    print(f\"ðŸ“ Columnas categÃ³ricas: {df.select_dtypes(include=['object']).columns.tolist()}\")\n    print(f\"âŒ Valores nulos por columna:\")\n    print(df.isnull().sum()[df.isnull().sum() > 0])\n\ndef plot_missing_values(df):\n    \"\"\"Visualizar valores faltantes\"\"\"\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    if len(missing) > 0:\n        plt.figure(figsize=(10, 6))\n        missing.plot(kind='bar')\n        plt.title('Valores Faltantes por Columna')\n        plt.ylabel('Cantidad')\n        plt.xticks(rotation=45)\n        plt.show()\n    else:\n        print(\"âœ… No hay valores faltantes en el dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:24:57.702558Z","iopub.execute_input":"2025-08-30T18:24:57.703643Z","iopub.status.idle":"2025-08-30T18:25:29.957678Z","shell.execute_reply.started":"2025-08-30T18:24:57.703606Z","shell.execute_reply":"2025-08-30T18:25:29.956620Z"}},"outputs":[{"name":"stderr","text":"2025-08-30 18:25:11.464770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756578311.743838      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756578311.836830      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow disponible\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"name":"stdout","text":"âœ… Todas las librerÃ­as importadas correctamente!\nðŸ“Š Pandas version: 2.2.3\nðŸ”¢ NumPy version: 1.26.4\nðŸ¤– Scikit-learn version: 1.2.2\nðŸ“ˆ Matplotlib version: 3.7.2\nðŸŽ¨ Seaborn version: 0.12.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#competencia https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/overview\n\n# Files\n# train.csv\n\n# county - An ID code for the county.\n# is_business - Boolean for whether or not the prosumer is a business.\n# product_type - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n# target - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n# is_consumption - Boolean for whether or not this row's target is consumption or production.\n# datetime - The Estonian time in EET (UTC+2) / EEST (UTC+3). It describes the start of the 1-hour period on which target is given.\n# data_block_id - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n# row_id - A unique identifier for the row.\n# prediction_unit_id - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n# gas_prices.csv\n\n# origin_date - The date when the day-ahead prices became available.\n# forecast_date - The date when the forecast prices should be relevant.\n# [lowest/highest]_price_per_mwh - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n# data_block_id\n# client.csv\n\n# product_type\n# county - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n# eic_count - The aggregated number of consumption points (EICs - European Identifier Code).\n# installed_capacity - Installed photovoltaic solar panel capacity in kilowatts.\n# is_business - Boolean for whether or not the prosumer is a business.\n# date\n# data_block_id\n# electricity_prices.csv\n\n# origin_date\n# forecast_date - Represents the start of the 1-hour period when the price is valid\n# euros_per_mwh - The price of electricity on the day ahead markets in euros per megawatt hour.\n# data_block_id\n# forecast_weather.csv Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n\n# [latitude/longitude] - The coordinates of the weather forecast.\n# origin_datetime - The timestamp of when the forecast was generated.\n# hours_ahead - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n# temperature - The air temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# dewpoint - The dew point temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# cloudcover_[low/mid/high/total] - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total. Estimated for the end of the 1-hour period.\n# 10_metre_[u/v]_wind_component - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second. Estimated for the end of the 1-hour period.\n# data_block_id\n# forecast_datetime - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead. This represents the start of the 1-hour period for which weather data are forecasted.\n# direct_solar_radiation - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the hour, in watt-hours per square meter.\n# surface_solar_radiation_downwards - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, accumulated during the hour, in watt-hours per square meter.\n# snowfall - Snowfall over hour in units of meters of water equivalent.\n# total_precipitation - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the described hour, in units of meters.\n# historical_weather.csv Historic weather data.\n\n# datetime - This represents the start of the 1-hour period for which weather data are measured.\n# temperature - Measured at the end of the 1-hour period.\n# dewpoint - Measured at the end of the 1-hour period.\n# rain - Different from the forecast conventions. The rain from large scale weather systems of the hour in millimeters.\n# snowfall - Different from the forecast conventions. Snowfall over the hour in centimeters.\n# surface_pressure - The air pressure at surface in hectopascals.\n# cloudcover_[low/mid/high/total] - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n# windspeed_10m - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n# winddirection_10m - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n# shortwave_radiation - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n# direct_solar_radiation\n# diffuse_radiation - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n# [latitude/longitude] - The coordinates of the weather station.\n# data_block_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:29.958869Z","iopub.execute_input":"2025-08-30T18:25:29.959280Z","iopub.status.idle":"2025-08-30T18:25:29.966517Z","shell.execute_reply.started":"2025-08-30T18:25:29.959248Z","shell.execute_reply":"2025-08-30T18:25:29.965471Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#modelos a correr: NN por ser forecasting y algun xgb o lgb\n#son 4 dias a predecir el comportamiento de los prosumers\n#predecir con rolling de a 1 dia\n\n\n#analizar y preparar\n# como siempre primero ver los faltantes y los NaN y en cada caso decidir que hacer\n# atento con valores expresados para el dia actual o la hora actual / la hora anterior / el dia de maÃ±ana , ver de no malinterpretar\n# ver la correlacion de precios sea gas y/o electricidad para ver como se comparta con business y no business\n# lo mismo con el clima para business y no business\n# ver si aplica el supuesto de que los business si operan bajo esos valores (costos de electricidad/gas) y los no business no es tan relevante\n# clima historical vs forecast , adaptar columnas que usan distintas proporciones , medir la certeza del forecast\n# analizar por fuera como funcionan los paneles y como miden lo que miden ( para mejor entendimiento del df)\n\n#columnas a agregar\n#Datetime dias lu-1/ma-2/mi-3/ju-4/vi-5/sa-6/do-7 \n#Date time horas y capaz minutos\n#horario laboral ejemplo 7 a 17hs (analizarlo) sea en el dataset y por fuera\n#horario business , para los que son business ver un pseudo horario de apertura y cierre como afecta y si se puede\n#Dia 6hs a 18hs / noche de 18hs a 6hs ver si lo adapto con 4 columnas por las estaciones\n#hora de dormir ejemplo 22hs a 6hs\n#dias festivos y vacaciones , columnas binarias en ambos\n#estaciones del aÃ±o 4 columnas binarias\n#periodogram para ver los lags y sea mensuales/quincenales/diarios/hora\n#ver si es relevante agregar alguna columna con la poblacion mensual y usar un ratio (info a buscar afuera)\n\n#sugerencias de features\n\n##EnergÃ©ticas:\n\n# Ratio production/consumption por prediction_unit_id (histÃ³rico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n##Temporales:\n\n# Lag de consumo/producciÃ³n del mismo prediction_unit_id (24h, 48h, 168h)\n# Media mÃ³vil de target por prediction_unit_id\n# Cambios dÃ­a a dÃ­a (delta vs dÃ­a anterior)\n\n##Weather engineering:\n\n# Ãndice de confort tÃ©rmico (combinando temp + humidity)\n# Potencial solar real (direct + diffuse radiation ajustado por cloudcover)\n# Diferencia forecast vs historical weather (para medir accuracy del forecast)\n\n##SegmentaciÃ³n:\n\n# Clustering de prediction_units por comportamiento similar\n# Ratios county-level (agregaciones por county vs individual)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:29.968895Z","iopub.execute_input":"2025-08-30T18:25:29.969298Z","iopub.status.idle":"2025-08-30T18:25:30.005402Z","shell.execute_reply.started":"2025-08-30T18:25:29.969270Z","shell.execute_reply":"2025-08-30T18:25:30.004071Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#county_id_to_name_map.json\n# {\n#   \"0\": \"HARJUMAA\",\n#   \"1\": \"HIIUMAA\",\n#   \"2\": \"IDA-VIRUMAA\",\n#   \"3\": \"JÃ„RVAMAA\",\n#   \"4\": \"JÃ•GEVAMAA\",\n#   \"5\": \"LÃ„Ã„NE-VIRUMAA\",\n#   \"6\": \"LÃ„Ã„NEMAA\",\n#   \"7\": \"PÃ„RNUMAA\",\n#   \"8\": \"PÃ•LVAMAA\",\n#   \"9\": \"RAPLAMAA\",\n#   \"10\": \"SAAREMAA\",\n#   \"11\": \"TARTUMAA\",\n#   \"12\": \"UNKNOWN\",\n#   \"13\": \"VALGAMAA\",\n#   \"14\": \"VILJANDIMAA\",\n#   \"15\": \"VÃ•RUMAA\"\n# }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:30.006725Z","iopub.execute_input":"2025-08-30T18:25:30.007129Z","iopub.status.idle":"2025-08-30T18:25:30.038668Z","shell.execute_reply.started":"2025-08-30T18:25:30.007104Z","shell.execute_reply":"2025-08-30T18:25:30.037474Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train                 = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\nclient                = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\nelectricity_prices    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\nforecast_weather      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\ngas_prices            = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\nhistorical_weather    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\nweather_station       = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv')\n\ntest                  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv')\nclient_t              = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv')\nelectricity_prices_t  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv')\nforecast_weather_t    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv')\ngas_prices_t          = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv')\nrevealed_targets      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:30.039653Z","iopub.execute_input":"2025-08-30T18:25:30.039986Z","iopub.status.idle":"2025-08-30T18:25:55.471283Z","shell.execute_reply.started":"2025-08-30T18:25:30.039963Z","shell.execute_reply":"2025-08-30T18:25:55.470301Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# pruebo pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:55.472737Z","iopub.execute_input":"2025-08-30T18:25:55.473151Z","iopub.status.idle":"2025-08-30T18:25:55.478610Z","shell.execute_reply.started":"2025-08-30T18:25:55.473119Z","shell.execute_reply":"2025-08-30T18:25:55.477431Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport unicodedata\nfrom sklearn.neighbors import NearestNeighbors\nimport gc\nfrom collections import defaultdict\nfrom pathlib import Path\n\nclass DataPreprocessingPipeline:\n    \"\"\"Pipeline modular para preprocessing de datos de train y test\"\"\"\n    \n    def __init__(self):\n        self.county_id_to_name_map = {\n            0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"JÃ„RVAMAA\",\n            4: \"JÃ•GEVAMAA\", 5: \"LÃ„Ã„NE-VIRUMAA\", 6: \"LÃ„Ã„NEMAA\", 7: \"PÃ„RNUMAA\",\n            8: \"PÃ•LVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n            12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"VÃ•RUMAA\"\n        }\n        self.county_name_to_id_map = {v: k for k, v in self.county_id_to_name_map.items()}\n        \n        # Diccionario de sinÃ³nimos para normalizaciÃ³n\n        self.synonyms_to_canonical = {\n            \"HARJU\": \"HARJUMAA\", \"HIIU\": \"HIIUMAA\", \"IDA-VIRU\": \"IDA-VIRUMAA\",\n            \"JARVA\": \"JÃ„RVAMAA\", \"JÃ„RVA\": \"JÃ„RVAMAA\", \"JOGEVA\": \"JÃ•GEVAMAA\", \n            \"JÃ•GEVA\": \"JÃ•GEVAMAA\", \"LAANE-VIRU\": \"LÃ„Ã„NE-VIRUMAA\", \n            \"LÃ„Ã„NE-VIRU\": \"LÃ„Ã„NE-VIRUMAA\", \"LAANE\": \"LÃ„Ã„NEMAA\", \"LÃ„Ã„NE\": \"LÃ„Ã„NEMAA\",\n            \"PARNU\": \"PÃ„RNUMAA\", \"PÃ„RNU\": \"PÃ„RNUMAA\", \"POLVA\": \"PÃ•LVAMAA\", \n            \"PÃ•LVA\": \"PÃ•LVAMAA\", \"RAPLA\": \"RAPLAMAA\", \"SAARE\": \"SAAREMAA\",\n            \"TARTU\": \"TARTUMAA\", \"VALGA\": \"VALGAMAA\", \"VILJANDI\": \"VILJANDIMAA\",\n            \"VORU\": \"VÃ•RUMAA\", \"VÃ•RU\": \"VÃ•RUMAA\",\n            # versiones ya canÃ³nicas\n            **{name: name for name in self.county_id_to_name_map.values()}\n        }\n    \n    def strip_accents(self, s: str) -> str:\n        \"\"\"Eliminar acentos de string\"\"\"\n        return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n    \n    def normalize_county_name(self, x):\n        \"\"\"Normalizar nombre de county\"\"\"\n        if pd.isna(x):\n            return np.nan\n        s = str(x).upper().strip()\n        # eliminar sufijos frecuentes\n        for suf in [\" COUNTY\", \" MAAKOND\"]:\n            if s.endswith(suf):\n                s = s[: -len(suf)]\n        s = s.replace(\"  \", \" \").replace(\"â€“\", \"-\")\n        s_noacc = self.strip_accents(s)\n        s_noacc = s_noacc.replace(\"  \", \" \")\n        \n        token = s_noacc.split()[0]\n        if \"-\" in s_noacc:\n            token = s_noacc\n        \n        # intentos de match\n        if s_noacc in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s_noacc]\n        if token in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[token]\n        if s in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s]\n        \n        return np.nan\n    \n    def process_weather_stations(self, weather_station):\n        \"\"\"Procesar estaciones meteorolÃ³gicas para asignar counties\"\"\"\n        weather_station = weather_station.copy()\n        \n        # Normalizar nombres de county\n        weather_station['county_name_norm'] = weather_station['county_name'].apply(self.normalize_county_name)\n        \n        # Si tengo 'county' numÃ©rico pero falta nombre, lo inferimos del ID\n        mask = weather_station['county_name_norm'].isna() & weather_station['county'].notna()\n        weather_station.loc[mask, 'county_name_norm'] = (\n            weather_station.loc[mask, 'county'].astype(int).map(self.county_id_to_name_map)\n        )\n        \n        # Completar faltantes por k-NN\n        fcols = ['latitude', 'longitude']\n        known = weather_station.dropna(subset=['county_name_norm']).copy()\n        unknown = weather_station[weather_station['county_name_norm'].isna()].copy()\n        \n        if not unknown.empty and not known.empty:\n            known_rad = np.radians(known[fcols].values)\n            unknown_rad = np.radians(unknown[fcols].values)\n            \n            nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n            nbrs.fit(known_rad)\n            distances, idxs = nbrs.kneighbors(unknown_rad)\n            \n            nn_names = known['county_name_norm'].values\n            filled = []\n            for neigh_idx in idxs:\n                cands = nn_names[neigh_idx]\n                vals, counts = np.unique(cands, return_counts=True)\n                filled.append(vals[np.argmax(counts)])\n            \n            weather_station.loc[unknown.index, 'county_name_norm'] = filled\n        \n        # Sincronizar columnas finales\n        weather_station['county_name'] = weather_station['county_name_norm']\n        weather_station['county'] = weather_station['county_name'].map(self.county_name_to_id_map).astype('Int64')\n        \n        # Cualquier remanente a UNKNOWN (12)\n        weather_station.loc[weather_station['county_name'].isna(), 'county'] = 12\n        weather_station['county'] = weather_station['county'].astype('Int64')\n        \n        weather_station.drop(columns=['county_name_norm'], inplace=True)\n        \n        return weather_station\n    \n    def optimize_dtypes(self, df, is_train=True):\n        \"\"\"Optimizar tipos de datos para reducir memoria\"\"\"\n        df = df.copy()\n        \n        # Tipos comunes\n        if 'county' in df.columns:\n            df['county'] = df['county'].astype('uint8')\n        if 'is_business' in df.columns:\n            df['is_business'] = df['is_business'].astype('uint8')\n        if 'product_type' in df.columns:\n            df['product_type'] = df['product_type'].astype('uint8')\n        if 'is_consumption' in df.columns:\n            df['is_consumption'] = df['is_consumption'].astype('uint8')\n        if 'hour' in df.columns:\n            df['hour'] = df['hour'].astype('uint8')\n        \n        # IDs\n        if 'data_block_id' in df.columns:\n            df['data_block_id'] = df['data_block_id'].astype('uint16')\n        if 'row_id' in df.columns:\n            df['row_id'] = df['row_id'].astype('uint32')\n        if 'prediction_unit_id' in df.columns:\n            df['prediction_unit_id'] = df['prediction_unit_id'].astype('uint8')\n        \n        # Coordenadas y variables meteorolÃ³gicas - float32\n        float_cols = ['latitude', 'longitude', 'target', 'lowest_price_per_mwh', \n                     'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', \n                     'installed_capacity']\n        \n        # Variables meteorolÃ³gicas\n        weather_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                       'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                       'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                       'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                       'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                       'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                       'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                       'direct_solar_radiation', 'diffuse_radiation']\n        \n        # Aplicar float32 a columnas que existen\n        for col in float_cols + weather_cols:\n            if col in df.columns:\n                df[col] = df[col].astype('float32')\n        \n        # Weather forecast hour\n        if 'weather_forecast_hour' in df.columns:\n            df['weather_forecast_hour'] = df['weather_forecast_hour'].astype('uint8')\n        \n        return df\n\n\nclass TrainPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline especÃ­fico para datos de entrenamiento\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.temp_dir = Path(\"temp_chunks\")\n        self.temp_dir.mkdir(exist_ok=True)\n    \n    def part1_prepare_base_merges(self, train, gas_prices, electricity_prices, client):\n        \"\"\"Parte 1: Preparar datos base y hacer primeros merges\"\"\"\n        print(\"=== PARTE 1: PreparaciÃ³n y merges base ===\")\n        \n        # Preparar copias\n        train1 = train.dropna().copy()\n        gas_prices1 = gas_prices.copy()\n        electricity_prices1 = electricity_prices.copy()\n        client1 = client.copy()\n        \n        # Procesar train\n        train1['datetime'] = pd.to_datetime(train1['datetime'])\n        train1['hour'] = train1['datetime'].dt.hour\n        train1['forecast_date'] = train1['datetime']\n        \n        # Merge 1: Gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        gas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n        \n        # Expandir gas a nivel horario\n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices1_hourly = pd.DataFrame(gas_hourly)\n        train2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')\n        \n        # Forward fill gas prices\n        train2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\n        train2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge 2: Electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        elec_columns = ['forecast_date', 'euros_per_mwh']\n        train3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')\n        train3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge 3: Client data\n        print(\"Procesando client data...\")\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        train4 = train3.merge(client1.drop('data_block_id', axis=1),\n                             left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                             right_on=['product_type', 'county', 'is_business', 'date'],\n                             how='left')\n        \n        train4 = train4.drop('date', axis=1)\n        train4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        print(\"Completando datos de clientes...\")\n        train4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        train4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        train4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        train4['eic_count'] = train4['eic_count'].fillna(train4.groupby('county')['eic_count'].transform('mean'))\n        train4['installed_capacity'] = train4['installed_capacity'].fillna(train4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Optimizar tipos\n        train4 = self.optimize_dtypes(train4, is_train=True)\n        \n        print(f\"Parte 1 completada. Dataset: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")\n        return train4\n    \n    def part2_prepare_weather_merge(self, train4, forecast_weather, historical_weather, weather_station):\n        \"\"\"Parte 2: Preparar datos meteorolÃ³gicos y hacer merge\"\"\"\n        print(\"=== PARTE 2: PreparaciÃ³n datos meteorolÃ³gicos ===\")\n        \n        # Procesar weather stations\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        forecast_weather1 = forecast_weather.dropna().copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Merge con historical weather\n        print(\"Mergeando forecast con historical weather...\")\n        historical_weather1 = historical_weather.copy()\n        historical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n        \n        forecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n        \n        merged_weather = pd.merge(\n            forecast_weather2,\n            historical_weather1,\n            left_on=['latitude', 'longitude', 'forecast_datetime'],\n            right_on=['latitude', 'longitude', 'datetime'],\n            how='inner'\n        )\n        \n        # Limpiar y optimizar\n        merged_weather = merged_weather.drop(['f_data_block_id', 'forecast_datetime'], axis=1)\n        merged_weather = self.optimize_dtypes(merged_weather, is_train=True)\n        \n        print(f\"Weather data preparado: {merged_weather.shape[0]:,} filas, {merged_weather.shape[1]} columnas\")\n        return train4, merged_weather\n    \n    def part3_final_merge_and_cleanup(self, train4, merged_weather, chunk_size=1_000_000):\n        \"\"\"Parte 3: Merge final y limpieza\"\"\"\n        print(\"=== PARTE 3: Merge final y limpieza ===\")\n        \n        # Definir perÃ­odos para procesar por chunks\n        periods = [\n            ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), \n            ('2021-12-01', '2022-01-01'), ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), \n            ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'), ('2022-05-01', '2022-06-01'), \n            ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n            ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), \n            ('2022-12-01', '2023-01-01'), ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), \n            ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'), ('2023-05-01', '2023-05-31')\n        ]\n        \n        # Procesar por chunks temporales\n        chunk_files = []\n        for i, (start_date, end_date) in enumerate(periods):\n            print(f\"Procesando perÃ­odo {i+1}/{len(periods)}: {start_date} a {end_date}\")\n            \n            # Filtrar por perÃ­odo\n            mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n            train4_chunk = train4[mask_train].copy()\n            \n            mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n            weather_chunk = merged_weather[mask_weather].copy()\n            \n            if len(train4_chunk) > 0 and len(weather_chunk) > 0:\n                # Merge chunk\n                chunk_result = pd.merge(train4_chunk, weather_chunk, \n                               on=['county', 'datetime'], \n                               how='left')\n                \n                # Guardar chunk temporal\n                chunk_file = self.temp_dir / f'train5_chunk_{i+1}.parquet'\n                chunk_result.to_parquet(chunk_file, index=False)\n                chunk_files.append(chunk_file)\n                \n                print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n                \n                # Liberar memoria\n                del train4_chunk, weather_chunk, chunk_result\n                gc.collect()\n        \n        # Concatenar chunks\n        print(\"Concatenando chunks...\")\n        final_chunks = []\n        for chunk_file in chunk_files:\n            chunk_data = pd.read_parquet(chunk_file)\n            final_chunks.append(chunk_data)\n        \n        train5 = pd.concat(final_chunks, ignore_index=True)\n        \n        # Limpieza final\n        print(\"Aplicando limpieza final...\")\n        train5 = self._cleanup_merged_data(train5, chunk_size)\n        \n        # Limpiar archivos temporales\n        for chunk_file in chunk_files:\n            chunk_file.unlink()\n        \n        return train5\n    \n    def _cleanup_merged_data(self, train5, chunk_size=1_000_000):\n        \"\"\"Limpiar datos despuÃ©s del merge\"\"\"\n        # Identificar columnas a eliminar y renombrar\n        sample = train5.head(1000)\n        cols_to_drop = [col for col in sample.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\n        \n        print(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n        \n        # Procesar por chunks\n        chunks_processed = []\n        total_rows = len(train5)\n        n_chunks = (total_rows // chunk_size) + 1\n        \n        for i in range(n_chunks):\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, total_rows)\n            \n            print(f\"Procesando chunk limpieza {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n            \n            chunk = train5.iloc[start_idx:end_idx].copy()\n            \n            # Filtrar county != 12\n            chunk = chunk[chunk['county'] != 12]\n            \n            if len(chunk) > 0:\n                # Eliminar columnas _y y renombrar _x\n                chunk = chunk.drop(columns=cols_to_drop)\n                chunk.rename(columns=rename_dict, inplace=True)\n                chunks_processed.append(chunk)\n            \n            del chunk\n            gc.collect()\n        \n        # Concatenar final\n        train5_clean = pd.concat(chunks_processed, ignore_index=True)\n        del chunks_processed\n        gc.collect()\n        \n        # Eliminar datos de clima faltantes\n        weather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n        initial_rows = len(train5_clean)\n        train5_clean = train5_clean.dropna(subset=weather_key_cols)\n        print(f\"Filas eliminadas por clima faltante: {initial_rows - len(train5_clean):,}\")\n        \n        # Reordenar columnas\n        train5_clean = self._reorder_columns(train5_clean)\n        \n        return train5_clean\n    \n    def _reorder_columns(self, df):\n        \"\"\"Reordenar columnas en el orden especificado\"\"\"\n        main_cols = [\n            'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = main_cols.copy()\n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n    \n    def run_full_pipeline(self, train, gas_prices, electricity_prices, client, \n                         forecast_weather, historical_weather, weather_station):\n        \"\"\"Ejecutar pipeline completo de entrenamiento\"\"\"\n        print(\"Iniciando pipeline completo de entrenamiento...\")\n        \n        # Parte 1: Merges base\n        train4 = self.part1_prepare_base_merges(train, gas_prices, electricity_prices, client)\n        \n        # Parte 2: Preparar datos meteorolÃ³gicos\n        train4, merged_weather = self.part2_prepare_weather_merge(\n            train4, forecast_weather, historical_weather, weather_station\n        )\n        \n        # Parte 3: Merge final y limpieza\n        train5 = self.part3_final_merge_and_cleanup(train4, merged_weather)\n        \n        print(f\"Pipeline completado! Dataset final: {train5.shape[0]:,} filas, {train5.shape[1]} columnas\")\n        return train5\n\n\nclass TestPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline simplificado para datos de test\"\"\"\n    \n    def run_test_pipeline(self, test, gas_prices_t, electricity_prices_t, client_t, \n                         forecast_weather_t, weather_station):\n        \"\"\"Pipeline completo para test set (mÃ¡s simple, sin historical weather)\"\"\"\n        print(\"=== PIPELINE TEST ===\")\n        \n        # Preparar datos base\n        test1 = test.copy()\n        test1['datetime'] = pd.to_datetime(test1['datetime'])\n        test1['hour'] = test1['datetime'].dt.hour\n        test1['forecast_date'] = test1['datetime']\n        \n        # Merge gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1 = gas_prices_t.copy()\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        \n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices_hourly = pd.DataFrame(gas_hourly)\n        test2 = pd.merge(test1, gas_prices_hourly, on='forecast_date', how='left')\n        test2['lowest_price_per_mwh'] = test2['lowest_price_per_mwh'].fillna(method='ffill')\n        test2['highest_price_per_mwh'] = test2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1 = electricity_prices_t.copy()\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        test3 = pd.merge(test2, electricity_prices1[['forecast_date', 'euros_per_mwh']], \n                        on='forecast_date', how='left')\n        test3['euros_per_mwh'] = test3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge client data\n        print(\"Procesando client data...\")\n        client1 = client_t.copy()\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        test4 = test3.merge(client1.drop('data_block_id', axis=1),\n                           left_on=['product_type', 'county', 'is_business', test3['datetime'].dt.date],\n                           right_on=['product_type', 'county', 'is_business', 'date'],\n                           how='left')\n        \n        test4 = test4.drop('date', axis=1)\n        test4 = test4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        test4 = test4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        test4['eic_count'] = test4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        test4['installed_capacity'] = test4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        test4['eic_count'] = test4['eic_count'].fillna(test4.groupby('county')['eic_count'].transform('mean'))\n        test4['installed_capacity'] = test4['installed_capacity'].fillna(test4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        forecast_weather1 = forecast_weather_t.copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Para test, no tenemos historical weather, asÃ­ que creamos columnas dummy o usamos solo forecast\n        # OpciÃ³n 1: Solo usar forecast weather (renombrar las f columns)\n        # OpciÃ³n 2: Crear columnas histÃ³ricas como NaN y llenar despuÃ©s\n        \n        # Vamos con opciÃ³n 1: usar forecast como histÃ³rico tambiÃ©n (aproximaciÃ³n)\n        historical_cols_mapping = {\n            'ftemperature': 'temperature',\n            'fdewpoint': 'dewpoint', \n            'fcloudcover_high': 'cloudcover_high',\n            'fcloudcover_low': 'cloudcover_low',\n            'fcloudcover_mid': 'cloudcover_mid', \n            'fcloudcover_total': 'cloudcover_total',\n            'fdirect_solar_radiation': 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards': 'shortwave_radiation',\n            'fsnowfall': 'snowfall',\n            'ftotal_precipitation': 'rain'\n        }\n        \n        # Crear versiones histÃ³ricas basadas en forecast\n        for fcol, hcol in historical_cols_mapping.items():\n            if fcol in forecast_weather2.columns:\n                forecast_weather2[hcol] = forecast_weather2[fcol]\n        \n        # Agregar columnas que solo existen en historical\n        forecast_weather2['surface_pressure'] = 1013.25  # valor tÃ­pico\n        forecast_weather2['windspeed_10m'] = np.sqrt(\n            forecast_weather2['f10_metre_u_wind_component']**2 + \n            forecast_weather2['f10_metre_v_wind_component']**2\n        ) if 'f10_metre_u_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['winddirection_10m'] = np.arctan2(\n            forecast_weather2['f10_metre_v_wind_component'], \n            forecast_weather2['f10_metre_u_wind_component']\n        ) * 180 / np.pi if 'f10_metre_v_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['diffuse_radiation'] = forecast_weather2.get('fsurface_solar_radiation_downwards', 0) - forecast_weather2.get('fdirect_solar_radiation', 0)\n        forecast_weather2['diffuse_radiation'] = forecast_weather2['diffuse_radiation'].clip(lower=0)\n        \n        # Limpiar y optimizar\n        forecast_weather2 = forecast_weather2.drop(columns=['data_block_id'], errors='ignore')\n        forecast_weather2 = self.optimize_dtypes(forecast_weather2, is_train=False)\n        \n        # Merge final con weather\n        print(\"Merge final con datos meteorolÃ³gicos...\")\n        test5 = pd.merge(test4, forecast_weather2, \n                        left_on=['datetime', 'county'], \n                        right_on=['forecast_datetime', 'county'], \n                        how='left')\n        \n        # Limpiar columnas duplicadas y renombrar\n        cols_to_drop = [col for col in test5.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in test5.columns if col.endswith('_x')}\n        \n        test5 = test5.drop(columns=cols_to_drop + ['forecast_datetime'], errors='ignore')\n        test5.rename(columns=rename_dict, inplace=True)\n        \n        # Optimizar tipos finales\n        test5 = self.optimize_dtypes(test5, is_train=False)\n        \n        # Reordenar columnas igual que train\n        test5 = self._reorder_columns_test(test5)\n        \n        print(f\"Pipeline test completado! Dataset: {test5.shape[0]:,} filas, {test5.shape[1]} columnas\")\n        return test5\n    \n    def _reorder_columns_test(self, df):\n        \"\"\"Reordenar columnas para test (mismo orden que train)\"\"\"\n        main_cols = [\n            'row_id', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = []\n        # Solo agregar columnas que existen\n        for col in main_cols:\n            if col in df.columns:\n                final_order.append(col)\n        \n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n\n\n# Funciones de utilidad para usar el pipeline\ndef run_train_pipeline(train, gas_prices, electricity_prices, client, \n                      forecast_weather, historical_weather, weather_station):\n    \"\"\"\n    FunciÃ³n principal para ejecutar pipeline de entrenamiento\n    \n    Returns:\n        pd.DataFrame: Dataset procesado train5\n    \"\"\"\n    pipeline = TrainPipeline()\n    return pipeline.run_full_pipeline(\n        train, gas_prices, electricity_prices, client, \n        forecast_weather, historical_weather, weather_station\n    )\n\ndef run_test_pipeline(test, gas_prices_t, electricity_prices_t, client_t, \n                     forecast_weather_t, weather_station):\n    \"\"\"\n    FunciÃ³n principal para ejecutar pipeline de test\n    \n    Returns:\n        pd.DataFrame: Dataset procesado test5\n    \"\"\"\n    pipeline = TestPipeline()\n    return pipeline.run_test_pipeline(\n        test, gas_prices_t, electricity_prices_t, client_t, \n        forecast_weather_t, weather_station\n    )\n\n# Ejemplo de uso:\n\"\"\"\n# Para train:\ntrain5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)\n\n# Para test:\ntest5 = run_test_pipeline(\n    test, gas_prices_t, electricity_prices_t, client_t, \n    forecast_weather_t, weather_station\n)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:55.480440Z","iopub.execute_input":"2025-08-30T18:25:55.480803Z","iopub.status.idle":"2025-08-30T18:25:55.603577Z","shell.execute_reply.started":"2025-08-30T18:25:55.480778Z","shell.execute_reply":"2025-08-30T18:25:55.601950Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n# Para train:\\ntrain5 = run_train_pipeline(\\n    train, gas_prices, electricity_prices, client, \\n    forecast_weather, historical_weather, weather_station\\n)\\n\\n# Para test:\\ntest5 = run_test_pipeline(\\n    test, gas_prices_t, electricity_prices_t, client_t, \\n    forecast_weather_t, weather_station\\n)\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:25:55.605608Z","iopub.execute_input":"2025-08-30T18:25:55.606146Z","iopub.status.idle":"2025-08-30T18:28:10.797356Z","shell.execute_reply.started":"2025-08-30T18:25:55.606113Z","shell.execute_reply":"2025-08-30T18:28:10.796217Z"}},"outputs":[{"name":"stdout","text":"Iniciando pipeline completo de entrenamiento...\n=== PARTE 1: PreparaciÃ³n y merges base ===\nProcesando gas prices...\nProcesando electricity prices...\nProcesando client data...\nCompletando datos de clientes...\nParte 1 completada. Dataset: 2,017,824 filas, 16 columnas\n=== PARTE 2: PreparaciÃ³n datos meteorolÃ³gicos ===\nProcesando forecast weather...\nCompletando 2140318 counties faltantes con k-NN...\nMergeando forecast con historical weather...\nWeather data preparado: 3,418,242 filas, 32 columnas\n=== PARTE 3: Merge final y limpieza ===\nProcesando perÃ­odo 1/21: 2021-09-01 a 2021-10-01\nChunk 1 guardado: (1423080, 46)\nProcesando perÃ­odo 2/21: 2021-10-01 a 2021-11-01\nChunk 2 guardado: (1532066, 46)\nProcesando perÃ­odo 3/21: 2021-11-01 a 2021-12-01\nChunk 3 guardado: (1484640, 46)\nProcesando perÃ­odo 4/21: 2021-12-01 a 2022-01-01\nChunk 4 guardado: (1543056, 46)\nProcesando perÃ­odo 5/21: 2022-01-01 a 2022-02-01\nChunk 5 guardado: (1590672, 46)\nProcesando perÃ­odo 6/21: 2022-02-01 a 2022-03-01\nChunk 6 guardado: (1441632, 46)\nProcesando perÃ­odo 7/21: 2022-03-01 a 2022-04-01\nChunk 7 guardado: (1597678, 46)\nProcesando perÃ­odo 8/21: 2022-04-01 a 2022-05-01\nChunk 8 guardado: (1630080, 46)\nProcesando perÃ­odo 9/21: 2022-05-01 a 2022-06-01\nChunk 9 guardado: (1644240, 46)\nProcesando perÃ­odo 10/21: 2022-06-01 a 2022-07-01\nChunk 10 guardado: (1573920, 46)\nProcesando perÃ­odo 11/21: 2022-07-01 a 2022-08-01\nChunk 11 guardado: (1581744, 46)\nProcesando perÃ­odo 12/21: 2022-08-01 a 2022-09-01\nChunk 12 guardado: (1581716, 46)\nProcesando perÃ­odo 13/21: 2022-09-01 a 2022-10-01\nChunk 13 guardado: (1573920, 46)\nProcesando perÃ­odo 14/21: 2022-10-01 a 2022-11-01\nChunk 14 guardado: (1686610, 46)\nProcesando perÃ­odo 15/21: 2022-11-01 a 2022-12-01\nChunk 15 guardado: (1643084, 46)\nProcesando perÃ­odo 16/21: 2022-12-01 a 2023-01-01\nChunk 16 guardado: (1669008, 46)\nProcesando perÃ­odo 17/21: 2023-01-01 a 2023-02-01\nChunk 17 guardado: (1654608, 46)\nProcesando perÃ­odo 18/21: 2023-02-01 a 2023-03-01\nChunk 18 guardado: (1494048, 46)\nProcesando perÃ­odo 19/21: 2023-03-01 a 2023-04-01\nChunk 19 guardado: (1604926, 46)\nProcesando perÃ­odo 20/21: 2023-04-01 a 2023-05-01\nChunk 20 guardado: (1514016, 46)\nProcesando perÃ­odo 21/21: 2023-05-01 a 2023-05-31\nChunk 21 guardado: (1530872, 46)\nConcatenando chunks...\nAplicando limpieza final...\nEliminando 1 columnas, renombrando 1\nProcesando chunk limpieza 1/33: filas 0 a 1,000,000\nProcesando chunk limpieza 2/33: filas 1,000,000 a 2,000,000\nProcesando chunk limpieza 3/33: filas 2,000,000 a 3,000,000\nProcesando chunk limpieza 4/33: filas 3,000,000 a 4,000,000\nProcesando chunk limpieza 5/33: filas 4,000,000 a 5,000,000\nProcesando chunk limpieza 6/33: filas 5,000,000 a 6,000,000\nProcesando chunk limpieza 7/33: filas 6,000,000 a 7,000,000\nProcesando chunk limpieza 8/33: filas 7,000,000 a 8,000,000\nProcesando chunk limpieza 9/33: filas 8,000,000 a 9,000,000\nProcesando chunk limpieza 10/33: filas 9,000,000 a 10,000,000\nProcesando chunk limpieza 11/33: filas 10,000,000 a 11,000,000\nProcesando chunk limpieza 12/33: filas 11,000,000 a 12,000,000\nProcesando chunk limpieza 13/33: filas 12,000,000 a 13,000,000\nProcesando chunk limpieza 14/33: filas 13,000,000 a 14,000,000\nProcesando chunk limpieza 15/33: filas 14,000,000 a 15,000,000\nProcesando chunk limpieza 16/33: filas 15,000,000 a 16,000,000\nProcesando chunk limpieza 17/33: filas 16,000,000 a 17,000,000\nProcesando chunk limpieza 18/33: filas 17,000,000 a 18,000,000\nProcesando chunk limpieza 19/33: filas 18,000,000 a 19,000,000\nProcesando chunk limpieza 20/33: filas 19,000,000 a 20,000,000\nProcesando chunk limpieza 21/33: filas 20,000,000 a 21,000,000\nProcesando chunk limpieza 22/33: filas 21,000,000 a 22,000,000\nProcesando chunk limpieza 23/33: filas 22,000,000 a 23,000,000\nProcesando chunk limpieza 24/33: filas 23,000,000 a 24,000,000\nProcesando chunk limpieza 25/33: filas 24,000,000 a 25,000,000\nProcesando chunk limpieza 26/33: filas 25,000,000 a 26,000,000\nProcesando chunk limpieza 27/33: filas 26,000,000 a 27,000,000\nProcesando chunk limpieza 28/33: filas 27,000,000 a 28,000,000\nProcesando chunk limpieza 29/33: filas 28,000,000 a 29,000,000\nProcesando chunk limpieza 30/33: filas 29,000,000 a 30,000,000\nProcesando chunk limpieza 31/33: filas 30,000,000 a 31,000,000\nProcesando chunk limpieza 32/33: filas 31,000,000 a 32,000,000\nProcesando chunk limpieza 33/33: filas 32,000,000 a 32,995,616\nFilas eliminadas por clima faltante: 2,024\nPipeline completado! Dataset final: 32,963,024 filas, 45 columnas\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:10.802414Z","iopub.execute_input":"2025-08-30T18:28:10.803007Z","iopub.status.idle":"2025-08-30T18:28:10.862130Z","shell.execute_reply.started":"2025-08-30T18:28:10.802973Z","shell.execute_reply":"2025-08-30T18:28:10.860551Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n6      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n7      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n8      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n9      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n10     366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n\n    data_block_id  prediction_unit_id  is_business  product_type  county  \\\n6               0                   0            0             1       0   \n7               0                   0            0             1       0   \n8               0                   0            0             1       0   \n9               0                   0            0             1       0   \n10              0                   0            0             1       0   \n\n     latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n6   59.099998  24.200001                 45.23                  46.32   \n7   59.099998  25.200001                 45.23                  46.32   \n8   59.400002  22.700001                 45.23                  46.32   \n9   59.400002  23.200001                 45.23                  46.32   \n10  59.400002  23.700001                 45.23                  46.32   \n\n    euros_per_mwh  eic_count  installed_capacity  ftemperature  temperature  \\\n6       86.879997      108.0          952.890015     12.681543         12.4   \n7       86.879997      108.0          952.890015     12.868921         12.3   \n8       86.879997      108.0          952.890015     15.041773         15.2   \n9       86.879997      108.0          952.890015     14.632105         14.9   \n10      86.879997      108.0          952.890015     14.480005         12.8   \n\n    fdewpoint  dewpoint  fcloudcover_high  cloudcover_high  fcloudcover_low  \\\n6    9.783228       9.7          0.023590             11.0         0.002380   \n7    9.498316       9.6          0.431854              6.0         0.211182   \n8   11.860376      11.8          0.134674             18.0         0.202515   \n9   11.773584      11.5          0.255188             19.0         0.036774   \n10  11.581568      10.4          0.338074             12.0         0.037109   \n\n    cloudcover_low  fcloudcover_mid  cloudcover_mid  fcloudcover_total  \\\n6             10.0         0.001251             0.0           0.026398   \n7             23.0         0.006790             1.0           0.548508   \n8              7.0         0.003906             0.0           0.308930   \n9              6.0         0.026245             0.0           0.286194   \n10             4.0         0.016510             1.0           0.368134   \n\n    cloudcover_total  f10_metre_u_wind_component  windspeed_10m  \\\n6               12.0                    1.840991       4.222222   \n7               23.0                    1.505298       4.027778   \n8               12.0                    3.185351       9.055555   \n9               11.0                    3.474780       8.361111   \n10               8.0                    3.211841       5.416667   \n\n    f10_metre_v_wind_component  winddirection_10m  fdirect_solar_radiation  \\\n6                    -3.857846              338.0                      0.0   \n7                    -3.590024              337.0                      0.0   \n8                    -8.173276              338.0                      0.0   \n9                    -8.008969              335.0                      0.0   \n10                   -7.426206              337.0                      0.0   \n\n    direct_solar_radiation  fsurface_solar_radiation_downwards  \\\n6                      0.0                                 0.0   \n7                      0.0                                 0.0   \n8                      0.0                                 0.0   \n9                      0.0                                 0.0   \n10                     0.0                                 0.0   \n\n    shortwave_radiation  diffuse_radiation  fsnowfall  snowfall  \\\n6                   0.0                0.0        0.0       0.0   \n7                   0.0                0.0        0.0       0.0   \n8                   0.0                0.0        0.0       0.0   \n9                   0.0                0.0        0.0       0.0   \n10                  0.0                0.0        0.0       0.0   \n\n    ftotal_precipitation  rain  surface_pressure  weather_forecast_hour  \\\n6                    0.0   0.0       1009.200012                    1.0   \n7                    0.0   0.0       1004.200012                    1.0   \n8                    0.0   0.0       1015.000000                    1.0   \n9                    0.0   0.0       1014.500000                    1.0   \n10                   0.0   0.0       1014.000000                    1.0   \n\n    is_consumption  \n6                0  \n7                0  \n8                0  \n9                0  \n10               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.681543</td>\n      <td>12.4</td>\n      <td>9.783228</td>\n      <td>9.7</td>\n      <td>0.023590</td>\n      <td>11.0</td>\n      <td>0.002380</td>\n      <td>10.0</td>\n      <td>0.001251</td>\n      <td>0.0</td>\n      <td>0.026398</td>\n      <td>12.0</td>\n      <td>1.840991</td>\n      <td>4.222222</td>\n      <td>-3.857846</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1009.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.868921</td>\n      <td>12.3</td>\n      <td>9.498316</td>\n      <td>9.6</td>\n      <td>0.431854</td>\n      <td>6.0</td>\n      <td>0.211182</td>\n      <td>23.0</td>\n      <td>0.006790</td>\n      <td>1.0</td>\n      <td>0.548508</td>\n      <td>23.0</td>\n      <td>1.505298</td>\n      <td>4.027778</td>\n      <td>-3.590024</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1004.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>15.041773</td>\n      <td>15.2</td>\n      <td>11.860376</td>\n      <td>11.8</td>\n      <td>0.134674</td>\n      <td>18.0</td>\n      <td>0.202515</td>\n      <td>7.0</td>\n      <td>0.003906</td>\n      <td>0.0</td>\n      <td>0.308930</td>\n      <td>12.0</td>\n      <td>3.185351</td>\n      <td>9.055555</td>\n      <td>-8.173276</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1015.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.632105</td>\n      <td>14.9</td>\n      <td>11.773584</td>\n      <td>11.5</td>\n      <td>0.255188</td>\n      <td>19.0</td>\n      <td>0.036774</td>\n      <td>6.0</td>\n      <td>0.026245</td>\n      <td>0.0</td>\n      <td>0.286194</td>\n      <td>11.0</td>\n      <td>3.474780</td>\n      <td>8.361111</td>\n      <td>-8.008969</td>\n      <td>335.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.500000</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.480005</td>\n      <td>12.8</td>\n      <td>11.581568</td>\n      <td>10.4</td>\n      <td>0.338074</td>\n      <td>12.0</td>\n      <td>0.037109</td>\n      <td>4.0</td>\n      <td>0.016510</td>\n      <td>1.0</td>\n      <td>0.368134</td>\n      <td>8.0</td>\n      <td>3.211841</td>\n      <td>5.416667</td>\n      <td>-7.426206</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"train5.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:30:09.640640Z","iopub.execute_input":"2025-08-30T18:30:09.641758Z","iopub.status.idle":"2025-08-30T18:30:13.606600Z","shell.execute_reply.started":"2025-08-30T18:30:09.641718Z","shell.execute_reply":"2025-08-30T18:30:13.605408Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"row_id                                0\ntarget                                0\ndatetime                              0\nforecast_date                         0\nhour                                  0\ndata_block_id                         0\nprediction_unit_id                    0\nis_business                           0\nproduct_type                          0\ncounty                                0\nlatitude                              0\nlongitude                             0\nlowest_price_per_mwh                  0\nhighest_price_per_mwh                 0\neuros_per_mwh                         0\neic_count                             0\ninstalled_capacity                    0\nftemperature                          0\ntemperature                           0\nfdewpoint                             0\ndewpoint                              0\nfcloudcover_high                      0\ncloudcover_high                       0\nfcloudcover_low                       0\ncloudcover_low                        0\nfcloudcover_mid                       0\ncloudcover_mid                        0\nfcloudcover_total                     0\ncloudcover_total                      0\nf10_metre_u_wind_component            0\nwindspeed_10m                         0\nf10_metre_v_wind_component            0\nwinddirection_10m                     0\nfdirect_solar_radiation               0\ndirect_solar_radiation                0\nfsurface_solar_radiation_downwards    0\nshortwave_radiation                   0\ndiffuse_radiation                     0\nfsnowfall                             0\nsnowfall                              0\nftotal_precipitation                  0\nrain                                  0\nsurface_pressure                      0\nweather_forecast_hour                 0\nis_consumption                        0\ndtype: int64"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:10.862956Z","iopub.execute_input":"2025-08-30T18:28:10.863338Z","iopub.status.idle":"2025-08-30T18:28:11.312532Z","shell.execute_reply.started":"2025-08-30T18:28:10.863307Z","shell.execute_reply":"2025-08-30T18:28:11.304991Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2167009006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"],"ename":"NameError","evalue":"name 'a' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"#target 528 nulos de mas de 2kk , los omito\n#fecha inicio 2021-09-01\n#Dataset Shape: (2018352, 9)\n#ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n#ðŸ“ Columnas categÃ³ricas: ['datetime']\n#quick_info(train)\n#train.head()\n#train.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.313362Z","iopub.status.idle":"2025-08-30T18:28:11.313823Z","shell.execute_reply.started":"2025-08-30T18:28:11.313659Z","shell.execute_reply":"2025-08-30T18:28:11.313676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1 = train.dropna()\n#quick_info(train1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.315867Z","iopub.status.idle":"2025-08-30T18:28:11.316415Z","shell.execute_reply.started":"2025-08-30T18:28:11.316198Z","shell.execute_reply":"2025-08-30T18:28:11.316219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 0 nulos\n#fecha inicio 2021-09-01\n# Dataset Shape: (41919, 7)\n#ðŸ”¢ Columnas numÃ©ricas: ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'data_block_id']\n#ðŸ“ Columnas categÃ³ricas: ['date']\n#quick_info(client)\n#client.head()\n# client.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.317403Z","iopub.status.idle":"2025-08-30T18:28:11.317730Z","shell.execute_reply.started":"2025-08-30T18:28:11.317571Z","shell.execute_reply":"2025-08-30T18:28:11.317584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 0 nulos\n#fecha inicio 2021-08-31\n#ðŸ“Š Dataset Shape: (15286, 4)\n#ðŸ”¢ Columnas numÃ©ricas: ['euros_per_mwh', 'data_block_id']\n#ðŸ“ Columnas categÃ³ricas: ['forecast_date', 'origin_date']\n#quick_info(electricity_prices)\n#electricity_prices.head()\n#electricity_prices.describe(include=\"all\")\n# euros_per_mwh porque el minimo es -10.06 ?\n# un supuesto seria que es lo que paga la empresa si aportas electricidad , pero siendo el max 4000 , hace ruido pensar eso","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.319175Z","iopub.status.idle":"2025-08-30T18:28:11.319530Z","shell.execute_reply.started":"2025-08-30T18:28:11.319384Z","shell.execute_reply":"2025-08-30T18:28:11.319397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2 nulos en 'surface_solar_radiation_downwards' , los omito\n#fecha inicio 2021-09-01\n#ðŸ“Š Dataset Shape: (3424512, 18)\n#ðŸ”¢ Columnas numÃ©ricas: ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid',\n#'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'data_block_id', 'direct_solar_radiation',\n#'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n#ðŸ“ Columnas categÃ³ricas: ['origin_datetime', 'forecast_datetime']\n#quick_info(forecast_weather)\n#forecast_weather.head()\n#forecast_weather.describe(include=\"all\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.321617Z","iopub.status.idle":"2025-08-30T18:28:11.322450Z","shell.execute_reply.started":"2025-08-30T18:28:11.322169Z","shell.execute_reply":"2025-08-30T18:28:11.322204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1 = forecast_weather.dropna()\n#quick_info(forecast_weather1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.324167Z","iopub.status.idle":"2025-08-30T18:28:11.324481Z","shell.execute_reply.started":"2025-08-30T18:28:11.324340Z","shell.execute_reply":"2025-08-30T18:28:11.324353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#0 nulos\n#fecha inicio 2021-09-01\n#ðŸ“Š Dataset Shape: (637, 5)\n#ðŸ”¢ Columnas numÃ©ricas: ['lowest_price_per_mwh', 'highest_price_per_mwh', 'data_block_id']\n#ðŸ“ Columnas categÃ³ricas: ['forecast_date', 'origin_date']\n#quick_info(gas_prices)\n#gas_prices.head()\n#gas_prices.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.327393Z","iopub.status.idle":"2025-08-30T18:28:11.328550Z","shell.execute_reply.started":"2025-08-30T18:28:11.328177Z","shell.execute_reply":"2025-08-30T18:28:11.328205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#0 nulos\n#fecha inicio 2021-09-01\n#ðŸ“Š Dataset Shape: (1710802, 18)\n#ðŸ”¢ Columnas numÃ©ricas: ['temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n#'cloudcover_mid', 'cloudcover_high', 'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n#'diffuse_radiation', 'latitude', 'longitude', 'data_block_id']\n#ðŸ“ Columnas categÃ³ricas: ['datetime']\n#quick_info(historical_weather)\n#historical_weather.head()\n#historical_weather.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.329682Z","iopub.status.idle":"2025-08-30T18:28:11.330141Z","shell.execute_reply.started":"2025-08-30T18:28:11.329954Z","shell.execute_reply":"2025-08-30T18:28:11.329985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ver si completo la info por fuera de los county_name\n# county_name 63 nulos / county 63 nulos\n# ðŸ“Š Dataset Shape: (112, 4)\n# ðŸ”¢ Columnas numÃ©ricas: ['longitude', 'latitude', 'county']\n# ðŸ“ Columnas categÃ³ricas: ['county_name']\n#quick_info(weather_station)\n#weather_station.head(50)\n#weather_station.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.331494Z","iopub.status.idle":"2025-08-30T18:28:11.331844Z","shell.execute_reply.started":"2025-08-30T18:28:11.331664Z","shell.execute_reply":"2025-08-30T18:28:11.331677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weather_station1 = weather_station.copy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.333504Z","iopub.status.idle":"2025-08-30T18:28:11.333981Z","shell.execute_reply.started":"2025-08-30T18:28:11.333742Z","shell.execute_reply":"2025-08-30T18:28:11.333761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unicodedata\nfrom sklearn.neighbors import NearestNeighbors\n\n# === 1) Diccionarios canonicos ===\ncounty_id_to_name_map = {\n    0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"JÃ„RVAMAA\",\n    4: \"JÃ•GEVAMAA\", 5: \"LÃ„Ã„NE-VIRUMAA\", 6: \"LÃ„Ã„NEMAA\", 7: \"PÃ„RNUMAA\",\n    8: \"PÃ•LVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n    12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"VÃ•RUMAA\"\n}\ncounty_name_to_id_map = {v: k for k, v in county_id_to_name_map.items()}\n\n# Para normalizar entradas (sin/ con acentos, \"County\"/\"maakond\", etc.)\nsynonyms_to_canonical = {\n    # clave \"base\" (sin acentos/variantes) -> canon con acentos\n    \"HARJU\": \"HARJUMAA\",\n    \"HIIU\": \"HIIUMAA\",\n    \"IDA-VIRU\": \"IDA-VIRUMAA\",\n    \"JARVA\": \"JÃ„RVAMAA\", \"JÃ„RVA\": \"JÃ„RVAMAA\",\n    \"JOGEVA\": \"JÃ•GEVAMAA\", \"JÃ•GEVA\": \"JÃ•GEVAMAA\",\n    \"LAANE-VIRU\": \"LÃ„Ã„NE-VIRUMAA\", \"LÃ„Ã„NE-VIRU\": \"LÃ„Ã„NE-VIRUMAA\",\n    \"LAANE\": \"LÃ„Ã„NEMAA\", \"LÃ„Ã„NE\": \"LÃ„Ã„NEMAA\",\n    \"PARNU\": \"PÃ„RNUMAA\", \"PÃ„RNU\": \"PÃ„RNUMAA\",\n    \"POLVA\": \"PÃ•LVAMAA\", \"PÃ•LVA\": \"PÃ•LVAMAA\",\n    \"RAPLA\": \"RAPLAMAA\",\n    \"SAARE\": \"SAAREMAA\",\n    \"TARTU\": \"TARTUMAA\",\n    \"VALGA\": \"VALGAMAA\",\n    \"VILJANDI\": \"VILJANDIMAA\",\n    \"VORU\": \"VÃ•RUMAA\", \"VÃ•RU\": \"VÃ•RUMAA\",\n\n    # versiones ya canÃ³nicas\n    \"HARJUMAA\": \"HARJUMAA\",\n    \"HIIUMAA\": \"HIIUMAA\",\n    \"IDA-VIRUMAA\": \"IDA-VIRUMAA\",\n    \"JÃ„RVAMAA\": \"JÃ„RVAMAA\",\n    \"JÃ•GEVAMAA\": \"JÃ•GEVAMAA\",\n    \"LÃ„Ã„NE-VIRUMAA\": \"LÃ„Ã„NE-VIRUMAA\",\n    \"LÃ„Ã„NEMAA\": \"LÃ„Ã„NEMAA\",\n    \"PÃ„RNUMAA\": \"PÃ„RNUMAA\",\n    \"PÃ•LVAMAA\": \"PÃ•LVAMAA\",\n    \"RAPLAMAA\": \"RAPLAMAA\",\n    \"SAAREMAA\": \"SAAREMAA\",\n    \"TARTUMAA\": \"TARTUMAA\",\n    \"VALGAMAA\": \"VALGAMAA\",\n    \"VILJANDIMAA\": \"VILJANDIMAA\",\n    \"VÃ•RUMAA\": \"VÃ•RUMAA\",\n}\n\ndef strip_accents(s: str) -> str:\n    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n\ndef normalize_county_name(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).upper().strip()\n    # eliminar sufijos frecuentes\n    for suf in [\" COUNTY\", \" MAAKOND\"]:\n        if s.endswith(suf):\n            s = s[: -len(suf)]\n    s = s.replace(\"  \", \" \").replace(\"â€“\", \"-\")  # guion raro -> normal\n    s_noacc = strip_accents(s)\n    s_noacc = s_noacc.replace(\"  \", \" \")\n    # prueba de match exacto o por prefijo lÃ³gico\n    # ej. \"HARJU\", \"HARJU MAAKOND\" -> \"HARJU\"\n    token = s_noacc.split()[0]  # primer palabra alcanza en estos casos\n    # casos con guion (IDA-VIRU, LAA NE-VIRU, etc.)\n    if \"-\" in s_noacc:\n        token = s_noacc  # mantener completo en compuestos\n\n    # intento 1: match directo con cadena completa sin acentos\n    if s_noacc in synonyms_to_canonical:\n        return synonyms_to_canonical[s_noacc]\n    # intento 2: match por token/prefijo conocido\n    if token in synonyms_to_canonical:\n        return synonyms_to_canonical[token]\n    # intento 3: si ya vino canÃ³nico exacto (con acentos)\n    if s in synonyms_to_canonical:\n        return synonyms_to_canonical[s]\n\n    return np.nan  # no lo reconozco\n\n# === 2) Normalizar lo existente sin romper los NaN ===\n# (NO convertir NaN a \"NAN\")\nweather_station1['county_name_norm'] = weather_station1['county_name'].apply(normalize_county_name)\n\n# Si tengo 'county' numÃ©rico pero falta nombre, lo inferimos del ID\nmask = weather_station1['county_name_norm'].isna() & weather_station1['county'].notna()\nweather_station1.loc[mask, 'county_name_norm'] = (\n    weather_station1.loc[mask, 'county'].astype(int).map(county_id_to_name_map)\n)\n\n# === 3) Completar faltantes por k-NN (lat/lon) ===\nfcols = ['latitude', 'longitude']\nknown = weather_station1.dropna(subset=['county_name_norm']).copy()\nunknown = weather_station1[weather_station1['county_name_norm'].isna()].copy()\n\nif not unknown.empty and not known.empty:\n    # k-NN en (lat, lon) usando Haversine (necesita radianes)\n    # ojo: haversine en sklearn requiere [lat, lon] en radianes\n    known_rad = np.radians(known[fcols].values)\n    unknown_rad = np.radians(unknown[fcols].values)\n\n    nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n    nbrs.fit(known_rad)\n    distances, idxs = nbrs.kneighbors(unknown_rad)\n\n    # votaciÃ³n simple de los 3 vecinos (majority vote)\n    nn_names = known['county_name_norm'].values\n    filled = []\n    for neigh_idx in idxs:\n        cands = nn_names[neigh_idx]\n        # mayorÃ­a; en empate, tomar el 1er vecino\n        vals, counts = np.unique(cands, return_counts=True)\n        filled.append(vals[np.argmax(counts)])\n\n    weather_station1.loc[unknown.index, 'county_name_norm'] = filled\n\n# === 4) Sincronizar columnas finales ===\nweather_station1['county_name'] = weather_station1['county_name_norm']\nweather_station1['county'] = weather_station1['county_name'].map(county_name_to_id_map).astype('Int64')\n\n# Cualquier remanente a UNKNOWN (12)\nweather_station1.loc[weather_station1['county_name'].isna(), 'county'] = 12\nweather_station1['county'] = weather_station1['county'].astype('Int64')\n\n# Limpieza\nweather_station1.drop(columns=['county_name_norm'], inplace=True)\n\n# Chequeo rÃ¡pido\nprint(weather_station1[['county_name','longitude','latitude','county']].head(25))\nprint(\"\\nFaltantes aÃºn:\", weather_station1['county_name'].isna().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.335823Z","iopub.status.idle":"2025-08-30T18:28:11.336287Z","shell.execute_reply.started":"2025-08-30T18:28:11.336084Z","shell.execute_reply":"2025-08-30T18:28:11.336103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#weather_station1[\"county\"].value_counts()\n#aplicando de esta forma no esta el 12 que es \"UNKNOWN\"\n#supongo que aplica y funciona bien","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.337660Z","iopub.status.idle":"2025-08-30T18:28:11.337972Z","shell.execute_reply.started":"2025-08-30T18:28:11.337803Z","shell.execute_reply":"2025-08-30T18:28:11.337815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0 nulos\n# ðŸ“Š Dataset Shape: (12480, 9)\n# ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n# ðŸ“ Columnas categÃ³ricas: ['prediction_datetime']\n# fecha inicio 2023-05-28\n#quick_info(test)\n#test.head()\n#test.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.339280Z","iopub.status.idle":"2025-08-30T18:28:11.340447Z","shell.execute_reply.started":"2025-08-30T18:28:11.340262Z","shell.execute_reply":"2025-08-30T18:28:11.340281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0 nulos # como identifico a este cliente ?\n# ðŸ“Š Dataset Shape: (262, 7)\n# ðŸ”¢ Columnas numÃ©ricas: ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'data_block_id']\n# ðŸ“ Columnas categÃ³ricas: ['date']\n#quick_info(client_t)\n#client_t.head()\n#client_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.341527Z","iopub.status.idle":"2025-08-30T18:28:11.341821Z","shell.execute_reply.started":"2025-08-30T18:28:11.341683Z","shell.execute_reply":"2025-08-30T18:28:11.341697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -10 el minimo como el de train , ver que hacer con eso\n# ðŸ“Š Dataset Shape: (96, 4)\n# ðŸ”¢ Columnas numÃ©ricas: ['euros_per_mwh', 'data_block_id']\n# ðŸ“ Columnas categÃ³ricas: ['forecast_date', 'origin_date']\n#fecha inicio 2023-05-26\n#quick_info(electricity_prices_t)\n#electricity_prices_t.head()\n#electricity_prices_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.343427Z","iopub.status.idle":"2025-08-30T18:28:11.343858Z","shell.execute_reply.started":"2025-08-30T18:28:11.343652Z","shell.execute_reply":"2025-08-30T18:28:11.343669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Š Dataset Shape: (21504, 18)\n# ðŸ”¢ Columnas numÃ©ricas: ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low',\n#                        'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'data_block_id',\n#                        'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n# ðŸ“ Columnas categÃ³ricas: ['origin_datetime', 'forecast_datetime']\n# fecha inicio 2023-05-27\n# quick_info(forecast_weather_t)\n# forecast_weather_t.head()\n#forecast_weather_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.346119Z","iopub.status.idle":"2025-08-30T18:28:11.346452Z","shell.execute_reply.started":"2025-08-30T18:28:11.346314Z","shell.execute_reply":"2025-08-30T18:28:11.346327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Š Dataset Shape: (4, 5)\n# ðŸ”¢ Columnas numÃ©ricas: ['lowest_price_per_mwh', 'highest_price_per_mwh', 'data_block_id']\n# ðŸ“ Columnas categÃ³ricas: ['forecast_date', 'origin_date']\n# fecha inicio 2023-5-26\n# quick_info(gas_prices_t)\n# gas_prices_t.head()\n# #gas_prices_t.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.349639Z","iopub.status.idle":"2025-08-30T18:28:11.350144Z","shell.execute_reply.started":"2025-08-30T18:28:11.349831Z","shell.execute_reply":"2025-08-30T18:28:11.349845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Š Dataset Shape: (12576, 9)\n# ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n# ðŸ“ Columnas categÃ³ricas: ['datetime']\n# fecha inicio 2023-05-26\n# quick_info(revealed_targets)\n#revealed_targets.head()\n#revealed_targets.describe(include=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.351812Z","iopub.status.idle":"2025-08-30T18:28:11.352391Z","shell.execute_reply.started":"2025-08-30T18:28:11.352084Z","shell.execute_reply":"2025-08-30T18:28:11.352103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lo mejor seria ver como ir combinando todo en 1 dataset ,hago copia y pongo los que estan listos para usar\n\ntrain1 = train.dropna()# train                 \nclient1 = client.copy() # client               \nelectricity_prices1 = electricity_prices.copy() # electricity_prices  \nforecast_weather1 = forecast_weather.dropna() # forecast_weather    \ngas_prices1 = gas_prices.copy() # gas_prices                        \nhistorical_weather1 = historical_weather.copy() # historical_weather    \nweather_station2 = weather_station1.copy()      \n\ntest1 = test.copy() # test                 \nclient_t = client_t.copy() # client_t              \nelectricity_prices_t1 = electricity_prices_t.copy() # electricity_prices_t  \nforecast_weather_t1 = forecast_weather_t.copy() # forecast_weather_t    \ngas_prices_t1 = gas_prices_t.copy() # gas_prices_t          \nrevealed_targets1 = revealed_targets.copy() # revealed_targets    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.353524Z","iopub.status.idle":"2025-08-30T18:28:11.353968Z","shell.execute_reply.started":"2025-08-30T18:28:11.353746Z","shell.execute_reply":"2025-08-30T18:28:11.353764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.355460Z","iopub.status.idle":"2025-08-30T18:28:11.355766Z","shell.execute_reply.started":"2025-08-30T18:28:11.355623Z","shell.execute_reply":"2025-08-30T18:28:11.355636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train1)\n\n# ðŸ“Š Dataset Shape: (2017824, 9)\n# ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id']\n# ðŸ“ Columnas categÃ³ricas: ['datetime']\n# âŒ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.356865Z","iopub.status.idle":"2025-08-30T18:28:11.357301Z","shell.execute_reply.started":"2025-08-30T18:28:11.357095Z","shell.execute_reply":"2025-08-30T18:28:11.357113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#gas_prices1.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.359190Z","iopub.status.idle":"2025-08-30T18:28:11.359590Z","shell.execute_reply.started":"2025-08-30T18:28:11.359404Z","shell.execute_reply":"2025-08-30T18:28:11.359418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(gas_prices1)\n\n# ðŸ“Š Dataset Shape: (637, 5)\n# ðŸ”¢ Columnas numÃ©ricas: ['lowest_price_per_mwh', 'highest_price_per_mwh', 'data_block_id']\n# ðŸ“ Columnas categÃ³ricas: ['forecast_date', 'origin_date']\n# âŒ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.362080Z","iopub.status.idle":"2025-08-30T18:28:11.362758Z","shell.execute_reply.started":"2025-08-30T18:28:11.362295Z","shell.execute_reply":"2025-08-30T18:28:11.362329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# el forecast de gas es de 24 horas\n\n# Convertir datetime y crear columnas separadas en train1\ntrain1['datetime'] = pd.to_datetime(train1['datetime'])\ntrain1['hour'] = train1['datetime'].dt.hour\ntrain1['forecast_date'] = train1['datetime']\n\n#\ngas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\ngas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\ngas_prices1['hour'] = gas_prices1['forecast_date'].dt.hour\ngas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.364084Z","iopub.status.idle":"2025-08-30T18:28:11.364464Z","shell.execute_reply.started":"2025-08-30T18:28:11.364314Z","shell.execute_reply":"2025-08-30T18:28:11.364328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train1.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.367865Z","iopub.status.idle":"2025-08-30T18:28:11.368332Z","shell.execute_reply.started":"2025-08-30T18:28:11.368143Z","shell.execute_reply":"2025-08-30T18:28:11.368164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#gas_prices1.tail(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.372917Z","iopub.status.idle":"2025-08-30T18:28:11.373652Z","shell.execute_reply.started":"2025-08-30T18:28:11.373400Z","shell.execute_reply":"2025-08-30T18:28:11.373425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Expandir gas_prices1 a nivel horario - solo columnas necesarias\ngas_hourly = []\nfor _, row in gas_prices1.iterrows():\n    for hour in range(24):\n        new_row = {\n            'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n            'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n            'highest_price_per_mwh': row['highest_price_per_mwh'],\n            'gas_origin_date': row['gas_origin_date']\n        }\n        gas_hourly.append(new_row)\n\ngas_prices1_hourly = pd.DataFrame(gas_hourly)\n\n# Merge train1 con gas_prices1_hourly\ntrain2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.376789Z","iopub.status.idle":"2025-08-30T18:28:11.377337Z","shell.execute_reply.started":"2025-08-30T18:28:11.377094Z","shell.execute_reply":"2025-08-30T18:28:11.377114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train2.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.379912Z","iopub.status.idle":"2025-08-30T18:28:11.380436Z","shell.execute_reply.started":"2025-08-30T18:28:11.380214Z","shell.execute_reply":"2025-08-30T18:28:11.380235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train2)\n\n# ðŸ“Š Dataset Shape: (2017824, 14)\n# ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n# ðŸ“ Columnas categÃ³ricas: []\n# âŒ Valores nulos por columna:\n# lowest_price_per_mwh     3120\n# highest_price_per_mwh    3120\n# gas_origin_date          3120\n# dtype: int64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.382878Z","iopub.status.idle":"2025-08-30T18:28:11.383404Z","shell.execute_reply.started":"2025-08-30T18:28:11.383163Z","shell.execute_reply":"2025-08-30T18:28:11.383185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\ntrain2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n\n# Eliminar columna gas_origin_date\ntrain2 = train2.drop(columns=['gas_origin_date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.384890Z","iopub.status.idle":"2025-08-30T18:28:11.385278Z","shell.execute_reply.started":"2025-08-30T18:28:11.385131Z","shell.execute_reply":"2025-08-30T18:28:11.385145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.386799Z","iopub.status.idle":"2025-08-30T18:28:11.387206Z","shell.execute_reply.started":"2025-08-30T18:28:11.387012Z","shell.execute_reply":"2025-08-30T18:28:11.387025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train2.tail()\n# ðŸ“Š Dataset Shape: (2017824, 16)\n# ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'minute', 'second', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n# ðŸ“ Columnas categÃ³ricas: []\n# âŒ Valores nulos por columna:\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.388832Z","iopub.status.idle":"2025-08-30T18:28:11.389154Z","shell.execute_reply.started":"2025-08-30T18:28:11.389018Z","shell.execute_reply":"2025-08-30T18:28:11.389029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#electricity_prices1.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.390624Z","iopub.status.idle":"2025-08-30T18:28:11.391171Z","shell.execute_reply.started":"2025-08-30T18:28:11.390854Z","shell.execute_reply":"2025-08-30T18:28:11.390869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(electricity_prices1)\n\n# ðŸ“Š Dataset Shape: (15286, 4)\n# ðŸ”¢ Columnas numÃ©ricas: ['euros_per_mwh', 'data_block_id']\n# ðŸ“ Columnas categÃ³ricas: ['forecast_date', 'origin_date']\n# âŒ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.393571Z","iopub.status.idle":"2025-08-30T18:28:11.393919Z","shell.execute_reply.started":"2025-08-30T18:28:11.393766Z","shell.execute_reply":"2025-08-30T18:28:11.393780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#electricity_prices1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.395276Z","iopub.status.idle":"2025-08-30T18:28:11.395643Z","shell.execute_reply.started":"2025-08-30T18:28:11.395497Z","shell.execute_reply":"2025-08-30T18:28:11.395511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparar electricity_prices1\nelectricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\nelectricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n\n# Extraer componentes de tiempo\nelectricity_prices1['hour'] = electricity_prices1['forecast_date'].dt.hour\n\n\n# Merge\nelec_columns = ['forecast_date', 'euros_per_mwh']\ntrain3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.397224Z","iopub.status.idle":"2025-08-30T18:28:11.397700Z","shell.execute_reply.started":"2025-08-30T18:28:11.397481Z","shell.execute_reply":"2025-08-30T18:28:11.397501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.401100Z","iopub.status.idle":"2025-08-30T18:28:11.401488Z","shell.execute_reply.started":"2025-08-30T18:28:11.401330Z","shell.execute_reply":"2025-08-30T18:28:11.401344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train3.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.403618Z","iopub.status.idle":"2025-08-30T18:28:11.404040Z","shell.execute_reply.started":"2025-08-30T18:28:11.403820Z","shell.execute_reply":"2025-08-30T18:28:11.403839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train3)\n\n# ðŸ“Š Dataset Shape: (2017824, 18)\n# ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'minute', 'second', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh']\n# ðŸ“ Columnas categÃ³ricas: []\n# âŒ Valores nulos por columna:\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.405746Z","iopub.status.idle":"2025-08-30T18:28:11.406154Z","shell.execute_reply.started":"2025-08-30T18:28:11.405956Z","shell.execute_reply":"2025-08-30T18:28:11.405973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train3.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.408074Z","iopub.status.idle":"2025-08-30T18:28:11.408494Z","shell.execute_reply.started":"2025-08-30T18:28:11.408328Z","shell.execute_reply":"2025-08-30T18:28:11.408343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#client1.tail(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.409807Z","iopub.status.idle":"2025-08-30T18:28:11.410479Z","shell.execute_reply.started":"2025-08-30T18:28:11.410263Z","shell.execute_reply":"2025-08-30T18:28:11.410292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quick_info(client1)\n\n# ðŸ“Š Dataset Shape: (41919, 7)\n# ðŸ”¢ Columnas numÃ©ricas: ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'data_block_id']\n# ðŸ“ Columnas categÃ³ricas: ['date']\n# âŒ Valores nulos por columna:\n# Series([], dtype: int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.412016Z","iopub.status.idle":"2025-08-30T18:28:11.412446Z","shell.execute_reply.started":"2025-08-30T18:28:11.412241Z","shell.execute_reply":"2025-08-30T18:28:11.412259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train3.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.413244Z","iopub.status.idle":"2025-08-30T18:28:11.413531Z","shell.execute_reply.started":"2025-08-30T18:28:11.413392Z","shell.execute_reply":"2025-08-30T18:28:11.413405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#client1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.415184Z","iopub.status.idle":"2025-08-30T18:28:11.415621Z","shell.execute_reply.started":"2025-08-30T18:28:11.415428Z","shell.execute_reply":"2025-08-30T18:28:11.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparar client1\nclient1['date'] = pd.to_datetime(client1['date']).dt.date\n\n# Merge con client1 usando datetime de train3\ntrain4 = train3.merge(client1.drop('data_block_id', axis=1),\n                      left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                      right_on=['product_type', 'county', 'is_business', 'date'],\n                      how='left')\n\n# Limpiar\ntrain4 = train4.drop('date', axis=1)\ntrain4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.418453Z","iopub.status.idle":"2025-08-30T18:28:11.418997Z","shell.execute_reply.started":"2025-08-30T18:28:11.418745Z","shell.execute_reply":"2025-08-30T18:28:11.418767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.420204Z","iopub.status.idle":"2025-08-30T18:28:11.420644Z","shell.execute_reply.started":"2025-08-30T18:28:11.420422Z","shell.execute_reply":"2025-08-30T18:28:11.420440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train4)\n\n\n# ðŸ“Š Dataset Shape: (2017824, 20)\n# ðŸ”¢ Columnas numÃ©ricas: ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'data_block_id', 'row_id', 'prediction_unit_id', 'hour', 'minute', 'second', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', 'installed_capacity']\n# ðŸ“ Columnas categÃ³ricas: []\n# âŒ Valores nulos por columna:\n# eic_count                6240\n# installed_capacity       6240\n# dtype: int64\n\n\n#falta corregir esto todavia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.422479Z","iopub.status.idle":"2025-08-30T18:28:11.422783Z","shell.execute_reply.started":"2025-08-30T18:28:11.422647Z","shell.execute_reply":"2025-08-30T18:28:11.422659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# decido completar asi los ultimos 2 dias que faltan\n# Verificar el problema\nprint(\"NaN en datos de clientes:\")\nprint(f\"eic_count: {train4['eic_count'].isna().sum():,}\")\nprint(f\"installed_capacity: {train4['installed_capacity'].isna().sum():,}\")\n\n# Ver las fechas con problemas\nnan_dates = train4[train4['eic_count'].isna()]['datetime'].dt.date.unique()\nprint(f\"Fechas con NaN: {sorted(nan_dates)}\")\n\n# Ver Ãºltimo dÃ­a con datos\nlast_valid_date = train4[train4['eic_count'].notna()]['datetime'].dt.date.max()\nprint(f\"Ãšltimo dÃ­a con datos: {last_valid_date}\")\n\n# Forward fill por grupo (county + is_business + product_type)\n# Esto mantiene los Ãºltimos valores conocidos para cada combinaciÃ³n\nprint(\"\\nCompletando con forward fill por grupo...\")\n\n# Ordenar por datetime para asegurar orden correcto\ntrain4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n\n# Forward fill por grupo\ntrain4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\ntrain4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n\n# Verificar resultado\nprint(f\"\\nDespuÃ©s del forward fill:\")\nprint(f\"eic_count NaN: {train4['eic_count'].isna().sum()}\")\nprint(f\"installed_capacity NaN: {train4['installed_capacity'].isna().sum()}\")\n\n# Si aÃºn quedan NaN, son combinaciones nuevas - usar 0 o promedio\nremaining_nan = train4['eic_count'].isna().sum()\nif remaining_nan > 0:\n    print(f\"\\n{remaining_nan} NaN restantes (combinaciones nuevas)\")\n    print(\"Opciones:\")\n    print(\"1. Rellenar con 0\")\n    print(\"2. Rellenar with promedio del county\")\n    \n    # OpciÃ³n recomendada: promedio por county\n    train4['eic_count'] = train4['eic_count'].fillna(\n        train4.groupby('county')['eic_count'].transform('mean')\n    )\n    train4['installed_capacity'] = train4['installed_capacity'].fillna(\n        train4.groupby('county')['installed_capacity'].transform('mean')\n    )\n    \n    print(f\"NaN finales - eic_count: {train4['eic_count'].isna().sum()}\")\n    print(f\"NaN finales - installed_capacity: {train4['installed_capacity'].isna().sum()}\")\n\nprint(f\"\\nDataset train4: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.424489Z","iopub.status.idle":"2025-08-30T18:28:11.424847Z","shell.execute_reply.started":"2025-08-30T18:28:11.424684Z","shell.execute_reply":"2025-08-30T18:28:11.424696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.425724Z","iopub.status.idle":"2025-08-30T18:28:11.426084Z","shell.execute_reply.started":"2025-08-30T18:28:11.425883Z","shell.execute_reply":"2025-08-30T18:28:11.425895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.428292Z","iopub.status.idle":"2025-08-30T18:28:11.429833Z","shell.execute_reply.started":"2025-08-30T18:28:11.429604Z","shell.execute_reply":"2025-08-30T18:28:11.429632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.430697Z","iopub.status.idle":"2025-08-30T18:28:11.431394Z","shell.execute_reply.started":"2025-08-30T18:28:11.431127Z","shell.execute_reply":"2025-08-30T18:28:11.431147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weather_station3 = weather_station2.copy()\nweather_station3.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.433067Z","iopub.status.idle":"2025-08-30T18:28:11.433404Z","shell.execute_reply.started":"2025-08-30T18:28:11.433254Z","shell.execute_reply":"2025-08-30T18:28:11.433268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge forecast_weather1 con weather_station1 por coordenadas\nforecast_weather1 = forecast_weather1.merge(weather_station3[['latitude', 'longitude', 'county', 'county_name']], \n                                          on=['latitude', 'longitude'], \n                                          how='left')\n\nprint(f\"Filas con county asignado: {forecast_weather1['county'].notna().sum()}\")\nprint(f\"Total filas: {len(forecast_weather1)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.434606Z","iopub.status.idle":"2025-08-30T18:28:11.434910Z","shell.execute_reply.started":"2025-08-30T18:28:11.434769Z","shell.execute_reply":"2025-08-30T18:28:11.434782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.436149Z","iopub.status.idle":"2025-08-30T18:28:11.436471Z","shell.execute_reply.started":"2025-08-30T18:28:11.436333Z","shell.execute_reply":"2025-08-30T18:28:11.436346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.438445Z","iopub.status.idle":"2025-08-30T18:28:11.438799Z","shell.execute_reply.started":"2025-08-30T18:28:11.438635Z","shell.execute_reply":"2025-08-30T18:28:11.438648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(forecast_weather1)\n\n# ðŸ“Š Dataset Shape: (3424510, 20)\n# ðŸ”¢ Columnas numÃ©ricas: ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'data_block_id', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation', 'county']\n# ðŸ“ Columnas categÃ³ricas: ['origin_datetime', 'forecast_datetime', 'county_name']\n# âŒ Valores nulos por columna:\n# county         2140318\n# county_name    2140318\n# dtype: int64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.440968Z","iopub.status.idle":"2025-08-30T18:28:11.441550Z","shell.execute_reply.started":"2025-08-30T18:28:11.441266Z","shell.execute_reply":"2025-08-30T18:28:11.441293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nimport numpy as np\n\n# Identificar filas sin county asignado\nmissing_county = forecast_weather1['county'].isna()\nprint(f\"Filas sin county: {missing_county.sum()}\")\n\nif missing_county.sum() > 0:\n   # Datos conocidos (con county)\n   known = weather_station1[['latitude', 'longitude', 'county', 'county_name']].copy()\n   \n   # Datos faltantes\n   missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n   \n   # K-NN con distancia euclidiana (para coordenadas pequeÃ±as como Estonia)\n   nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n   nbrs.fit(known[['latitude', 'longitude']].values)\n   \n   # Encontrar vecino mÃ¡s cercano\n   distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n   \n   # Asignar county y county_name del vecino mÃ¡s cercano\n   nearest_counties = known.iloc[indices.flatten()]\n   \n   forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n   forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n\nprint(f\"Filas con county despuÃ©s del K-NN: {forecast_weather1['county'].notna().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.442861Z","iopub.status.idle":"2025-08-30T18:28:11.443343Z","shell.execute_reply.started":"2025-08-30T18:28:11.443191Z","shell.execute_reply":"2025-08-30T18:28:11.443206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(forecast_weather1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.445263Z","iopub.status.idle":"2025-08-30T18:28:11.446120Z","shell.execute_reply.started":"2025-08-30T18:28:11.445804Z","shell.execute_reply":"2025-08-30T18:28:11.445823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather1.tail()\n# unir por latitu/longitude y forecast_datetime# esto por ahora no\n# sacar county_name\n# sacar origin_datetime\n# renombrar las variables de forecast con una f adelante (excepto county/hours_ahead/latitude/longitude)\n# eliminar origin_datetime\n# agregar horas/minutos/segundos y dejar 1 columna con aÃ±o/mes/dia cuando transforme forecast_datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.447224Z","iopub.status.idle":"2025-08-30T18:28:11.447740Z","shell.execute_reply.started":"2025-08-30T18:28:11.447528Z","shell.execute_reply":"2025-08-30T18:28:11.447547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forecast_weather2 = forecast_weather1.copy()\n\n# Eliminar columnas innecesarias\nforecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n\n\n# Convertir forecast_datetime a datetime\nforecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n\n# Crear columnas temporales\nforecast_weather2['forecast_date'] = forecast_weather2['forecast_datetime'].dt.date\nforecast_weather2['hour'] = forecast_weather2['forecast_datetime'].dt.hour\n\n# Renombrar variables con 'f' adelante (excepto las especificadas)\nrename_dict = {}\nfor col in forecast_weather2.columns:\n   if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                  'forecast_datetime', 'forecast_date', 'hour']:\n       rename_dict[col] = 'f' + col\n\nforecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n\nprint(\"Columnas despuÃ©s de transformaciones:\")\nprint(forecast_weather2.columns.tolist())\n\nforecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\":\"weather_forecast_hour\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.449572Z","iopub.status.idle":"2025-08-30T18:28:11.449915Z","shell.execute_reply.started":"2025-08-30T18:28:11.449744Z","shell.execute_reply":"2025-08-30T18:28:11.449757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forecast_weather2.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.451997Z","iopub.status.idle":"2025-08-30T18:28:11.452749Z","shell.execute_reply.started":"2025-08-30T18:28:11.452390Z","shell.execute_reply":"2025-08-30T18:28:11.452482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#historical_weather1.tail()\n#agregarle horas/minutos/segundos y cambiar datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.454612Z","iopub.status.idle":"2025-08-30T18:28:11.455114Z","shell.execute_reply.started":"2025-08-30T18:28:11.454841Z","shell.execute_reply":"2025-08-30T18:28:11.454856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(historical_weather1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.456388Z","iopub.status.idle":"2025-08-30T18:28:11.457268Z","shell.execute_reply.started":"2025-08-30T18:28:11.456772Z","shell.execute_reply":"2025-08-30T18:28:11.456792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"historical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n\n# Renombrar y merge\nforecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n\nmerged_weather = pd.merge(\n    forecast_weather2,\n    historical_weather1,\n    left_on=['latitude', 'longitude', 'forecast_datetime'],\n    right_on=['latitude', 'longitude', 'datetime'],\n    how='inner'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.459437Z","iopub.status.idle":"2025-08-30T18:28:11.459800Z","shell.execute_reply.started":"2025-08-30T18:28:11.459649Z","shell.execute_reply":"2025-08-30T18:28:11.459664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merged_weather.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.460791Z","iopub.status.idle":"2025-08-30T18:28:11.461103Z","shell.execute_reply.started":"2025-08-30T18:28:11.460962Z","shell.execute_reply":"2025-08-30T18:28:11.460974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merged_weather.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.462968Z","iopub.status.idle":"2025-08-30T18:28:11.463479Z","shell.execute_reply.started":"2025-08-30T18:28:11.463197Z","shell.execute_reply":"2025-08-30T18:28:11.463215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(merged_weather)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.465801Z","iopub.status.idle":"2025-08-30T18:28:11.466244Z","shell.execute_reply.started":"2025-08-30T18:28:11.466033Z","shell.execute_reply":"2025-08-30T18:28:11.466065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.467409Z","iopub.status.idle":"2025-08-30T18:28:11.467788Z","shell.execute_reply.started":"2025-08-30T18:28:11.467607Z","shell.execute_reply":"2025-08-30T18:28:11.467620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizar tipos de datos\ntrain4['county'] = train4['county'].astype('uint8')  # 0-15\ntrain4['is_business'] = train4['is_business'].astype('uint8')  # 0-1\ntrain4['product_type'] = train4['product_type'].astype('uint8')  # 0-3\ntrain4['is_consumption'] = train4['is_consumption'].astype('uint8')  # 0-1\ntrain4['hour'] = train4['hour'].astype('uint8')  # 0-23\n\n# IDs optimizados\ntrain4['data_block_id'] = train4['data_block_id'].astype('uint16')  # 0-700\ntrain4['row_id'] = train4['row_id'].astype('uint32')  # 0-3M\ntrain4['prediction_unit_id'] = train4['prediction_unit_id'].astype('uint8')  # 0-68\n\n# Flotantes a float32 (suficiente precision para los rangos dados)\ntrain4['target'] = train4['target'].astype('float32')\ntrain4['lowest_price_per_mwh'] = train4['lowest_price_per_mwh'].astype('float32')\ntrain4['highest_price_per_mwh'] = train4['highest_price_per_mwh'].astype('float32')\ntrain4['euros_per_mwh'] = train4['euros_per_mwh'].astype('float32')\ntrain4['eic_count'] = train4['eic_count'].astype('float32')\ntrain4['installed_capacity'] = train4['installed_capacity'].astype('float32')\n\nprint(\"OptimizaciÃ³n completada\")\nprint(train4.info(memory_usage='deep'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.469154Z","iopub.status.idle":"2025-08-30T18:28:11.469485Z","shell.execute_reply.started":"2025-08-30T18:28:11.469331Z","shell.execute_reply":"2025-08-30T18:28:11.469344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Eliminar columnas innecesarias\nmerged_weather = merged_weather.drop('f_data_block_id', axis=1)\n\n# Coordenadas - float32 suficiente para la precisiÃ³n geogrÃ¡fica\nmerged_weather['latitude'] = merged_weather['latitude'].astype('float32')  # 57.6-59.7\nmerged_weather['longitude'] = merged_weather['longitude'].astype('float32')  # 21.7-28.2\n\n# Enteros pequeÃ±os\nmerged_weather['weather_forecast_hour'] = merged_weather['weather_forecast_hour'].astype('uint8')  # 1-48\nmerged_weather['county'] = merged_weather['county'].astype('uint8')  # 0-15\nmerged_weather['hour'] = merged_weather['hour'].astype('uint8')  # 0-23\n\n# IDs\nmerged_weather['data_block_id'] = merged_weather['data_block_id'].astype('uint16')  # 1-637\n\n# Variables meteorolÃ³gicas - float32 es suficiente\nweather_float_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                     'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                     'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                     'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                     'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                     'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                     'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                     'direct_solar_radiation', 'diffuse_radiation']\n\nfor col in weather_float_cols:\n    merged_weather[col] = merged_weather[col].astype('float32')\n\nprint(\"OptimizaciÃ³n de merged_weather completada\")\nprint(merged_weather.info(memory_usage='deep'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.470837Z","iopub.status.idle":"2025-08-30T18:28:11.471315Z","shell.execute_reply.started":"2025-08-30T18:28:11.471116Z","shell.execute_reply":"2025-08-30T18:28:11.471134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(train4.info(memory_usage='deep'))\n\n#ANTES\n# <class 'pandas.core.frame.DataFrame'>\n# RangeIndex: 2017824 entries, 0 to 2017823\n# Data columns (total 22 columns):\n#  #   Column                 Dtype         \n# ---  ------                 -----         \n#  0   county                 int64         \n#  1   is_business            int64         \n#  2   product_type           int64         \n#  3   target                 float64       \n#  4   is_consumption         int64         \n#  5   datetime               datetime64[ns]\n#  6   data_block_id          int64         \n#  7   row_id                 int64         \n#  8   prediction_unit_id     int64         \n#  9   hour                   int32         \n#  10  minute                 int32         \n#  11  second                 int32         \n#  12  forecast_date          datetime64[ns]\n#  13  lowest_price_per_mwh   float64       \n#  14  highest_price_per_mwh  float64       \n#  15  gas_origin_date        datetime64[ns]\n#  16  euros_per_mwh          float64       \n#  17  elec_origin_date       datetime64[ns]\n#  18  eic_count              float64       \n#  19  installed_capacity     float64       \n#  20  elec_forecast_hour     int64         \n#  21  gas_forecast_hour      int64         \n# dtypes: datetime64[ns](4), float64(6), int32(3), int64(9)\n# memory usage: 315.6 MB\n# None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.472182Z","iopub.status.idle":"2025-08-30T18:28:11.472592Z","shell.execute_reply.started":"2025-08-30T18:28:11.472391Z","shell.execute_reply":"2025-08-30T18:28:11.472409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(merged_weather.info(memory_usage='deep'))\n\n# Antes\n# <class 'pandas.core.frame.DataFrame'>\n# RangeIndex: 3418242 entries, 0 to 3418241\n# Data columns (total 38 columns):\n#  #   Column                              Dtype         \n# ---  ------                              -----         \n#  0   latitude                            float64     # 2 digitos y 1 decimal  \n#  1   longitude                           float64     # 2 digitos y 1 decimal   \n#  2   weather_forecast_hour               int64       # 0 a 4 digitos \n#  3   ftemperature                        float64     #  \n#  4   fdewpoint                           float64       \n#  5   fcloudcover_high                    float64       \n#  6   fcloudcover_low                     float64       \n#  7   fcloudcover_mid                     float64       \n#  8   fcloudcover_total                   float64       \n#  9   f10_metre_u_wind_component          float64       \n#  10  f10_metre_v_wind_component          float64       \n#  11  f_data_block_id                     int64         \n#  12  forecast_datetime                   datetime64[ns]\n#  13  fdirect_solar_radiation             float64       \n#  14  fsurface_solar_radiation_downwards  float64       \n#  15  fsnowfall                           float64       \n#  16  ftotal_precipitation                float64       \n#  17  county                              Int64         \n#  18  forecast_date                       object        \n#  19  hour                                int32         \n#  20  minute                              int32         \n#  21  second                              int32         \n#  22  datetime                            datetime64[ns]\n#  23  temperature                         float64       \n#  24  dewpoint                            float64       \n#  25  rain                                float64       \n#  26  snowfall                            float64       \n#  27  surface_pressure                    float64       \n#  28  cloudcover_total                    int64         \n#  29  cloudcover_low                      int64         \n#  30  cloudcover_mid                      int64         \n#  31  cloudcover_high                     int64         \n#  32  windspeed_10m                       float64       \n#  33  winddirection_10m                   int64         \n#  34  shortwave_radiation                 float64       \n#  35  direct_solar_radiation              float64       \n#  36  diffuse_radiation                   float64       \n#  37  data_block_id                       float64       \n# dtypes: Int64(1), datetime64[ns](2), float64(24), int32(3), int64(7), object(1)\n# memory usage: 1.0 GB\n# None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.475130Z","iopub.status.idle":"2025-08-30T18:28:11.475593Z","shell.execute_reply.started":"2025-08-30T18:28:11.475375Z","shell.execute_reply":"2025-08-30T18:28:11.475394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train3, train2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.477363Z","iopub.status.idle":"2025-08-30T18:28:11.477743Z","shell.execute_reply.started":"2025-08-30T18:28:11.477565Z","shell.execute_reply":"2025-08-30T18:28:11.477578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_weather = merged_weather.drop('forecast_datetime', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.479583Z","iopub.status.idle":"2025-08-30T18:28:11.480095Z","shell.execute_reply.started":"2025-08-30T18:28:11.479840Z","shell.execute_reply":"2025-08-30T18:28:11.479861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train4.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.481651Z","iopub.status.idle":"2025-08-30T18:28:11.482047Z","shell.execute_reply.started":"2025-08-30T18:28:11.481858Z","shell.execute_reply":"2025-08-30T18:28:11.481871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merged_weather.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.483169Z","iopub.status.idle":"2025-08-30T18:28:11.483481Z","shell.execute_reply.started":"2025-08-30T18:28:11.483344Z","shell.execute_reply":"2025-08-30T18:28:11.483356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train, client, electricity_prices, forecast_weather, gas_prices, historical_weather, weather_station","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.485599Z","iopub.status.idle":"2025-08-30T18:28:11.485891Z","shell.execute_reply.started":"2025-08-30T18:28:11.485759Z","shell.execute_reply":"2025-08-30T18:28:11.485772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merge por county y datetime\ndef process_weather_chunks_corrected(start_chunk, end_chunk):\n    periods = [\n        ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), ('2021-12-01', '2022-01-01'),\n        ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'),\n        ('2022-05-01', '2022-06-01'), ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n        ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), ('2022-12-01', '2023-01-01'),\n        ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'),\n        ('2023-05-01', '2023-05-31')\n    ]\n    \n    for i in range(start_chunk, end_chunk):\n        start_date, end_date = periods[i]\n        print(f\"Procesando perÃ­odo {i+1}: {start_date} a {end_date}\")\n        \n        # Filtrar por perÃ­odo\n        mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n        train4_chunk = train4[mask_train].copy()\n        \n        mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n        weather_chunk = merged_weather[mask_weather].copy()\n        \n        # Merge CORREGIDO - por county y datetime\n        chunk_result = pd.merge(train4_chunk, weather_chunk, \n                               on=['county', 'datetime'], \n                               how='left')\n        \n        chunk_result.to_parquet(f'train5_corrected_chunk_{i+1}.parquet', index=False)\n        print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n        \n        # Liberar memoria\n        del train4_chunk, weather_chunk, chunk_result\n\n# Probar con un chunk pequeÃ±o primero\nprocess_weather_chunks_corrected(0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.487236Z","iopub.status.idle":"2025-08-30T18:28:11.487571Z","shell.execute_reply.started":"2025-08-30T18:28:11.487430Z","shell.execute_reply":"2025-08-30T18:28:11.487443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(0, 3)\n","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-08-30T18:28:11.489732Z","shell.execute_reply.started":"2025-08-30T18:28:11.489518Z","shell.execute_reply":"2025-08-30T18:28:11.489537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(3, 9)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.492324Z","iopub.status.idle":"2025-08-30T18:28:11.492646Z","shell.execute_reply.started":"2025-08-30T18:28:11.492491Z","shell.execute_reply":"2025-08-30T18:28:11.492521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(9, 15)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.494079Z","iopub.status.idle":"2025-08-30T18:28:11.494610Z","shell.execute_reply.started":"2025-08-30T18:28:11.494385Z","shell.execute_reply":"2025-08-30T18:28:11.494407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_weather_chunks_corrected(15, 21)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.495821Z","iopub.status.idle":"2025-08-30T18:28:11.496337Z","shell.execute_reply.started":"2025-08-30T18:28:11.496055Z","shell.execute_reply":"2025-08-30T18:28:11.496085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenar de a grupos para evitar saturar memoria\ndef concat_chunks_by_groups(group_size=1):\n    group_files = []\n    \n    # Crear grupos intermedios\n    for group_start in range(1, 22, group_size):\n        group_end = min(group_start + group_size, 22)\n        print(f\"Procesando grupo: chunks {group_start} a {group_end-1}\")\n        \n        # Leer chunks del grupo\n        group_chunks = []\n        for i in range(group_start, group_end):\n            try:\n                chunk = pd.read_parquet(f'train5_corrected_chunk_{i}.parquet')  # CORREGIDO\n                group_chunks.append(chunk)\n            except FileNotFoundError:\n                print(f\"Archivo train5_corrected_chunk_{i}.parquet no encontrado\")\n        \n        # Concatenar grupo y guardar\n        if group_chunks:\n            group_df = pd.concat(group_chunks, ignore_index=True)\n            group_filename = f'train5_group_{group_start}_{group_end-1}.parquet'\n            group_df.to_parquet(group_filename, index=False)\n            group_files.append(group_filename)\n            print(f\"Grupo guardado: {group_filename} - Shape: {group_df.shape}\")\n            \n            # Liberar memoria\n            del group_chunks, group_df\n    \n    # Ahora concatenar los grupos (serÃ¡n muchos menos archivos)\n    print(\"Concatenando grupos finales...\")\n    final_chunks = []\n    for group_file in group_files:\n        group_data = pd.read_parquet(group_file)\n        final_chunks.append(group_data)\n    \n    train5 = pd.concat(final_chunks, ignore_index=True)\n    return train5\n\n# Ejecutar\ntrain5 = concat_chunks_by_groups(1)\nprint(f\"Dataset final: {train5.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.499142Z","iopub.status.idle":"2025-08-30T18:28:11.499530Z","shell.execute_reply.started":"2025-08-30T18:28:11.499377Z","shell.execute_reply":"2025-08-30T18:28:11.499392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.501454Z","iopub.status.idle":"2025-08-30T18:28:11.501859Z","shell.execute_reply.started":"2025-08-30T18:28:11.501686Z","shell.execute_reply":"2025-08-30T18:28:11.501700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick_info(train5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.503551Z","iopub.status.idle":"2025-08-30T18:28:11.503852Z","shell.execute_reply.started":"2025-08-30T18:28:11.503718Z","shell.execute_reply":"2025-08-30T18:28:11.503731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\n\n# 1. Identificar columnas a eliminar y renombrar SIN cargar datos\nsample = train5.head(1000)  # Solo una muestra\ncols_to_drop = [col for col in sample.columns if col.endswith('_y')]\nrename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\nprint(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n\n# 2. Procesar por chunks de 1M filas\nchunk_size = 1_000_000\nchunks_processed = []\n\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Procesando chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx].copy()\n    \n    # Filtrar county != 12\n    chunk = chunk[chunk['county'] != 12]\n    \n    if len(chunk) > 0:  # Solo si quedan filas\n        # Eliminar columnas _y\n        chunk = chunk.drop(columns=cols_to_drop)\n        \n        # Renombrar _x\n        chunk.rename(columns=rename_dict, inplace=True)\n        \n        chunks_processed.append(chunk)\n    \n    # Limpiar memoria\n    del chunk\n    gc.collect()\n    \n    print(f\"Memoria liberada, chunks guardados: {len(chunks_processed)}\")\n\n# 3. Concatenar chunks finales\nprint(\"Concatenando chunks finales...\")\ntrain5_clean = pd.concat(chunks_processed, ignore_index=True)\n\n# 4. Limpiar\ndel chunks_processed, train5\ngc.collect()\n\nprint(f\"Dataset final: {train5_clean.shape[0]:,} filas y {train5_clean.shape[1]} columnas\")\n\n# Renombrar para usar el mismo nombre\ntrain5 = train5_clean\ndel train5_clean\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.504804Z","iopub.status.idle":"2025-08-30T18:28:11.505162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.506301Z","iopub.status.idle":"2025-08-30T18:28:11.507136Z","shell.execute_reply.started":"2025-08-30T18:28:11.506898Z","shell.execute_reply":"2025-08-30T18:28:11.506917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.508115Z","iopub.status.idle":"2025-08-30T18:28:11.508506Z","shell.execute_reply.started":"2025-08-30T18:28:11.508336Z","shell.execute_reply":"2025-08-30T18:28:11.508354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Orden especÃ­fico que solicitaste\nmain_cols = [\n    'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n    'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n    'longitude', 'lowest_price_per_mwh', \n    'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', 'installed_capacity'\n]\n\n# Pares de clima: pronÃ³stico (f) vs histÃ³rico - lado a lado para comparar\nweather_pairs = [\n    # Temperatura\n    'ftemperature', 'temperature',\n    \n    # Punto de rocÃ­o  \n    'fdewpoint', 'dewpoint',\n    \n    # Cobertura de nubes\n    'fcloudcover_high', 'cloudcover_high',\n    'fcloudcover_low', 'cloudcover_low', \n    'fcloudcover_mid', 'cloudcover_mid',\n    'fcloudcover_total', 'cloudcover_total',\n    \n    # Viento\n    'f10_metre_u_wind_component', 'windspeed_10m',\n    'f10_metre_v_wind_component', 'winddirection_10m',\n    \n    # RadiaciÃ³n solar\n    'fdirect_solar_radiation', 'direct_solar_radiation',\n    'fsurface_solar_radiation_downwards', 'shortwave_radiation',\n    'diffuse_radiation',  # solo histÃ³rico\n    \n    # PrecipitaciÃ³n\n    'fsnowfall', 'snowfall',\n    'ftotal_precipitation', 'rain',\n    \n    # Otras variables climÃ¡ticas\n    'surface_pressure',  # solo histÃ³rico\n    'weather_forecast_hour'\n]\n\n# Construir orden final\nfinal_order = main_cols.copy()\n\n# Agregar solo las columnas que existen\nfor col in weather_pairs:\n    if col in train5.columns:\n        final_order.append(col)\n\n# Agregar cualquier columna restante\nremaining_cols = [col for col in train5.columns if col not in final_order]\nfinal_order.extend(remaining_cols)\n\nprint(f\"Reordenando {len(final_order)} columnas...\")\nprint(\"Pares clima encontrados:\")\nweather_cols_found = [col for col in weather_pairs if col in train5.columns]\nfor i in range(0, len(weather_cols_found), 2):\n    pair = weather_cols_found[i:i+2]\n    print(f\"  {' vs '.join(pair)}\")\n\n# Reordenar\ntrain5 = train5[final_order]\n\nprint(\"Â¡Columnas reordenadas!\")\nprint(f\"Primeras 10: {list(train5.columns[:10])}\")\nprint(f\"Clima inicia en posiciÃ³n: {final_order.index(weather_cols_found[0]) if weather_cols_found else 'N/A'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.510760Z","iopub.status.idle":"2025-08-30T18:28:11.511140Z","shell.execute_reply.started":"2025-08-30T18:28:11.510978Z","shell.execute_reply":"2025-08-30T18:28:11.510994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.tail(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.512271Z","iopub.status.idle":"2025-08-30T18:28:11.512594Z","shell.execute_reply.started":"2025-08-30T18:28:11.512445Z","shell.execute_reply":"2025-08-30T18:28:11.512457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hacer scatterplot con \n# train3.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) el alpha para ver con mas claridad las zonas mas poblada\n# podria agregar colores para diferencia temperatura o exposicion a rayos solares o nubes y poder encarar distintos el dataset\n# sino funciona probar %matplotlib inline\n# train3.hist(bins=50, figsize=(20,15)) ver tmb los histograma de los datos cuando este todo junto# cambiar los bins a valores mas chicos\n# plt.show() agregar si hace falta\n# corr_matrix = train3.corr()\n# corr_matrix[\"target\"].sort_values(ascending=False) ver si aplica pero creo que no , recordad probar en columnas numericas si hay otra relacion\n# ejemplo crear columna con raiz/potencia/log y ver como apartan esos valores en correlacion\n# o usar scatter_matrix() de pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.514045Z","iopub.status.idle":"2025-08-30T18:28:11.514537Z","shell.execute_reply.started":"2025-08-30T18:28:11.514385Z","shell.execute_reply":"2025-08-30T18:28:11.514402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# Procesar por chunks de 1M filas\nchunk_size = 1_000_000\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\n# Variables para acumular resultados\nmissing_dates_count = defaultdict(int)\nmissing_by_county = defaultdict(int)\nfirst_missing = None\nlast_complete = None\ntotal_missing = 0\n\nprint(f\"Procesando {total_rows:,} filas en {n_chunks} chunks...\")\n\nweather_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx][['datetime', 'county'] + weather_cols]\n    \n    # Encontrar filas con NaN en clima\n    missing_mask = chunk[weather_cols].isna().any(axis=1)\n    missing_chunk = chunk[missing_mask]\n    \n    if len(missing_chunk) > 0:\n        # Acumular conteos por fecha\n        dates = missing_chunk['datetime'].dt.date.value_counts()\n        for date, count in dates.items():\n            missing_dates_count[date] += count\n        \n        # Acumular por county\n        counties = missing_chunk['county'].value_counts()\n        for county, count in counties.items():\n            missing_by_county[county] += count\n        \n        # Tracking de primera fecha faltante\n        chunk_first_missing = missing_chunk['datetime'].min()\n        if first_missing is None or chunk_first_missing < first_missing:\n            first_missing = chunk_first_missing\n        \n        total_missing += len(missing_chunk)\n    \n    # Tracking de Ãºltima fecha completa\n    complete_chunk = chunk[chunk['latitude'].notna()]\n    if len(complete_chunk) > 0:\n        chunk_last_complete = complete_chunk['datetime'].max()\n        if last_complete is None or chunk_last_complete > last_complete:\n            last_complete = chunk_last_complete\n    \n    # Limpiar memoria\n    del chunk, missing_chunk\n    gc.collect()\n    \n    if i % 5 == 0:  # cada 5 chunks\n        print(f\"  Faltantes encontrados hasta ahora: {total_missing:,}\")\n\n# Mostrar resultados\nprint(f\"\\n=== RESULTADOS ===\")\nprint(f\"Total filas con clima faltante: {total_missing:,}\")\nprint(f\"Total fechas afectadas: {len(missing_dates_count)}\")\n\nif missing_dates_count:\n    # Convertir a Series para ordenar\n    missing_dates_series = pd.Series(missing_dates_count).sort_index()\n    \n    print(f\"\\nPrimeras 10 fechas con mÃ¡s faltantes:\")\n    print(missing_dates_series.head(10))\n    \n    print(f\"\\nRango de fechas problemÃ¡ticas:\")\n    print(f\"Desde: {missing_dates_series.index.min()}\")\n    print(f\"Hasta: {missing_dates_series.index.max()}\")\n    \n    print(f\"\\nTop 5 counties con mÃ¡s faltantes:\")\n    county_series = pd.Series(missing_by_county).sort_values(ascending=False)\n    print(county_series.head())\n    \n    print(f\"\\nTimeline:\")\n    print(f\"Ãšltima fecha completa: {last_complete}\")\n    print(f\"Primera fecha faltante: {first_missing}\")\n    if last_complete and first_missing:\n        print(f\"Gap de tiempo: {first_missing - last_complete}\")\nelse:\n    print(\"No se encontraron datos de clima faltantes!\")\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.516674Z","iopub.status.idle":"2025-08-30T18:28:11.517534Z","shell.execute_reply.started":"2025-08-30T18:28:11.517256Z","shell.execute_reply":"2025-08-30T18:28:11.517283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hago drop de los faltantes\n\n# Eliminar filas con datos de clima faltantes\nprint(f\"Dataset original: {len(train5):,} filas\")\n\n# Drop filas con NaN en clima\nweather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\ntrain5 = train5.dropna(subset=weather_key_cols)\n\nprint(f\"Dataset limpio: {len(train5):,} filas\")\nprint(f\"Filas eliminadas: {32_965_048 - len(train5):,}\")\n\n# Verificar que no quedan NaN en clima\nremaining_weather_nan = train5[weather_key_cols].isna().sum().sum()\nprint(f\"NaN restantes en clima: {remaining_weather_nan}\")\n\n# Verificar fechas disponibles para el perÃ­odo de predicciÃ³n\ntest_period = train5[\n    (train5['datetime'].dt.date >= pd.to_datetime('2023-05-20').date()) &\n    (train5['datetime'].dt.date <= pd.to_datetime('2023-05-31').date())\n]['datetime'].dt.date.unique()\n\nprint(f\"\\nFechas disponibles cerca del perÃ­odo de test:\")\nprint(sorted(test_period))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.518830Z","iopub.status.idle":"2025-08-30T18:28:11.519295Z","shell.execute_reply.started":"2025-08-30T18:28:11.519014Z","shell.execute_reply":"2025-08-30T18:28:11.519037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.521229Z","iopub.status.idle":"2025-08-30T18:28:11.521747Z","shell.execute_reply.started":"2025-08-30T18:28:11.521536Z","shell.execute_reply":"2025-08-30T18:28:11.521554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.523106Z","iopub.status.idle":"2025-08-30T18:28:11.523803Z","shell.execute_reply.started":"2025-08-30T18:28:11.523342Z","shell.execute_reply":"2025-08-30T18:28:11.523361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.526271Z","iopub.status.idle":"2025-08-30T18:28:11.526737Z","shell.execute_reply.started":"2025-08-30T18:28:11.526520Z","shell.execute_reply":"2025-08-30T18:28:11.526539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Inicio de guia para hacer Pipeline\n#Voy pegando los codigos de lo que fui aplicando asi podes usarlo de guia\n#quiero poder aplicarlo a Train y a Test ,\n#si hay diferencias con test y queres hacer Un Pipeline para Train y otro Pipeline para Test para que sea mas simple , me da igual\n#esto termina teniendo 33kk de filas en el final ,asi que tene en cuenta opciones que vayan haciendo de a 1kk para no saturar la memoria , o ir borrando\n\n#fui poniendo los pasos que fui haciendo , obviamente hay que optimizarlo y hay cosas que no son necesarias\n#me parece que para train podria ser en 3 partes , primero preparar todo hasta antes del ultimo merge , luego el merge y tercero encargarse de los problemas del merge\n#se que para trabajar luego deberia haber otro pipeline para usar standarcaler y otras cosas mas , pero quiero dejarlo hasta aca para ver por mi mismo \n# de que forma continuar\n\n\n\n\n#1er paso el weather to county que utilizo en train y luego voy a utilizar en test\n\nimport unicodedata\nfrom sklearn.neighbors import NearestNeighbors\n\n# === 1) Diccionarios canonicos ===\ncounty_id_to_name_map = {\n    0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"JÃ„RVAMAA\",\n    4: \"JÃ•GEVAMAA\", 5: \"LÃ„Ã„NE-VIRUMAA\", 6: \"LÃ„Ã„NEMAA\", 7: \"PÃ„RNUMAA\",\n    8: \"PÃ•LVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n    12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"VÃ•RUMAA\"\n}\ncounty_name_to_id_map = {v: k for k, v in county_id_to_name_map.items()}\n\n# Para normalizar entradas (sin/ con acentos, \"County\"/\"maakond\", etc.)\nsynonyms_to_canonical = {\n    # clave \"base\" (sin acentos/variantes) -> canon con acentos\n    \"HARJU\": \"HARJUMAA\",\n    \"HIIU\": \"HIIUMAA\",\n    \"IDA-VIRU\": \"IDA-VIRUMAA\",\n    \"JARVA\": \"JÃ„RVAMAA\", \"JÃ„RVA\": \"JÃ„RVAMAA\",\n    \"JOGEVA\": \"JÃ•GEVAMAA\", \"JÃ•GEVA\": \"JÃ•GEVAMAA\",\n    \"LAANE-VIRU\": \"LÃ„Ã„NE-VIRUMAA\", \"LÃ„Ã„NE-VIRU\": \"LÃ„Ã„NE-VIRUMAA\",\n    \"LAANE\": \"LÃ„Ã„NEMAA\", \"LÃ„Ã„NE\": \"LÃ„Ã„NEMAA\",\n    \"PARNU\": \"PÃ„RNUMAA\", \"PÃ„RNU\": \"PÃ„RNUMAA\",\n    \"POLVA\": \"PÃ•LVAMAA\", \"PÃ•LVA\": \"PÃ•LVAMAA\",\n    \"RAPLA\": \"RAPLAMAA\",\n    \"SAARE\": \"SAAREMAA\",\n    \"TARTU\": \"TARTUMAA\",\n    \"VALGA\": \"VALGAMAA\",\n    \"VILJANDI\": \"VILJANDIMAA\",\n    \"VORU\": \"VÃ•RUMAA\", \"VÃ•RU\": \"VÃ•RUMAA\",\n\n    # versiones ya canÃ³nicas\n    \"HARJUMAA\": \"HARJUMAA\",\n    \"HIIUMAA\": \"HIIUMAA\",\n    \"IDA-VIRUMAA\": \"IDA-VIRUMAA\",\n    \"JÃ„RVAMAA\": \"JÃ„RVAMAA\",\n    \"JÃ•GEVAMAA\": \"JÃ•GEVAMAA\",\n    \"LÃ„Ã„NE-VIRUMAA\": \"LÃ„Ã„NE-VIRUMAA\",\n    \"LÃ„Ã„NEMAA\": \"LÃ„Ã„NEMAA\",\n    \"PÃ„RNUMAA\": \"PÃ„RNUMAA\",\n    \"PÃ•LVAMAA\": \"PÃ•LVAMAA\",\n    \"RAPLAMAA\": \"RAPLAMAA\",\n    \"SAAREMAA\": \"SAAREMAA\",\n    \"TARTUMAA\": \"TARTUMAA\",\n    \"VALGAMAA\": \"VALGAMAA\",\n    \"VILJANDIMAA\": \"VILJANDIMAA\",\n    \"VÃ•RUMAA\": \"VÃ•RUMAA\",\n}\n\ndef strip_accents(s: str) -> str:\n    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n\ndef normalize_county_name(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).upper().strip()\n    # eliminar sufijos frecuentes\n    for suf in [\" COUNTY\", \" MAAKOND\"]:\n        if s.endswith(suf):\n            s = s[: -len(suf)]\n    s = s.replace(\"  \", \" \").replace(\"â€“\", \"-\")  # guion raro -> normal\n    s_noacc = strip_accents(s)\n    s_noacc = s_noacc.replace(\"  \", \" \")\n    # prueba de match exacto o por prefijo lÃ³gico\n    # ej. \"HARJU\", \"HARJU MAAKOND\" -> \"HARJU\"\n    token = s_noacc.split()[0]  # primer palabra alcanza en estos casos\n    # casos con guion (IDA-VIRU, LAA NE-VIRU, etc.)\n    if \"-\" in s_noacc:\n        token = s_noacc  # mantener completo en compuestos\n\n    # intento 1: match directo con cadena completa sin acentos\n    if s_noacc in synonyms_to_canonical:\n        return synonyms_to_canonical[s_noacc]\n    # intento 2: match por token/prefijo conocido\n    if token in synonyms_to_canonical:\n        return synonyms_to_canonical[token]\n    # intento 3: si ya vino canÃ³nico exacto (con acentos)\n    if s in synonyms_to_canonical:\n        return synonyms_to_canonical[s]\n\n    return np.nan  # no lo reconozco\n\n# === 2) Normalizar lo existente sin romper los NaN ===\n# (NO convertir NaN a \"NAN\")\nweather_station1['county_name_norm'] = weather_station1['county_name'].apply(normalize_county_name)\n\n# Si tengo 'county' numÃ©rico pero falta nombre, lo inferimos del ID\nmask = weather_station1['county_name_norm'].isna() & weather_station1['county'].notna()\nweather_station1.loc[mask, 'county_name_norm'] = (\n    weather_station1.loc[mask, 'county'].astype(int).map(county_id_to_name_map)\n)\n\n# === 3) Completar faltantes por k-NN (lat/lon) ===\nfcols = ['latitude', 'longitude']\nknown = weather_station1.dropna(subset=['county_name_norm']).copy()\nunknown = weather_station1[weather_station1['county_name_norm'].isna()].copy()\n\nif not unknown.empty and not known.empty:\n    # k-NN en (lat, lon) usando Haversine (necesita radianes)\n    # ojo: haversine en sklearn requiere [lat, lon] en radianes\n    known_rad = np.radians(known[fcols].values)\n    unknown_rad = np.radians(unknown[fcols].values)\n\n    nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n    nbrs.fit(known_rad)\n    distances, idxs = nbrs.kneighbors(unknown_rad)\n\n    # votaciÃ³n simple de los 3 vecinos (majority vote)\n    nn_names = known['county_name_norm'].values\n    filled = []\n    for neigh_idx in idxs:\n        cands = nn_names[neigh_idx]\n        # mayorÃ­a; en empate, tomar el 1er vecino\n        vals, counts = np.unique(cands, return_counts=True)\n        filled.append(vals[np.argmax(counts)])\n\n    weather_station1.loc[unknown.index, 'county_name_norm'] = filled\n\n# === 4) Sincronizar columnas finales ===\nweather_station1['county_name'] = weather_station1['county_name_norm']\nweather_station1['county'] = weather_station1['county_name'].map(county_name_to_id_map).astype('Int64')\n\n# Cualquier remanente a UNKNOWN (12)\nweather_station1.loc[weather_station1['county_name'].isna(), 'county'] = 12\nweather_station1['county'] = weather_station1['county'].astype('Int64')\n\n# Limpieza\nweather_station1.drop(columns=['county_name_norm'], inplace=True)\n\n# Chequeo rÃ¡pido\nprint(weather_station1[['county_name','longitude','latitude','county']].head(25))\nprint(\"\\nFaltantes aÃºn:\", weather_station1['county_name'].isna().sum())\n\n\n#2do paso crear copias antes de empezar a juntar/merge los df , las cuales elimino al final para liberar memoria\n# en algunos caso elimino los na con drop , en otros no hace falta respetar como esta aca \n# en test no hay historical porque estoy prediciendo a futuro \n# weather_station2 es asi por lo que hice arriba\n\ntrain1 = train.dropna()# train                 \nclient1 = client.copy() # client               \nelectricity_prices1 = electricity_prices.copy() # electricity_prices  \nforecast_weather1 = forecast_weather.dropna() # forecast_weather    \ngas_prices1 = gas_prices.copy() # gas_prices                        \nhistorical_weather1 = historical_weather.copy() # historical_weather    \nweather_station2 = weather_station1.copy()      \n\ntest1 = test.copy() # test                 \nclient_t = client_t.copy() # client_t              \nelectricity_prices_t1 = electricity_prices_t.copy() # electricity_prices_t  \nforecast_weather_t1 = forecast_weather_t.copy() # forecast_weather_t    \ngas_prices_t1 = gas_prices_t.copy() # gas_prices_t          \nrevealed_targets1 = revealed_targets.copy() # revealed_targets\n\n#3er paso trabajo con train1 y gas_prices1 para hacer el primer merge\n\n# el forecast de gas es de 24 horas\n\n# Convertir datetime y crear columnas separadas en train1\ntrain1['datetime'] = pd.to_datetime(train1['datetime'])\ntrain1['hour'] = train1['datetime'].dt.hour\ntrain1['forecast_date'] = train1['datetime']\n\n#\ngas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\ngas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\ngas_prices1['hour'] = gas_prices1['forecast_date'].dt.hour\ngas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n\n# Expandir gas_prices1 a nivel horario - solo columnas necesarias\ngas_hourly = []\nfor _, row in gas_prices1.iterrows():\n    for hour in range(24):\n        new_row = {\n            'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n            'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n            'highest_price_per_mwh': row['highest_price_per_mwh'],\n            'gas_origin_date': row['gas_origin_date']\n        }\n        gas_hourly.append(new_row)\n\ngas_prices1_hourly = pd.DataFrame(gas_hourly)\n\n# Merge train1 con gas_prices1_hourly\ntrain2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')\n# completo los datos faltantes antes de continuar\ntrain2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\ntrain2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n\n# Eliminar columna gas_origin_date\ntrain2 = train2.drop(columns=['gas_origin_date'])\n\n# ahora se llama train2 que es train1+gas_prices1\n# a \"gas_origin_date\" despues lo elimino , si lees todo el pipeline y cuando lo corrijas ves que es mejor eliminarlo ahora , hacelo ahora \n\n\n\n#3er paso\n\n\n# Preparar electricity_prices1 para el merge con train2\nelectricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\nelectricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n\n# Extraer componentes de tiempo\nelectricity_prices1['hour'] = electricity_prices1['forecast_date'].dt.hour\n\n\n# Merge\nelec_columns = ['forecast_date', 'euros_per_mwh']\ntrain3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')\n\n# me ocupo de los valores faltantes antes de continuar\n\ntrain3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n\n\n#4to paso , preparo client1 para unirlo con train3\n\n# Preparar client1\nclient1['date'] = pd.to_datetime(client1['date']).dt.date\n\n# Merge con client1 usando datetime de train3\ntrain4 = train3.merge(client1.drop('data_block_id', axis=1),\n                      left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                      right_on=['product_type', 'county', 'is_business', 'date'],\n                      how='left')\n\n# Limpiar\ntrain4 = train4.drop('date', axis=1)\ntrain4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})\n\n# me ocupo de los valores faltantes antes de continuar\n# decido completar asi los ultimos 2 dias que faltan\n# Verificar el problema\nprint(\"NaN en datos de clientes:\")\nprint(f\"eic_count: {train4['eic_count'].isna().sum():,}\")\nprint(f\"installed_capacity: {train4['installed_capacity'].isna().sum():,}\")\n\n# Ver las fechas con problemas\nnan_dates = train4[train4['eic_count'].isna()]['datetime'].dt.date.unique()\nprint(f\"Fechas con NaN: {sorted(nan_dates)}\")\n\n# Ver Ãºltimo dÃ­a con datos\nlast_valid_date = train4[train4['eic_count'].notna()]['datetime'].dt.date.max()\nprint(f\"Ãšltimo dÃ­a con datos: {last_valid_date}\")\n\n# Forward fill por grupo (county + is_business + product_type)\n# Esto mantiene los Ãºltimos valores conocidos para cada combinaciÃ³n\nprint(\"\\nCompletando con forward fill por grupo...\")\n\n# Ordenar por datetime para asegurar orden correcto\ntrain4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n\n# Forward fill por grupo\ntrain4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\ntrain4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n\n# Verificar resultado\nprint(f\"\\nDespuÃ©s del forward fill:\")\nprint(f\"eic_count NaN: {train4['eic_count'].isna().sum()}\")\nprint(f\"installed_capacity NaN: {train4['installed_capacity'].isna().sum()}\")\n\n# Si aÃºn quedan NaN, son combinaciones nuevas - usar 0 o promedio\nremaining_nan = train4['eic_count'].isna().sum()\nif remaining_nan > 0:\n    print(f\"\\n{remaining_nan} NaN restantes (combinaciones nuevas)\")\n    print(\"Opciones:\")\n    print(\"1. Rellenar con 0\")\n    print(\"2. Rellenar with promedio del county\")\n    \n    # OpciÃ³n recomendada: promedio por county\n    train4['eic_count'] = train4['eic_count'].fillna(\n        train4.groupby('county')['eic_count'].transform('mean')\n    )\n    train4['installed_capacity'] = train4['installed_capacity'].fillna(\n        train4.groupby('county')['installed_capacity'].transform('mean')\n    )\n    \n    print(f\"NaN finales - eic_count: {train4['eic_count'].isna().sum()}\")\n    print(f\"NaN finales - installed_capacity: {train4['installed_capacity'].isna().sum()}\")\n\nprint(f\"\\nDataset train4: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")\n\n# 5to paso antes de continuar con train4 sigo con los df relacionados al clima , junto forecast_weather1 con weather station1 ,\n\n# Merge forecast_weather1 con weather_station1 por coordenadas\nforecast_weather1 = forecast_weather1.merge(weather_station3[['latitude', 'longitude', 'county', 'county_name']], \n                                          on=['latitude', 'longitude'], \n                                          how='left')\n\nprint(f\"Filas con county asignado: {forecast_weather1['county'].notna().sum()}\")\nprint(f\"Total filas: {len(forecast_weather1)}\")\n\n# completo los valores faltantes\n\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\n\n# Identificar filas sin county asignado\nmissing_county = forecast_weather1['county'].isna()\nprint(f\"Filas sin county: {missing_county.sum()}\")\n\nif missing_county.sum() > 0:\n   # Datos conocidos (con county)\n   known = weather_station1[['latitude', 'longitude', 'county', 'county_name']].copy()\n   \n   # Datos faltantes\n   missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n   \n   # K-NN con distancia euclidiana (para coordenadas pequeÃ±as como Estonia)\n   nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n   nbrs.fit(known[['latitude', 'longitude']].values)\n   \n   # Encontrar vecino mÃ¡s cercano\n   distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n   \n   # Asignar county y county_name del vecino mÃ¡s cercano\n   nearest_counties = known.iloc[indices.flatten()]\n   \n   forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n   forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n\nprint(f\"Filas con county despuÃ©s del K-NN: {forecast_weather1['county'].notna().sum()}\")\n\n#6to paso trabajo sobre forecast_weather \n\nforecast_weather2 = forecast_weather1.copy()\n\n# Eliminar columnas innecesarias\nforecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n\n\n# Convertir forecast_datetime a datetime\nforecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n\n# Crear columnas temporales\nforecast_weather2['forecast_date'] = forecast_weather2['forecast_datetime'].dt.date\nforecast_weather2['hour'] = forecast_weather2['forecast_datetime'].dt.hour\n\n# Renombrar variables con 'f' adelante (excepto las especificadas)\nrename_dict = {}\nfor col in forecast_weather2.columns:\n   if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                  'forecast_datetime', 'forecast_date', 'hour']:\n       rename_dict[col] = 'f' + col\n\nforecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n\nprint(\"Columnas despuÃ©s de transformaciones:\")\nprint(forecast_weather2.columns.tolist())\n\nforecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\":\"weather_forecast_hour\"})\n\n#7mo paso preparo y hago merge con historical weather\n\nhistorical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n\n# Renombrar y merge\nforecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n\nmerged_weather = pd.merge(\n    forecast_weather2,\n    historical_weather1,\n    left_on=['latitude', 'longitude', 'forecast_datetime'],\n    right_on=['latitude', 'longitude', 'datetime'],\n    how='inner'\n)\n\n#8vo paso , optimizo datos por un tema de memoria antes de continuar\n\n# Optimizar tipos de datos\ntrain4['county'] = train4['county'].astype('uint8')  # 0-15\ntrain4['is_business'] = train4['is_business'].astype('uint8')  # 0-1\ntrain4['product_type'] = train4['product_type'].astype('uint8')  # 0-3\ntrain4['is_consumption'] = train4['is_consumption'].astype('uint8')  # 0-1\ntrain4['hour'] = train4['hour'].astype('uint8')  # 0-23\n\n# IDs optimizados\ntrain4['data_block_id'] = train4['data_block_id'].astype('uint16')  # 0-700\ntrain4['row_id'] = train4['row_id'].astype('uint32')  # 0-3M\ntrain4['prediction_unit_id'] = train4['prediction_unit_id'].astype('uint8')  # 0-68\n\n# Flotantes a float32 (suficiente precision para los rangos dados)\ntrain4['target'] = train4['target'].astype('float32')\ntrain4['lowest_price_per_mwh'] = train4['lowest_price_per_mwh'].astype('float32')\ntrain4['highest_price_per_mwh'] = train4['highest_price_per_mwh'].astype('float32')\ntrain4['euros_per_mwh'] = train4['euros_per_mwh'].astype('float32')\ntrain4['eic_count'] = train4['eic_count'].astype('float32')\ntrain4['installed_capacity'] = train4['installed_capacity'].astype('float32')\n\nprint(\"OptimizaciÃ³n completada\")\nprint(train4.info(memory_usage='deep'))\n\n# optimizo y preparo para merge con train4 \n\n# Eliminar columnas innecesarias\nmerged_weather = merged_weather.drop('f_data_block_id', axis=1)\n\n# Coordenadas - float32 suficiente para la precisiÃ³n geogrÃ¡fica\nmerged_weather['latitude'] = merged_weather['latitude'].astype('float32')  # 57.6-59.7\nmerged_weather['longitude'] = merged_weather['longitude'].astype('float32')  # 21.7-28.2\n\n# Enteros pequeÃ±os\nmerged_weather['weather_forecast_hour'] = merged_weather['weather_forecast_hour'].astype('uint8')  # 1-48\nmerged_weather['county'] = merged_weather['county'].astype('uint8')  # 0-15\nmerged_weather['hour'] = merged_weather['hour'].astype('uint8')  # 0-23\n\n# IDs\nmerged_weather['data_block_id'] = merged_weather['data_block_id'].astype('uint16')  # 1-637\n\n# Variables meteorolÃ³gicas - float32 es suficiente\nweather_float_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                     'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                     'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                     'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                     'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                     'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                     'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                     'direct_solar_radiation', 'diffuse_radiation']\n\nfor col in weather_float_cols:\n    merged_weather[col] = merged_weather[col].astype('float32')\n\nprint(\"OptimizaciÃ³n de merged_weather completada\")\nprint(merged_weather.info(memory_usage='deep'))\n\nmerged_weather = merged_weather.drop('forecast_datetime', axis=1)\n\n\n#9no paso , defino funciones y luego merge de a chunks por un tema de memoria\n\ndef process_weather_chunks(start_chunk, end_chunk):\n    periods = [\n        ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), ('2021-12-01', '2022-01-01'),\n        ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'),\n        ('2022-05-01', '2022-06-01'), ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n        ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), ('2022-12-01', '2023-01-01'),\n        ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'),\n        ('2023-05-01', '2023-05-31')\n    ]\n    \n    for i in range(start_chunk, end_chunk):\n        start_date, end_date = periods[i]\n        print(f\"Procesando perÃ­odo {i+1}: {start_date} a {end_date}\")\n        \n        # Filtrar por perÃ­odo\n        mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n        train4_chunk = train4[mask_train].copy()\n        \n        mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n        weather_chunk = merged_weather[mask_weather].copy()\n        \n        # Merge y guardar\n        chunk_result = pd.merge(train4_chunk, weather_chunk, on='datetime', how='left')\n        chunk_result.to_parquet(f'train5_chunk_{i+1}.parquet', index=False)\n        print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n        \n        # Liberar memoria\n        del train4_chunk, weather_chunk, chunk_result\n\n# lo hice asi para no saturar\nprocess_weather_chunks_corrected(0, 3)\n\nprocess_weather_chunks_corrected(3, 9)\n\nprocess_weather_chunks_corrected(9, 15)\n\nprocess_weather_chunks_corrected(15, 21)\n\n\n#junto todo\n\n# Concatenar de a grupos para evitar saturar memoria\ndef concat_chunks_by_groups(group_size=1):\n    group_files = []\n    \n    # Crear grupos intermedios\n    for group_start in range(1, 22, group_size):\n        group_end = min(group_start + group_size, 22)\n        print(f\"Procesando grupo: chunks {group_start} a {group_end-1}\")\n        \n        # Leer chunks del grupo\n        group_chunks = []\n        for i in range(group_start, group_end):\n            try:\n                chunk = pd.read_parquet(f'train5_corrected_chunk_{i}.parquet')  # CORREGIDO\n                group_chunks.append(chunk)\n            except FileNotFoundError:\n                print(f\"Archivo train5_corrected_chunk_{i}.parquet no encontrado\")\n        \n        # Concatenar grupo y guardar\n        if group_chunks:\n            group_df = pd.concat(group_chunks, ignore_index=True)\n            group_filename = f'train5_group_{group_start}_{group_end-1}.parquet'\n            group_df.to_parquet(group_filename, index=False)\n            group_files.append(group_filename)\n            print(f\"Grupo guardado: {group_filename} - Shape: {group_df.shape}\")\n            \n            # Liberar memoria\n            del group_chunks, group_df\n    \n    # Ahora concatenar los grupos (serÃ¡n muchos menos archivos)\n    print(\"Concatenando grupos finales...\")\n    final_chunks = []\n    for group_file in group_files:\n        group_data = pd.read_parquet(group_file)\n        final_chunks.append(group_data)\n    \n    train5 = pd.concat(final_chunks, ignore_index=True)\n    return train5\n\n# Ejecutar\ntrain5 = concat_chunks_by_groups(1)\nprint(f\"Dataset final: {train5.shape}\")\n\n#10mo paso , trabajo los problemas generados del merge\n\nimport gc\n\n# 1. Identificar columnas a eliminar y renombrar SIN cargar datos\nsample = train5.head(1000)  # Solo una muestra\ncols_to_drop = [col for col in sample.columns if col.endswith('_y')]\nrename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\nprint(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n\n# 2. Procesar por chunks de 1M filas\nchunk_size = 1_000_000\nchunks_processed = []\n\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Procesando chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx].copy()\n    \n    # Filtrar county != 12\n    chunk = chunk[chunk['county'] != 12]\n    \n    if len(chunk) > 0:  # Solo si quedan filas\n        # Eliminar columnas _y\n        chunk = chunk.drop(columns=cols_to_drop)\n        \n        # Renombrar _x\n        chunk.rename(columns=rename_dict, inplace=True)\n        \n        chunks_processed.append(chunk)\n    \n    # Limpiar memoria\n    del chunk\n    gc.collect()\n    \n    print(f\"Memoria liberada, chunks guardados: {len(chunks_processed)}\")\n\n# 3. Concatenar chunks finales\nprint(\"Concatenando chunks finales...\")\ntrain5_clean = pd.concat(chunks_processed, ignore_index=True)\n\n# 4. Limpiar\ndel chunks_processed, train5\ngc.collect()\n\nprint(f\"Dataset final: {train5_clean.shape[0]:,} filas y {train5_clean.shape[1]} columnas\")\n\n# Renombrar para usar el mismo nombre\ntrain5 = train5_clean\ndel train5_clean\ngc.collect()\n\n# reordeno y veo que problemas siguen estando\n\n# Orden especÃ­fico que solicitaste\nmain_cols = [\n    'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n    'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n    'longitude', 'lowest_price_per_mwh', \n    'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', 'installed_capacity'\n]\n\n# Pares de clima: pronÃ³stico (f) vs histÃ³rico - lado a lado para comparar\nweather_pairs = [\n    # Temperatura\n    'ftemperature', 'temperature',\n    \n    # Punto de rocÃ­o  \n    'fdewpoint', 'dewpoint',\n    \n    # Cobertura de nubes\n    'fcloudcover_high', 'cloudcover_high',\n    'fcloudcover_low', 'cloudcover_low', \n    'fcloudcover_mid', 'cloudcover_mid',\n    'fcloudcover_total', 'cloudcover_total',\n    \n    # Viento\n    'f10_metre_u_wind_component', 'windspeed_10m',\n    'f10_metre_v_wind_component', 'winddirection_10m',\n    \n    # RadiaciÃ³n solar\n    'fdirect_solar_radiation', 'direct_solar_radiation',\n    'fsurface_solar_radiation_downwards', 'shortwave_radiation',\n    'diffuse_radiation',  # solo histÃ³rico\n    \n    # PrecipitaciÃ³n\n    'fsnowfall', 'snowfall',\n    'ftotal_precipitation', 'rain',\n    \n    # Otras variables climÃ¡ticas\n    'surface_pressure',  # solo histÃ³rico\n    'weather_forecast_hour'\n]\n\n# Construir orden final\nfinal_order = main_cols.copy()\n\n# Agregar solo las columnas que existen\nfor col in weather_pairs:\n    if col in train5.columns:\n        final_order.append(col)\n\n# Agregar cualquier columna restante\nremaining_cols = [col for col in train5.columns if col not in final_order]\nfinal_order.extend(remaining_cols)\n\nprint(f\"Reordenando {len(final_order)} columnas...\")\nprint(\"Pares clima encontrados:\")\nweather_cols_found = [col for col in weather_pairs if col in train5.columns]\nfor i in range(0, len(weather_cols_found), 2):\n    pair = weather_cols_found[i:i+2]\n    print(f\"  {' vs '.join(pair)}\")\n\n# Reordenar\ntrain5 = train5[final_order]\n\nprint(\"Â¡Columnas reordenadas!\")\nprint(f\"Primeras 10: {list(train5.columns[:10])}\")\nprint(f\"Clima inicia en posiciÃ³n: {final_order.index(weather_cols_found[0]) if weather_cols_found else 'N/A'}\")\n\n# re check de los na y lo hago de a chunks\nfrom collections import defaultdict\n\n# Procesar por chunks de 1M filas\nchunk_size = 1_000_000\ntotal_rows = len(train5)\nn_chunks = (total_rows // chunk_size) + 1\n\n# Variables para acumular resultados\nmissing_dates_count = defaultdict(int)\nmissing_by_county = defaultdict(int)\nfirst_missing = None\nlast_complete = None\ntotal_missing = 0\n\nprint(f\"Procesando {total_rows:,} filas en {n_chunks} chunks...\")\n\nweather_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n\nfor i in range(n_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, total_rows)\n    \n    print(f\"Chunk {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n    \n    # Procesar chunk\n    chunk = train5.iloc[start_idx:end_idx][['datetime', 'county'] + weather_cols]\n    \n    # Encontrar filas con NaN en clima\n    missing_mask = chunk[weather_cols].isna().any(axis=1)\n    missing_chunk = chunk[missing_mask]\n    \n    if len(missing_chunk) > 0:\n        # Acumular conteos por fecha\n        dates = missing_chunk['datetime'].dt.date.value_counts()\n        for date, count in dates.items():\n            missing_dates_count[date] += count\n        \n        # Acumular por county\n        counties = missing_chunk['county'].value_counts()\n        for county, count in counties.items():\n            missing_by_county[county] += count\n        \n        # Tracking de primera fecha faltante\n        chunk_first_missing = missing_chunk['datetime'].min()\n        if first_missing is None or chunk_first_missing < first_missing:\n            first_missing = chunk_first_missing\n        \n        total_missing += len(missing_chunk)\n    \n    # Tracking de Ãºltima fecha completa\n    complete_chunk = chunk[chunk['latitude'].notna()]\n    if len(complete_chunk) > 0:\n        chunk_last_complete = complete_chunk['datetime'].max()\n        if last_complete is None or chunk_last_complete > last_complete:\n            last_complete = chunk_last_complete\n    \n    # Limpiar memoria\n    del chunk, missing_chunk\n    gc.collect()\n    \n    if i % 5 == 0:  # cada 5 chunks\n        print(f\"  Faltantes encontrados hasta ahora: {total_missing:,}\")\n\n# Mostrar resultados\nprint(f\"\\n=== RESULTADOS ===\")\nprint(f\"Total filas con clima faltante: {total_missing:,}\")\nprint(f\"Total fechas afectadas: {len(missing_dates_count)}\")\n\nif missing_dates_count:\n    # Convertir a Series para ordenar\n    missing_dates_series = pd.Series(missing_dates_count).sort_index()\n    \n    print(f\"\\nPrimeras 10 fechas con mÃ¡s faltantes:\")\n    print(missing_dates_series.head(10))\n    \n    print(f\"\\nRango de fechas problemÃ¡ticas:\")\n    print(f\"Desde: {missing_dates_series.index.min()}\")\n    print(f\"Hasta: {missing_dates_series.index.max()}\")\n    \n    print(f\"\\nTop 5 counties con mÃ¡s faltantes:\")\n    county_series = pd.Series(missing_by_county).sort_values(ascending=False)\n    print(county_series.head())\n    \n    print(f\"\\nTimeline:\")\n    print(f\"Ãšltima fecha completa: {last_complete}\")\n    print(f\"Primera fecha faltante: {first_missing}\")\n    if last_complete and first_missing:\n        print(f\"Gap de tiempo: {first_missing - last_complete}\")\nelse:\n    print(\"No se encontraron datos de clima faltantes!\")\n\ngc.collect()\n\n## hago drop de los faltantes\n\n# Eliminar filas con datos de clima faltantes\nprint(f\"Dataset original: {len(train5):,} filas\")\n\n# Drop filas con NaN en clima\nweather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\ntrain5 = train5.dropna(subset=weather_key_cols)\n\nprint(f\"Dataset limpio: {len(train5):,} filas\")\nprint(f\"Filas eliminadas: {32_965_048 - len(train5):,}\")\n\n# Verificar que no quedan NaN en clima\nremaining_weather_nan = train5[weather_key_cols].isna().sum().sum()\nprint(f\"NaN restantes en clima: {remaining_weather_nan}\")\n\n# Verificar fechas disponibles para el perÃ­odo de predicciÃ³n\ntest_period = train5[\n    (train5['datetime'].dt.date >= pd.to_datetime('2023-05-20').date()) &\n    (train5['datetime'].dt.date <= pd.to_datetime('2023-05-31').date())\n]['datetime'].dt.date.unique()\n\nprint(f\"\\nFechas disponibles cerca del perÃ­odo de test:\")\nprint(sorted(test_period))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.528535Z","iopub.status.idle":"2025-08-30T18:28:11.529011Z","shell.execute_reply.started":"2025-08-30T18:28:11.528780Z","shell.execute_reply":"2025-08-30T18:28:11.528798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.531462Z","iopub.status.idle":"2025-08-30T18:28:11.531902Z","shell.execute_reply.started":"2025-08-30T18:28:11.531699Z","shell.execute_reply":"2025-08-30T18:28:11.531718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.539857Z","iopub.status.idle":"2025-08-30T18:28:11.540343Z","shell.execute_reply.started":"2025-08-30T18:28:11.540150Z","shell.execute_reply":"2025-08-30T18:28:11.540173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T18:28:11.542204Z","iopub.status.idle":"2025-08-30T18:28:11.542918Z","shell.execute_reply.started":"2025-08-30T18:28:11.542687Z","shell.execute_reply":"2025-08-30T18:28:11.542705Z"}},"outputs":[],"execution_count":null}]}