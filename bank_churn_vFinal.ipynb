{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12511437,"sourceType":"datasetVersion","datasetId":7897042}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:19.933558Z","iopub.execute_input":"2025-07-22T20:48:19.933947Z","iopub.status.idle":"2025-07-22T20:48:20.278289Z","shell.execute_reply.started":"2025-07-22T20:48:19.933919Z","shell.execute_reply":"2025-07-22T20:48:20.277517Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bank-churn/sample_submission.csv\n/kaggle/input/bank-churn/trainBCD.csv\n/kaggle/input/bank-churn/testBCD.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# https://www.kaggle.com/competitions/playground-series-s4e1/overview #competicion\n# https://www.kaggle.com/competitions/playground-series-s4e1/data # archivos\n# https://www.kaggle.com/competitions/playground-series-s4e1/leaderboard # leaderboard\n\n\n# CORE DATA MANIPULATION\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import pearsonr, spearmanr\nimport math\n\n# MACHINE LEARNING - SCIKIT-LEARN\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, roc_curve, auc\nfrom sklearn.pipeline import Pipeline\n\n# ADVANCED ML LIBRARIES\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# VISUALIZATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\n\n# CONFIGURATION FOR PLOTS\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n# UTILITIES\nimport os\nimport sys\nimport warnings\nimport itertools\nfrom datetime import datetime, timedelta\nimport time\nfrom collections import Counter\nimport pickle\nimport joblib\n\n# JUPYTER SPECIFIC\nfrom IPython.display import display, HTML\nfrom tqdm.notebook import tqdm\n\n# SUPPRESS WARNINGS\nwarnings.filterwarnings('ignore')\n\n# PANDAS CONFIGURATION\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n# NUMPY CONFIGURATION\nnp.random.seed(42)\n\ndef quick_info(df):\n    \"\"\"Informaci√≥n r√°pida del dataset\"\"\"\n    print(f\"üìä Dataset Shape: {df.shape}\")\n    print(f\"üî¢ Columnas num√©ricas: {df.select_dtypes(include=[np.number]).columns.tolist()}\")\n    print(f\"üìù Columnas categ√≥ricas: {df.select_dtypes(include=['object']).columns.tolist()}\")\n    print(f\"‚ùå Valores nulos por columna:\")\n    print(df.isnull().sum()[df.isnull().sum() > 0])\n\ndef plot_missing_values(df):\n    \"\"\"Visualizar valores faltantes\"\"\"\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    if len(missing) > 0:\n        plt.figure(figsize=(10, 6))\n        missing.plot(kind='bar')\n        plt.title('Valores Faltantes por Columna')\n        plt.ylabel('Cantidad')\n        plt.xticks(rotation=45)\n        plt.show()\n    else:\n        print(\"‚úÖ No hay valores faltantes en el dataset\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:20.279564Z","iopub.execute_input":"2025-07-22T20:48:20.280005Z","iopub.status.idle":"2025-07-22T20:48:46.798726Z","shell.execute_reply.started":"2025-07-22T20:48:20.279983Z","shell.execute_reply":"2025-07-22T20:48:46.797829Z"}},"outputs":[{"name":"stderr","text":"2025-07-22 20:48:31.808371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753217312.071483      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753217312.148287      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow disponible\nLibrer√≠as de procesamiento de im√°genes disponibles\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"name":"stdout","text":"‚úÖ Todas las librer√≠as importadas correctamente!\nüìä Pandas version: 2.2.3\nüî¢ NumPy version: 1.26.4\nü§ñ Scikit-learn version: 1.2.2\nüìà Matplotlib version: 3.7.2\nüé® Seaborn version: 0.12.2\nüéØ Template listo para usar en Kaggle!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/bank-churn/trainBCD.csv\")\ntest = pd.read_csv(\"/kaggle/input/bank-churn/testBCD.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:46.799538Z","iopub.execute_input":"2025-07-22T20:48:46.799858Z","iopub.status.idle":"2025-07-22T20:48:47.426385Z","shell.execute_reply.started":"2025-07-22T20:48:46.799835Z","shell.execute_reply":"2025-07-22T20:48:47.425327Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.428258Z","iopub.execute_input":"2025-07-22T20:48:47.428516Z","iopub.status.idle":"2025-07-22T20:48:47.458865Z","shell.execute_reply.started":"2025-07-22T20:48:47.428496Z","shell.execute_reply":"2025-07-22T20:48:47.458073Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id  CustomerId         Surname  CreditScore Geography Gender   Age  Tenure  \\\n0   0    15674932  Okwudilichukwu          668    France   Male  33.0       3   \n1   1    15749177   Okwudiliolisa          627    France   Male  33.0       1   \n2   2    15694510           Hsueh          678    France   Male  40.0      10   \n3   3    15741417             Kao          581    France   Male  34.0       2   \n4   4    15766172       Chiemenam          716     Spain   Male  33.0       5   \n\n     Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n0       0.00              2        1.0             0.0        181449.97   \n1       0.00              2        1.0             1.0         49503.50   \n2       0.00              2        1.0             0.0        184866.69   \n3  148882.54              1        1.0             1.0         84560.88   \n4       0.00              2        1.0             1.0         15068.83   \n\n   Exited  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>15674932</td>\n      <td>Okwudilichukwu</td>\n      <td>668</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>3</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>181449.97</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>15749177</td>\n      <td>Okwudiliolisa</td>\n      <td>627</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>49503.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>15694510</td>\n      <td>Hsueh</td>\n      <td>678</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>40.0</td>\n      <td>10</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>184866.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>15741417</td>\n      <td>Kao</td>\n      <td>581</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>34.0</td>\n      <td>2</td>\n      <td>148882.54</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>84560.88</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>15766172</td>\n      <td>Chiemenam</td>\n      <td>716</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>5</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>15068.83</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"quick_info(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.459830Z","iopub.execute_input":"2025-07-22T20:48:47.460095Z","iopub.status.idle":"2025-07-22T20:48:47.535164Z","shell.execute_reply.started":"2025-07-22T20:48:47.460075Z","shell.execute_reply":"2025-07-22T20:48:47.534238Z"}},"outputs":[{"name":"stdout","text":"üìä Dataset Shape: (165034, 14)\nüî¢ Columnas num√©ricas: ['id', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\nüìù Columnas categ√≥ricas: ['Surname', 'Geography', 'Gender']\n‚ùå Valores nulos por columna:\nSeries([], dtype: int64)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Para empezar deberia eliminar lo que no me sirve:\"Customerid\",\"Surname\"\n# \"id\" lo guardo porque es necesario en el submission de test\n# hacer one-hot encoding con \"Geography\",\"Gender\"\n# aplicar logaritmo a \"EstimatedSalary\" y a \"Balance\"\n# esta transformacion es muy poco probable que empeore el resultado , como mucho lo mejora de forma nula o poca\n# podria ver analisis de varianzas y correlaciones pero es preferible empezar con un RandomForest para saber como estamos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.536128Z","iopub.execute_input":"2025-07-22T20:48:47.536473Z","iopub.status.idle":"2025-07-22T20:48:47.539993Z","shell.execute_reply.started":"2025-07-22T20:48:47.536452Z","shell.execute_reply":"2025-07-22T20:48:47.539278Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.540799Z","iopub.execute_input":"2025-07-22T20:48:47.541134Z","iopub.status.idle":"2025-07-22T20:48:47.664222Z","shell.execute_reply.started":"2025-07-22T20:48:47.541108Z","shell.execute_reply":"2025-07-22T20:48:47.663303Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                id    CustomerId    CreditScore            Age         Tenure  \\\ncount  165034.0000  1.650340e+05  165034.000000  165034.000000  165034.000000   \nmean    82516.5000  1.569201e+07     656.454373      38.125888       5.020353   \nstd     47641.3565  7.139782e+04      80.103340       8.867205       2.806159   \nmin         0.0000  1.556570e+07     350.000000      18.000000       0.000000   \n25%     41258.2500  1.563314e+07     597.000000      32.000000       3.000000   \n50%     82516.5000  1.569017e+07     659.000000      37.000000       5.000000   \n75%    123774.7500  1.575682e+07     710.000000      42.000000       7.000000   \nmax    165033.0000  1.581569e+07     850.000000      92.000000      10.000000   \n\n             Balance  NumOfProducts      HasCrCard  IsActiveMember  \\\ncount  165034.000000  165034.000000  165034.000000   165034.000000   \nmean    55478.086689       1.554455       0.753954        0.497770   \nstd     62817.663278       0.547154       0.430707        0.499997   \nmin         0.000000       1.000000       0.000000        0.000000   \n25%         0.000000       1.000000       1.000000        0.000000   \n50%         0.000000       2.000000       1.000000        0.000000   \n75%    119939.517500       2.000000       1.000000        1.000000   \nmax    250898.090000       4.000000       1.000000        1.000000   \n\n       EstimatedSalary         Exited  \ncount    165034.000000  165034.000000  \nmean     112574.822734       0.211599  \nstd       50292.865585       0.408443  \nmin          11.580000       0.000000  \n25%       74637.570000       0.000000  \n50%      117948.000000       0.000000  \n75%      155152.467500       0.000000  \nmax      199992.480000       1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>CustomerId</th>\n      <th>CreditScore</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>165034.0000</td>\n      <td>1.650340e+05</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n      <td>165034.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>82516.5000</td>\n      <td>1.569201e+07</td>\n      <td>656.454373</td>\n      <td>38.125888</td>\n      <td>5.020353</td>\n      <td>55478.086689</td>\n      <td>1.554455</td>\n      <td>0.753954</td>\n      <td>0.497770</td>\n      <td>112574.822734</td>\n      <td>0.211599</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47641.3565</td>\n      <td>7.139782e+04</td>\n      <td>80.103340</td>\n      <td>8.867205</td>\n      <td>2.806159</td>\n      <td>62817.663278</td>\n      <td>0.547154</td>\n      <td>0.430707</td>\n      <td>0.499997</td>\n      <td>50292.865585</td>\n      <td>0.408443</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.0000</td>\n      <td>1.556570e+07</td>\n      <td>350.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>11.580000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>41258.2500</td>\n      <td>1.563314e+07</td>\n      <td>597.000000</td>\n      <td>32.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>74637.570000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>82516.5000</td>\n      <td>1.569017e+07</td>\n      <td>659.000000</td>\n      <td>37.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>117948.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>123774.7500</td>\n      <td>1.575682e+07</td>\n      <td>710.000000</td>\n      <td>42.000000</td>\n      <td>7.000000</td>\n      <td>119939.517500</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>155152.467500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>165033.0000</td>\n      <td>1.581569e+07</td>\n      <td>850.000000</td>\n      <td>92.000000</td>\n      <td>10.000000</td>\n      <td>250898.090000</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>199992.480000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_id = df[\"id\"].copy()\ntest_id = test[\"id\"].copy()\n\ndf1 = df.drop([\"CustomerId\",\"Surname\",\"id\"], axis=1)\ndf1.head()\n\ntest1 = test.drop([\"CustomerId\",\"Surname\",\"id\"], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.665131Z","iopub.execute_input":"2025-07-22T20:48:47.665425Z","iopub.status.idle":"2025-07-22T20:48:47.684690Z","shell.execute_reply.started":"2025-07-22T20:48:47.665396Z","shell.execute_reply":"2025-07-22T20:48:47.683852Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.685677Z","iopub.execute_input":"2025-07-22T20:48:47.686029Z","iopub.status.idle":"2025-07-22T20:48:47.699880Z","shell.execute_reply.started":"2025-07-22T20:48:47.686001Z","shell.execute_reply":"2025-07-22T20:48:47.699056Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   CreditScore Geography  Gender   Age  Tenure    Balance  NumOfProducts  \\\n0          586    France  Female  23.0       2       0.00              2   \n1          683    France  Female  46.0       2       0.00              1   \n2          656    France  Female  34.0       7       0.00              2   \n3          681    France    Male  36.0       8       0.00              1   \n4          752   Germany    Male  38.0      10  121263.62              1   \n\n   HasCrCard  IsActiveMember  EstimatedSalary  \n0        0.0             1.0        160976.75  \n1        1.0             0.0         72549.27  \n2        1.0             0.0        138882.09  \n3        1.0             0.0        113931.57  \n4        1.0             0.0        139431.00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>586</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>23.0</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>160976.75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>683</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>46.0</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>72549.27</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>656</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>34.0</td>\n      <td>7</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>138882.09</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>681</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>36.0</td>\n      <td>8</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>113931.57</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>752</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>38.0</td>\n      <td>10</td>\n      <td>121263.62</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>139431.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df1[\"LogEstimatedSalary\"] = np.log(df1[\"EstimatedSalary\"])\ntest1[\"LogEstimatedSalary\"] = np.log(test1[\"EstimatedSalary\"])\n\ndf1 = df1.drop(\"EstimatedSalary\", axis = 1)\ntest1 = test1.drop(\"EstimatedSalary\", axis = 1)\n\ndf1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.702849Z","iopub.execute_input":"2025-07-22T20:48:47.703119Z","iopub.status.idle":"2025-07-22T20:48:47.756947Z","shell.execute_reply.started":"2025-07-22T20:48:47.703100Z","shell.execute_reply":"2025-07-22T20:48:47.756179Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   CreditScore Geography Gender   Age  Tenure    Balance  NumOfProducts  \\\n0          668    France   Male  33.0       3       0.00              2   \n1          627    France   Male  33.0       1       0.00              2   \n2          678    France   Male  40.0      10       0.00              2   \n3          581    France   Male  34.0       2  148882.54              1   \n4          716     Spain   Male  33.0       5       0.00              2   \n\n   HasCrCard  IsActiveMember  Exited  LogEstimatedSalary  \n0        1.0             0.0       0           12.108735  \n1        1.0             1.0       0           10.809799  \n2        1.0             0.0       0           12.127390  \n3        1.0             1.0       0           11.345227  \n4        1.0             1.0       0            9.620384  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>Exited</th>\n      <th>LogEstimatedSalary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>668</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>3</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>12.108735</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>627</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>10.809799</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>678</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>40.0</td>\n      <td>10</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>12.127390</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>581</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>34.0</td>\n      <td>2</td>\n      <td>148882.54</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>11.345227</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>716</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>5</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>9.620384</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df1[\"LogBalance\"] = np.log1p(df1[\"Balance\"])\ntest1[\"LogBalance\"] = np.log1p(test1[\"Balance\"])\ndf1 = df1.drop(\"Balance\", axis = 1)\ntest1 = test1.drop(\"Balance\", axis = 1)\n\ndf1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.757695Z","iopub.execute_input":"2025-07-22T20:48:47.757941Z","iopub.status.idle":"2025-07-22T20:48:47.802997Z","shell.execute_reply.started":"2025-07-22T20:48:47.757923Z","shell.execute_reply":"2025-07-22T20:48:47.802024Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   CreditScore Geography Gender   Age  Tenure  NumOfProducts  HasCrCard  \\\n0          668    France   Male  33.0       3              2        1.0   \n1          627    France   Male  33.0       1              2        1.0   \n2          678    France   Male  40.0      10              2        1.0   \n3          581    France   Male  34.0       2              1        1.0   \n4          716     Spain   Male  33.0       5              2        1.0   \n\n   IsActiveMember  Exited  LogEstimatedSalary  LogBalance  \n0             0.0       0           12.108735     0.00000  \n1             1.0       0           10.809799     0.00000  \n2             0.0       0           12.127390     0.00000  \n3             1.0       0           11.345227    11.91092  \n4             1.0       0            9.620384     0.00000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>Exited</th>\n      <th>LogEstimatedSalary</th>\n      <th>LogBalance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>668</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>12.108735</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>627</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>10.809799</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>678</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>40.0</td>\n      <td>10</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>12.127390</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>581</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>34.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>11.345227</td>\n      <td>11.91092</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>716</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>33.0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>9.620384</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df1 = pd.get_dummies(df1, columns=[\"Geography\",\"Gender\"])\ntest1 = pd.get_dummies(test1, columns=[\"Geography\",\"Gender\"])\n\ndf1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.804141Z","iopub.execute_input":"2025-07-22T20:48:47.804470Z","iopub.status.idle":"2025-07-22T20:48:47.909949Z","shell.execute_reply.started":"2025-07-22T20:48:47.804441Z","shell.execute_reply":"2025-07-22T20:48:47.909178Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   CreditScore   Age  Tenure  NumOfProducts  HasCrCard  IsActiveMember  \\\n0          668  33.0       3              2        1.0             0.0   \n1          627  33.0       1              2        1.0             1.0   \n2          678  40.0      10              2        1.0             0.0   \n3          581  34.0       2              1        1.0             1.0   \n4          716  33.0       5              2        1.0             1.0   \n\n   Exited  LogEstimatedSalary  LogBalance  Geography_France  \\\n0       0           12.108735     0.00000              True   \n1       0           10.809799     0.00000              True   \n2       0           12.127390     0.00000              True   \n3       0           11.345227    11.91092              True   \n4       0            9.620384     0.00000             False   \n\n   Geography_Germany  Geography_Spain  Gender_Female  Gender_Male  \n0              False            False          False         True  \n1              False            False          False         True  \n2              False            False          False         True  \n3              False            False          False         True  \n4              False             True          False         True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>Exited</th>\n      <th>LogEstimatedSalary</th>\n      <th>LogBalance</th>\n      <th>Geography_France</th>\n      <th>Geography_Germany</th>\n      <th>Geography_Spain</th>\n      <th>Gender_Female</th>\n      <th>Gender_Male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>668</td>\n      <td>33.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>12.108735</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>627</td>\n      <td>33.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>10.809799</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>678</td>\n      <td>40.0</td>\n      <td>10</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>12.127390</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>581</td>\n      <td>34.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>11.345227</td>\n      <td>11.91092</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>716</td>\n      <td>33.0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>9.620384</td>\n      <td>0.00000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"y_train = df1[\"Exited\"]\nX_train = df1.drop(\"Exited\", axis=1)\n\nX_test = test1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.910985Z","iopub.execute_input":"2025-07-22T20:48:47.911273Z","iopub.status.idle":"2025-07-22T20:48:47.919442Z","shell.execute_reply.started":"2025-07-22T20:48:47.911241Z","shell.execute_reply":"2025-07-22T20:48:47.918514Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.920209Z","iopub.execute_input":"2025-07-22T20:48:47.920415Z","iopub.status.idle":"2025-07-22T20:48:47.937669Z","shell.execute_reply.started":"2025-07-22T20:48:47.920398Z","shell.execute_reply":"2025-07-22T20:48:47.936867Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   CreditScore   Age  Tenure  NumOfProducts  HasCrCard  IsActiveMember  \\\n0          586  23.0       2              2        0.0             1.0   \n1          683  46.0       2              1        1.0             0.0   \n2          656  34.0       7              2        1.0             0.0   \n3          681  36.0       8              1        1.0             0.0   \n4          752  38.0      10              1        1.0             0.0   \n\n   LogEstimatedSalary  LogBalance  Geography_France  Geography_Germany  \\\n0           11.989015     0.00000              True              False   \n1           11.192021     0.00000              True              False   \n2           11.841381     0.00000              True              False   \n3           11.643353     0.00000              True              False   \n4           11.845325    11.70573             False               True   \n\n   Geography_Spain  Gender_Female  Gender_Male  \n0            False           True        False  \n1            False           True        False  \n2            False           True        False  \n3            False          False         True  \n4            False          False         True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>LogEstimatedSalary</th>\n      <th>LogBalance</th>\n      <th>Geography_France</th>\n      <th>Geography_Germany</th>\n      <th>Geography_Spain</th>\n      <th>Gender_Female</th>\n      <th>Gender_Male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>586</td>\n      <td>23.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>11.989015</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>683</td>\n      <td>46.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>11.192021</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>656</td>\n      <td>34.0</td>\n      <td>7</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>11.841381</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>681</td>\n      <td>36.0</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>11.643353</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>752</td>\n      <td>38.0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>11.845325</td>\n      <td>11.70573</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.938656Z","iopub.execute_input":"2025-07-22T20:48:47.939048Z","iopub.status.idle":"2025-07-22T20:48:47.957308Z","shell.execute_reply.started":"2025-07-22T20:48:47.939023Z","shell.execute_reply":"2025-07-22T20:48:47.956400Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0    0\n1    0\n2    0\n3    0\n4    0\nName: Exited, dtype: int64"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.958307Z","iopub.execute_input":"2025-07-22T20:48:47.959044Z","iopub.status.idle":"2025-07-22T20:48:47.982997Z","shell.execute_reply.started":"2025-07-22T20:48:47.959020Z","shell.execute_reply":"2025-07-22T20:48:47.981947Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"   CreditScore   Age  Tenure  NumOfProducts  HasCrCard  IsActiveMember  \\\n0          668  33.0       3              2        1.0             0.0   \n1          627  33.0       1              2        1.0             1.0   \n2          678  40.0      10              2        1.0             0.0   \n3          581  34.0       2              1        1.0             1.0   \n4          716  33.0       5              2        1.0             1.0   \n\n   LogEstimatedSalary  LogBalance  Geography_France  Geography_Germany  \\\n0           12.108735     0.00000              True              False   \n1           10.809799     0.00000              True              False   \n2           12.127390     0.00000              True              False   \n3           11.345227    11.91092              True              False   \n4            9.620384     0.00000             False              False   \n\n   Geography_Spain  Gender_Female  Gender_Male  \n0            False          False         True  \n1            False          False         True  \n2            False          False         True  \n3            False          False         True  \n4             True          False         True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>LogEstimatedSalary</th>\n      <th>LogBalance</th>\n      <th>Geography_France</th>\n      <th>Geography_Germany</th>\n      <th>Geography_Spain</th>\n      <th>Gender_Female</th>\n      <th>Gender_Male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>668</td>\n      <td>33.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>12.108735</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>627</td>\n      <td>33.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>10.809799</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>678</td>\n      <td>40.0</td>\n      <td>10</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>12.127390</td>\n      <td>0.00000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>581</td>\n      <td>34.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>11.345227</td>\n      <td>11.91092</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>716</td>\n      <td>33.0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>9.620384</td>\n      <td>0.00000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"\n\nrf = RandomForestClassifier(random_state=1722)\n\nscoring = {\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"}\n\nparam_dist = {\n    'n_estimators': [500],\n    'max_depth': [10],\n    'min_samples_split': [5],\n    'min_samples_leaf': [2],\n    'max_features': ['log2']\n}\n#param_dist = {\n    #'n_estimators': [100, 200, 300, 500],\n    #'max_depth': [None, 10, 20, 30],\n    #'min_samples_split': [2, 5, 10],\n    #'min_samples_leaf': [1, 2, 4],\n    #'max_features': ['sqrt', 'log2']\n#}\n\nrandom_search = RandomizedSearchCV(\n    rf,\n    param_dist,\n    n_iter=1,\n    cv=5,\n    n_jobs=-1,\n    scoring=scoring,\n    refit=\"roc_auc\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:47.983960Z","iopub.execute_input":"2025-07-22T20:48:47.984263Z","iopub.status.idle":"2025-07-22T20:48:48.000713Z","shell.execute_reply.started":"2025-07-22T20:48:47.984238Z","shell.execute_reply":"2025-07-22T20:48:47.999831Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"random_search.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:48:48.001800Z","iopub.execute_input":"2025-07-22T20:48:48.002100Z","iopub.status.idle":"2025-07-22T20:52:52.199662Z","shell.execute_reply.started":"2025-07-22T20:48:48.002074Z","shell.execute_reply":"2025-07-22T20:52:52.198638Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=1722),\n                   n_iter=1, n_jobs=-1,\n                   param_distributions={'max_depth': [10],\n                                        'max_features': ['log2'],\n                                        'min_samples_leaf': [2],\n                                        'min_samples_split': [5],\n                                        'n_estimators': [500]},\n                   refit='roc_auc',\n                   scoring={'accuracy', 'recall', 'f1', 'precision', 'roc_auc'})","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=1722),\n                   n_iter=1, n_jobs=-1,\n                   param_distributions={&#x27;max_depth&#x27;: [10],\n                                        &#x27;max_features&#x27;: [&#x27;log2&#x27;],\n                                        &#x27;min_samples_leaf&#x27;: [2],\n                                        &#x27;min_samples_split&#x27;: [5],\n                                        &#x27;n_estimators&#x27;: [500]},\n                   refit=&#x27;roc_auc&#x27;,\n                   scoring={&#x27;accuracy&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;precision&#x27;, &#x27;roc_auc&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=1722),\n                   n_iter=1, n_jobs=-1,\n                   param_distributions={&#x27;max_depth&#x27;: [10],\n                                        &#x27;max_features&#x27;: [&#x27;log2&#x27;],\n                                        &#x27;min_samples_leaf&#x27;: [2],\n                                        &#x27;min_samples_split&#x27;: [5],\n                                        &#x27;n_estimators&#x27;: [500]},\n                   refit=&#x27;roc_auc&#x27;,\n                   scoring={&#x27;accuracy&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;precision&#x27;, &#x27;roc_auc&#x27;})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=1722)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=1722)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"random_search.best_params_\n#{'n_estimators': 500,\n# 'min_samples_split': 5,\n# 'min_samples_leaf': 2,\n# 'max_features': 'log2',\n# 'max_depth': 10}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:52:52.200626Z","iopub.execute_input":"2025-07-22T20:52:52.200932Z","iopub.status.idle":"2025-07-22T20:52:52.207295Z","shell.execute_reply.started":"2025-07-22T20:52:52.200907Z","shell.execute_reply":"2025-07-22T20:52:52.206279Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'n_estimators': 500,\n 'min_samples_split': 5,\n 'min_samples_leaf': 2,\n 'max_features': 'log2',\n 'max_depth': 10}"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"random_search.best_score_\n#0.8872367824663965","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:52:52.208346Z","iopub.execute_input":"2025-07-22T20:52:52.208907Z","iopub.status.idle":"2025-07-22T20:52:52.226903Z","shell.execute_reply.started":"2025-07-22T20:52:52.208877Z","shell.execute_reply":"2025-07-22T20:52:52.226080Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0.8872367824663965"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def make_results(model_name: str, model_object, metric: str):\n    \"\"\"\n    Arguments:\n        model_name (string): what you want the model to be called in the output table\n        model_object: a fit GridSearchCV object\n        metric (string): precision, recall, f1, accuracy, or roc_auc\n    \n    Returns a pandas df with the f1, recall, precision, accuracy, and roc_auc scores\n    for the model with the best mean 'metric' score across all validation folds.\n    \"\"\"\n    \n    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n    metric_dict = {\n        'roc_auc': 'mean_test_roc_auc',\n        'precision': 'mean_test_precision', \n        'recall': 'mean_test_recall',\n        'f1': 'mean_test_f1',\n        'accuracy': 'mean_test_accuracy'\n    }\n    \n    # Get all the results from the CV and put them in a df\n    cv_results = pd.DataFrame(model_object.cv_results_)\n    \n    # Isolate the row of the df with the max(metric) score\n    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n    \n    # Extract accuracy, precision, recall, and f1 score from that row\n    roc_auc = best_estimator_results.mean_test_roc_auc\n    f1 = best_estimator_results.mean_test_f1\n    recall = best_estimator_results.mean_test_recall\n    precision = best_estimator_results.mean_test_precision\n    accuracy = best_estimator_results.mean_test_accuracy\n    \n    # Create table of results\n    table = pd.DataFrame({\n        'model': [model_name],\n        'precision': [precision],\n        'recall': [recall], \n        'f1': [f1],\n        'accuracy': [accuracy],\n        'roc_auc': [roc_auc]\n    })\n    \n    return table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:52:52.227793Z","iopub.execute_input":"2025-07-22T20:52:52.228055Z","iopub.status.idle":"2025-07-22T20:52:52.246934Z","shell.execute_reply.started":"2025-07-22T20:52:52.228035Z","shell.execute_reply":"2025-07-22T20:52:52.246026Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"rf1_cv_results = make_results(\"random forest v1\",random_search, \"roc_auc\")\nprint(rf1_cv_results)\n\n#              model  precision    recall        f1  accuracy   roc_auc\n#0  random forest cv    0.76682  0.507288  0.610575  0.863089  0.887237","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:52:52.247988Z","iopub.execute_input":"2025-07-22T20:52:52.248305Z","iopub.status.idle":"2025-07-22T20:52:52.276854Z","shell.execute_reply.started":"2025-07-22T20:52:52.248275Z","shell.execute_reply":"2025-07-22T20:52:52.275820Z"}},"outputs":[{"name":"stdout","text":"              model  precision    recall        f1  accuracy   roc_auc\n0  random forest v1    0.76682  0.507288  0.610575  0.863089  0.887237\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#version 2\n\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',random_state=1722)\n\nscoring = {\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"}\n\n#param_dist = {\n#    'n_estimators': [100,200,500],\n#    'max_depth': [3,5,7,10],\n#    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n#    'min_child_weight': [1, 3, 5],\n#    'colsample_bytree': np.arange(0.5, 1.0, 0.1)\n#}\nparam_dist = {\n    'n_estimators': [500],\n    'max_depth': [3],\n    'learning_rate': [0.1],\n    'min_child_weight': [3],\n    'colsample_bytree': [0.6]\n}\n\nrandom_search1 = RandomizedSearchCV(\n    xgb,\n    param_dist,\n    n_iter=50,\n    cv=5,\n    n_jobs=-1,\n    scoring=scoring,\n    refit=\"roc_auc\",\n    random_state=1722\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:52:52.277963Z","iopub.execute_input":"2025-07-22T20:52:52.278302Z","iopub.status.idle":"2025-07-22T20:52:52.297284Z","shell.execute_reply.started":"2025-07-22T20:52:52.278267Z","shell.execute_reply":"2025-07-22T20:52:52.296307Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"%%time\nrandom_search1.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:52:52.298145Z","iopub.execute_input":"2025-07-22T20:52:52.298389Z","iopub.status.idle":"2025-07-22T20:53:06.811621Z","shell.execute_reply.started":"2025-07-22T20:52:52.298370Z","shell.execute_reply":"2025-07-22T20:53:06.810793Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 9.32 s, sys: 110 ms, total: 9.43 s\nWall time: 14.5 s\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric='logloss',\n                                           feature_types=None, gamma=None,\n                                           grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning...\n                                           monotone_constraints=None,\n                                           multi_strategy=None,\n                                           n_estimators=None, n_jobs=None,\n                                           num_parallel_tree=None,\n                                           random_state=1722, ...),\n                   n_iter=50, n_jobs=-1,\n                   param_distributions={'colsample_bytree': [0.6],\n                                        'learning_rate': [0.1],\n                                        'max_depth': [3],\n                                        'min_child_weight': [3],\n                                        'n_estimators': [500]},\n                   random_state=1722, refit='roc_auc',\n                   scoring={'accuracy', 'recall', 'f1', 'precision', 'roc_auc'})","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric=&#x27;logloss&#x27;,\n                                           feature_types=None, gamma=None,\n                                           grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning...\n                                           monotone_constraints=None,\n                                           multi_strategy=None,\n                                           n_estimators=None, n_jobs=None,\n                                           num_parallel_tree=None,\n                                           random_state=1722, ...),\n                   n_iter=50, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.6],\n                                        &#x27;learning_rate&#x27;: [0.1],\n                                        &#x27;max_depth&#x27;: [3],\n                                        &#x27;min_child_weight&#x27;: [3],\n                                        &#x27;n_estimators&#x27;: [500]},\n                   random_state=1722, refit=&#x27;roc_auc&#x27;,\n                   scoring={&#x27;accuracy&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;precision&#x27;, &#x27;roc_auc&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric=&#x27;logloss&#x27;,\n                                           feature_types=None, gamma=None,\n                                           grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning...\n                                           monotone_constraints=None,\n                                           multi_strategy=None,\n                                           n_estimators=None, n_jobs=None,\n                                           num_parallel_tree=None,\n                                           random_state=1722, ...),\n                   n_iter=50, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.6],\n                                        &#x27;learning_rate&#x27;: [0.1],\n                                        &#x27;max_depth&#x27;: [3],\n                                        &#x27;min_child_weight&#x27;: [3],\n                                        &#x27;n_estimators&#x27;: [500]},\n                   random_state=1722, refit=&#x27;roc_auc&#x27;,\n                   scoring={&#x27;accuracy&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;precision&#x27;, &#x27;roc_auc&#x27;})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=1722, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=1722, ...)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"random_search1.best_params_\n#{'n_estimators': 500,\n#'min_child_weight': 3,\n#'max_depth': 3,\n#'learning_rate': 0.1,\n#'colsample_bytree': 0.6}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:53:06.812386Z","iopub.execute_input":"2025-07-22T20:53:06.812714Z","iopub.status.idle":"2025-07-22T20:53:06.819016Z","shell.execute_reply.started":"2025-07-22T20:53:06.812685Z","shell.execute_reply":"2025-07-22T20:53:06.818022Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'n_estimators': 500,\n 'min_child_weight': 3,\n 'max_depth': 3,\n 'learning_rate': 0.1,\n 'colsample_bytree': 0.6}"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"random_search1.best_score_\n#0.8898609732518489","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:53:06.819701Z","iopub.execute_input":"2025-07-22T20:53:06.819940Z","iopub.status.idle":"2025-07-22T20:53:06.844149Z","shell.execute_reply.started":"2025-07-22T20:53:06.819922Z","shell.execute_reply":"2025-07-22T20:53:06.843089Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.8898609732518489"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"xgb_cv_results = make_results(\"xgb v1\",random_search1, \"roc_auc\")\nprint(rf1_cv_results)\nprint(xgb_cv_results)\n#              model  precision    recall        f1  accuracy   roc_auc\n#0  random forest v1    0.76682  0.507288  0.610575  0.863089  0.887237\n#    model  precision    recall        f1  accuracy   roc_auc\n#0  xgb v1    0.74883  0.551158  0.634941    0.8659  0.889826","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:53:06.845231Z","iopub.execute_input":"2025-07-22T20:53:06.845571Z","iopub.status.idle":"2025-07-22T20:53:06.871404Z","shell.execute_reply.started":"2025-07-22T20:53:06.845549Z","shell.execute_reply":"2025-07-22T20:53:06.870407Z"}},"outputs":[{"name":"stdout","text":"              model  precision    recall        f1  accuracy   roc_auc\n0  random forest v1    0.76682  0.507288  0.610575  0.863089  0.887237\n    model  precision    recall        f1  accuracy   roc_auc\n0  xgb v1   0.747666  0.550643  0.634176  0.865579  0.889861\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"lgbm_v1 = LGBMClassifier(random_state=1722)\n\nscoring = {\n    \"accuracy\": \"accuracy\",\n    \"precision\": \"precision\",\n    \"recall\": \"recall\",\n    \"f1\": \"f1\",\n    \"roc_auc\": \"roc_auc\"\n}\n\nparam_dist = {\n    'n_estimators': [100,200,500],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'num_leaves': [20, 31, 40, 50],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [0, 0.1, 0.5]\n}\n\n\nrandom_search2 = RandomizedSearchCV(\n    lgbm_v1,\n    param_dist,\n    n_iter=50,\n    cv=5,\n    n_jobs=-1,\n    scoring=scoring,\n    refit=\"roc_auc\",\n    random_state=1722\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:53:06.875310Z","iopub.execute_input":"2025-07-22T20:53:06.875576Z","iopub.status.idle":"2025-07-22T20:53:06.881969Z","shell.execute_reply.started":"2025-07-22T20:53:06.875556Z","shell.execute_reply":"2025-07-22T20:53:06.880882Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"%%time\nrandom_search2.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:53:06.882985Z","iopub.execute_input":"2025-07-22T20:53:06.883308Z","iopub.status.idle":"2025-07-22T21:06:11.764420Z","shell.execute_reply.started":"2025-07-22T20:53:06.883278Z","shell.execute_reply":"2025-07-22T21:06:11.763434Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044161 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031523 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035524 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038798 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058499 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035497 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034758 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039477 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011126 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007474 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007892 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029906 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009288 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035387 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041900 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029542 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016142 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045041 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035100 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039514 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013117 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030378 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041081 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011741 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010423 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040998 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030515 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037926 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033090 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028110 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033488 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041753 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030306 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029473 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010378 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042508 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036486 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035517 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007127 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035509 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037846 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029518 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040480 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027242 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035515 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006341 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039995 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039108 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029147 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035547 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062506 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009827 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037338 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040734 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040474 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006379 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036554 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037480 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008271 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036628 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006516 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035026 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026766 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041515 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034990 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032468 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041396 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009928 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034047 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037866 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009684 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011149 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042402 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041482 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011745 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029945 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033877 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038337 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041467 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041503 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008220 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031479 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047801 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010187 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041517 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036462 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009639 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075144 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034098 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035541 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039388 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034158 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010524 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037690 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040450 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029093 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030722 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010347 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006705 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015999 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029800 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006315 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006975 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006465 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035026 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029091 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006838 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037491 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007801 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040545 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029170 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039529 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029529 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052083 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034899 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034889 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029538 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007326 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035529 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036299 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011997 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006454 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035505 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009904 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007071 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051374 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035100 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012586 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043721 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031031 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006639 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032532 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007475 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041497 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041484 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011493 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030283 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039122 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010396 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041104 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007250 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006864 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041530 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041489 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041059 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006609 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034960 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016021 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037504 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006324 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048504 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006895 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053519 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042483 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035518 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042998 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031423 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041531 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012761 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018623 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036204 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011995 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039673 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035498 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034138 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007229 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010499 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041937 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041043 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007464 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035510 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039499 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039463 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028931 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041456 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023844 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028706 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031819 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010994 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010550 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006309 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023830 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035518 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011574 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035505 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036154 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041499 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007598 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006248 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008313 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006309 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035530 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007441 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044540 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029523 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035110 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041124 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047046 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050512 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007449 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017109 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090[LightGBM] [Info] Number of positive: 34921, number of negative: 130113\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 165034, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315315\n[LightGBM] [Info] Start training from score -1.315315\nCPU times: user 12.5 s, sys: 1.14 s, total: 13.7 s\nWall time: 13min 4s\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=5, estimator=LGBMClassifier(random_state=1722), n_iter=50,\n                   n_jobs=-1,\n                   param_distributions={'colsample_bytree': [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n                                        'n_estimators': [100, 200, 500],\n                                        'num_leaves': [20, 31, 40, 50],\n                                        'reg_alpha': [0, 0.1, 0.5],\n                                        'reg_lambda': [0, 0.1, 0.5]},\n                   random_state=1722, refit='roc_auc',\n                   scoring={'accuracy': 'accuracy', 'f1': 'f1',\n                            'precision': 'precision', 'recall': 'recall',\n                            'roc_auc': 'roc_auc'})","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=LGBMClassifier(random_state=1722), n_iter=50,\n                   n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n                                        &#x27;n_estimators&#x27;: [100, 200, 500],\n                                        &#x27;num_leaves&#x27;: [20, 31, 40, 50],\n                                        &#x27;reg_alpha&#x27;: [0, 0.1, 0.5],\n                                        &#x27;reg_lambda&#x27;: [0, 0.1, 0.5]},\n                   random_state=1722, refit=&#x27;roc_auc&#x27;,\n                   scoring={&#x27;accuracy&#x27;: &#x27;accuracy&#x27;, &#x27;f1&#x27;: &#x27;f1&#x27;,\n                            &#x27;precision&#x27;: &#x27;precision&#x27;, &#x27;recall&#x27;: &#x27;recall&#x27;,\n                            &#x27;roc_auc&#x27;: &#x27;roc_auc&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=LGBMClassifier(random_state=1722), n_iter=50,\n                   n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n                                        &#x27;n_estimators&#x27;: [100, 200, 500],\n                                        &#x27;num_leaves&#x27;: [20, 31, 40, 50],\n                                        &#x27;reg_alpha&#x27;: [0, 0.1, 0.5],\n                                        &#x27;reg_lambda&#x27;: [0, 0.1, 0.5]},\n                   random_state=1722, refit=&#x27;roc_auc&#x27;,\n                   scoring={&#x27;accuracy&#x27;: &#x27;accuracy&#x27;, &#x27;f1&#x27;: &#x27;f1&#x27;,\n                            &#x27;precision&#x27;: &#x27;precision&#x27;, &#x27;recall&#x27;: &#x27;recall&#x27;,\n                            &#x27;roc_auc&#x27;: &#x27;roc_auc&#x27;})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(random_state=1722)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(random_state=1722)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"random_search2.best_params_\n#{'reg_lambda': 0.1,\n#'reg_alpha': 0.5,\n#'num_leaves': 31,\n#'n_estimators': 500,\n#'learning_rate': 0.05,\n#'colsample_bytree': 0.7}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:06:11.765517Z","iopub.execute_input":"2025-07-22T21:06:11.765968Z","iopub.status.idle":"2025-07-22T21:06:11.771682Z","shell.execute_reply.started":"2025-07-22T21:06:11.765935Z","shell.execute_reply":"2025-07-22T21:06:11.770933Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'reg_lambda': 0.1,\n 'reg_alpha': 0.5,\n 'num_leaves': 31,\n 'n_estimators': 500,\n 'learning_rate': 0.05,\n 'colsample_bytree': 0.7}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"random_search2.best_score_\n#np.float64(0.8900515382076835)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:06:11.772977Z","iopub.execute_input":"2025-07-22T21:06:11.773546Z","iopub.status.idle":"2025-07-22T21:06:11.798154Z","shell.execute_reply.started":"2025-07-22T21:06:11.773517Z","shell.execute_reply":"2025-07-22T21:06:11.797181Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.8900515382076835"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"lgbm_cv_results = make_results(\"lgbm v1\",random_search1, \"roc_auc\")\nprint(rf1_cv_results)\nprint(xgb_cv_results)\nprint(lgbm_cv_results)\n#              model  precision    recall        f1  accuracy   roc_auc\n#0  random forest v1    0.76682  0.507288  0.610575  0.863089  0.887237\n#    model  precision    recall        f1  accuracy   roc_auc\n#0  xgb v1    0.74883  0.551158  0.634941    0.8659  0.889826\n#     model  precision    recall        f1  accuracy   roc_auc\n#0  lgbm v1    0.74883  0.551158  0.634941    0.8659  0.889826","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:06:11.799487Z","iopub.execute_input":"2025-07-22T21:06:11.799892Z","iopub.status.idle":"2025-07-22T21:06:11.834877Z","shell.execute_reply.started":"2025-07-22T21:06:11.799861Z","shell.execute_reply":"2025-07-22T21:06:11.834101Z"}},"outputs":[{"name":"stdout","text":"              model  precision    recall        f1  accuracy   roc_auc\n0  random forest v1    0.76682  0.507288  0.610575  0.863089  0.887237\n    model  precision    recall        f1  accuracy   roc_auc\n0  xgb v1   0.747666  0.550643  0.634176  0.865579  0.889861\n     model  precision    recall        f1  accuracy   roc_auc\n0  lgbm v1   0.747666  0.550643  0.634176  0.865579  0.889861\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Ver las top 10 features m√°s importantes\nimportances = pd.Series(\n    random_search.best_estimator_.feature_importances_, \n    index=X_train.columns\n).sort_values(ascending=False)\n\nprint(importances.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:06:11.838371Z","iopub.execute_input":"2025-07-22T21:06:11.838682Z","iopub.status.idle":"2025-07-22T21:06:11.883025Z","shell.execute_reply.started":"2025-07-22T21:06:11.838660Z","shell.execute_reply":"2025-07-22T21:06:11.882063Z"}},"outputs":[{"name":"stdout","text":"Age                   0.356367\nNumOfProducts         0.310523\nIsActiveMember        0.104806\nLogBalance            0.059748\nGeography_Germany     0.057782\nGender_Female         0.022183\nGender_Male           0.021779\nLogEstimatedSalary    0.018481\nCreditScore           0.018071\nGeography_France      0.013246\ndtype: float64\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Ver las top 10 features m√°s importantes\nimportances1 = pd.Series(\n    random_search1.best_estimator_.feature_importances_, \n    index=X_train.columns\n).sort_values(ascending=False)\n\nprint(importances1.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:06:11.911078Z","iopub.execute_input":"2025-07-22T21:06:11.911404Z","iopub.status.idle":"2025-07-22T21:06:11.935463Z","shell.execute_reply.started":"2025-07-22T21:06:11.911370Z","shell.execute_reply":"2025-07-22T21:06:11.934551Z"}},"outputs":[{"name":"stdout","text":"NumOfProducts        0.308581\nIsActiveMember       0.246450\nAge                  0.121184\nGender_Male          0.097222\nGeography_Germany    0.091224\nGender_Female        0.055487\nGeography_France     0.033770\nLogBalance           0.022274\nHasCrCard            0.007796\nGeography_Spain      0.006779\ndtype: float32\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Ver las top 10 features m√°s importantes\nimportances2 = pd.Series(\n    random_search2.best_estimator_.feature_importances_, \n    index=X_train.columns\n).sort_values(ascending=False)\n\nprint(importances2.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:06:11.936405Z","iopub.execute_input":"2025-07-22T21:06:11.936698Z","iopub.status.idle":"2025-07-22T21:06:11.953784Z","shell.execute_reply.started":"2025-07-22T21:06:11.936679Z","shell.execute_reply":"2025-07-22T21:06:11.952976Z"}},"outputs":[{"name":"stdout","text":"CreditScore           3023\nLogEstimatedSalary    2855\nLogBalance            2822\nAge                   2544\nTenure                1219\nNumOfProducts          614\nIsActiveMember         411\nGender_Female          373\nGeography_Germany      332\nHasCrCard              300\ndtype: int32\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"comparison_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'rf_importance': importances / importances.sum(),\n    'xgb_importance': importances1 / importances1.sum(), \n    'lgbm_importance': importances2 / importances2.sum()\n})\ncomparison_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:06:11.954605Z","iopub.execute_input":"2025-07-22T21:06:11.954922Z","iopub.status.idle":"2025-07-22T21:06:11.979890Z","shell.execute_reply.started":"2025-07-22T21:06:11.954901Z","shell.execute_reply":"2025-07-22T21:06:11.979013Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                               feature  rf_importance  xgb_importance  \\\nAge                        CreditScore       0.356367        0.121184   \nCreditScore                        Age       0.018071        0.003456   \nGender_Female                   Tenure       0.022183        0.055487   \nGender_Male              NumOfProducts       0.021779        0.097222   \nGeography_France             HasCrCard       0.013246        0.033770   \nGeography_Germany       IsActiveMember       0.057782        0.091224   \nGeography_Spain     LogEstimatedSalary       0.005662        0.006779   \nHasCrCard                   LogBalance       0.002982        0.007796   \nIsActiveMember        Geography_France       0.104806        0.246450   \nLogBalance           Geography_Germany       0.059748        0.022274   \nLogEstimatedSalary     Geography_Spain       0.018481        0.002941   \nNumOfProducts            Gender_Female       0.310523        0.308581   \nTenure                     Gender_Male       0.008370        0.002834   \n\n                    lgbm_importance  \nAge                        0.169600  \nCreditScore                0.201533  \nGender_Female              0.024867  \nGender_Male                0.009600  \nGeography_France           0.013533  \nGeography_Germany          0.022133  \nGeography_Spain            0.010667  \nHasCrCard                  0.020000  \nIsActiveMember             0.027400  \nLogBalance                 0.188133  \nLogEstimatedSalary         0.190333  \nNumOfProducts              0.040933  \nTenure                     0.081267  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>rf_importance</th>\n      <th>xgb_importance</th>\n      <th>lgbm_importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Age</th>\n      <td>CreditScore</td>\n      <td>0.356367</td>\n      <td>0.121184</td>\n      <td>0.169600</td>\n    </tr>\n    <tr>\n      <th>CreditScore</th>\n      <td>Age</td>\n      <td>0.018071</td>\n      <td>0.003456</td>\n      <td>0.201533</td>\n    </tr>\n    <tr>\n      <th>Gender_Female</th>\n      <td>Tenure</td>\n      <td>0.022183</td>\n      <td>0.055487</td>\n      <td>0.024867</td>\n    </tr>\n    <tr>\n      <th>Gender_Male</th>\n      <td>NumOfProducts</td>\n      <td>0.021779</td>\n      <td>0.097222</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <th>Geography_France</th>\n      <td>HasCrCard</td>\n      <td>0.013246</td>\n      <td>0.033770</td>\n      <td>0.013533</td>\n    </tr>\n    <tr>\n      <th>Geography_Germany</th>\n      <td>IsActiveMember</td>\n      <td>0.057782</td>\n      <td>0.091224</td>\n      <td>0.022133</td>\n    </tr>\n    <tr>\n      <th>Geography_Spain</th>\n      <td>LogEstimatedSalary</td>\n      <td>0.005662</td>\n      <td>0.006779</td>\n      <td>0.010667</td>\n    </tr>\n    <tr>\n      <th>HasCrCard</th>\n      <td>LogBalance</td>\n      <td>0.002982</td>\n      <td>0.007796</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <th>IsActiveMember</th>\n      <td>Geography_France</td>\n      <td>0.104806</td>\n      <td>0.246450</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <th>LogBalance</th>\n      <td>Geography_Germany</td>\n      <td>0.059748</td>\n      <td>0.022274</td>\n      <td>0.188133</td>\n    </tr>\n    <tr>\n      <th>LogEstimatedSalary</th>\n      <td>Geography_Spain</td>\n      <td>0.018481</td>\n      <td>0.002941</td>\n      <td>0.190333</td>\n    </tr>\n    <tr>\n      <th>NumOfProducts</th>\n      <td>Gender_Female</td>\n      <td>0.310523</td>\n      <td>0.308581</td>\n      <td>0.040933</td>\n    </tr>\n    <tr>\n      <th>Tenure</th>\n      <td>Gender_Male</td>\n      <td>0.008370</td>\n      <td>0.002834</td>\n      <td>0.081267</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030493 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n\n[LightGBM] [Info] Start training from score -1.315304\n\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033287 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031483 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009671 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035507 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007376 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006182 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029687 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033500 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029924 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006329 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006649 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055128 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007199 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031691 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012293 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029218 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034527 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011379 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041506 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041501 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"#el modelo en ganar la competicion hizo 0.90585 https://www.kaggle.com/competitions/playground-series-s4e1/leaderboard\n#vs                                  rf 0.887237\n#vs                                 xgb 0.889826\n#vs                                lgbm 0.889826\n#Para continuar puedo \n# 1 priorizar hacer feature engineering con las variables mas relevantes\n# 2 poner en orden las variables de mas importantes a menos importantes\n# 3 hacer un modelo de ensemble con los 3 / probar mas hyperparametros / probar catboost u otros modelos NB o logistic\n# 4 hacer una red neuronal no muy profunda\n# Al ser uno de los primeros modeloes en hacer solo , prefiero continuar viendo otras competiciones y ganar mas experiencia ,\n# que estar mucho tiempo con este modelo\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}