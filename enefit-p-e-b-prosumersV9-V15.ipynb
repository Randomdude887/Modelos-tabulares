{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:57:32.449921Z","iopub.execute_input":"2025-09-05T10:57:32.451416Z","iopub.status.idle":"2025-09-05T10:57:33.426393Z","shell.execute_reply.started":"2025-09-05T10:57:32.451363Z","shell.execute_reply":"2025-09-05T10:57:33.425257Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/public_timeseries_testing_util.py\n/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/county_id_to_name_map.json\n/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/sample_submission.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CORE DATA MANIPULATION\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import pearsonr, spearmanr\nimport math\n\n# MACHINE LEARNING - SCIKIT-LEARN\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, roc_curve, auc\nfrom sklearn.pipeline import Pipeline\n\n# ADVANCED ML LIBRARIES\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# VISUALIZATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\n\n# CONFIGURATION FOR PLOTS\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n\n# DEEP LEARNING \ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n    print(\"TensorFlow disponible\")\nexcept ImportError:\n    print(\"TensorFlow no disponible en este entorno\")\n\n\n# UTILITIES\nimport os\nimport sys\nimport warnings\nimport itertools\nfrom datetime import datetime, timedelta\nimport time\nfrom collections import Counter\nimport pickle\nimport joblib\n\n# JUPYTER SPECIFIC\nfrom IPython.display import display, HTML\nfrom tqdm.notebook import tqdm\n\n# SUPPRESS WARNINGS\nwarnings.filterwarnings('ignore')\n\n# PANDAS CONFIGURATION\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n# NUMPY CONFIGURATION\nnp.random.seed(42)\n\n# PLOTLY CONFIGURATION\npyo.init_notebook_mode(connected=True)\n\nprint(\"‚úÖ Todas las librer√≠as importadas correctamente!\")\nprint(f\"üìä Pandas version: {pd.__version__}\")\nprint(f\"üî¢ NumPy version: {np.__version__}\")\nprint(f\"ü§ñ Scikit-learn version: {__import__('sklearn').__version__}\")\nprint(f\"üìà Matplotlib version: {__import__('matplotlib').__version__}\")\nprint(f\"üé® Seaborn version: {sns.__version__}\")\n\n# ============================================================================\n# FUNCIONES √öTILES ADICIONALES\n# ============================================================================\n\ndef quick_info(df):\n    \"\"\"Informaci√≥n r√°pida del dataset\"\"\"\n    print(f\"üìä Dataset Shape: {df.shape}\")\n    print(f\"üî¢ Columnas num√©ricas: {df.select_dtypes(include=[np.number]).columns.tolist()}\")\n    print(f\"üìù Columnas categ√≥ricas: {df.select_dtypes(include=['object']).columns.tolist()}\")\n    print(f\"‚ùå Valores nulos por columna:\")\n    print(df.isnull().sum()[df.isnull().sum() > 0])\n\ndef plot_missing_values(df):\n    \"\"\"Visualizar valores faltantes\"\"\"\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    if len(missing) > 0:\n        plt.figure(figsize=(10, 6))\n        missing.plot(kind='bar')\n        plt.title('Valores Faltantes por Columna')\n        plt.ylabel('Cantidad')\n        plt.xticks(rotation=45)\n        plt.show()\n    else:\n        print(\"‚úÖ No hay valores faltantes en el dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:57:33.428036Z","iopub.execute_input":"2025-09-05T10:57:33.428480Z","iopub.status.idle":"2025-09-05T10:57:49.255488Z","shell.execute_reply.started":"2025-09-05T10:57:33.428455Z","shell.execute_reply":"2025-09-05T10:57:49.254551Z"}},"outputs":[{"name":"stderr","text":"2025-09-05 10:57:41.292679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757069861.510716     179 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757069861.572769     179 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow disponible\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"name":"stdout","text":"‚úÖ Todas las librer√≠as importadas correctamente!\nüìä Pandas version: 2.2.3\nüî¢ NumPy version: 1.26.4\nü§ñ Scikit-learn version: 1.2.2\nüìà Matplotlib version: 3.7.2\nüé® Seaborn version: 0.12.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#competencia https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/overview\n\n# Files\n# train.csv\n\n# county - An ID code for the county.\n# is_business - Boolean for whether or not the prosumer is a business.\n# product_type - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n# target - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n# is_consumption - Boolean for whether or not this row's target is consumption or production.\n# datetime - The Estonian time in EET (UTC+2) / EEST (UTC+3). It describes the start of the 1-hour period on which target is given.\n# data_block_id - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n# row_id - A unique identifier for the row.\n# prediction_unit_id - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n# gas_prices.csv\n\n# origin_date - The date when the day-ahead prices became available.\n# forecast_date - The date when the forecast prices should be relevant.\n# [lowest/highest]_price_per_mwh - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n# data_block_id\n# client.csv\n\n# product_type\n# county - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n# eic_count - The aggregated number of consumption points (EICs - European Identifier Code).\n# installed_capacity - Installed photovoltaic solar panel capacity in kilowatts.\n# is_business - Boolean for whether or not the prosumer is a business.\n# date\n# data_block_id\n# electricity_prices.csv\n\n# origin_date\n# forecast_date - Represents the start of the 1-hour period when the price is valid\n# euros_per_mwh - The price of electricity on the day ahead markets in euros per megawatt hour.\n# data_block_id\n# forecast_weather.csv Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n\n# [latitude/longitude] - The coordinates of the weather forecast.\n# origin_datetime - The timestamp of when the forecast was generated.\n# hours_ahead - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n# temperature - The air temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# dewpoint - The dew point temperature at 2 meters above ground in degrees Celsius. Estimated for the end of the 1-hour period.\n# cloudcover_[low/mid/high/total] - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total. Estimated for the end of the 1-hour period.\n# 10_metre_[u/v]_wind_component - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second. Estimated for the end of the 1-hour period.\n# data_block_id\n# forecast_datetime - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead. This represents the start of the 1-hour period for which weather data are forecasted.\n# direct_solar_radiation - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the hour, in watt-hours per square meter.\n# surface_solar_radiation_downwards - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, accumulated during the hour, in watt-hours per square meter.\n# snowfall - Snowfall over hour in units of meters of water equivalent.\n# total_precipitation - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the described hour, in units of meters.\n# historical_weather.csv Historic weather data.\n\n# datetime - This represents the start of the 1-hour period for which weather data are measured.\n# temperature - Measured at the end of the 1-hour period.\n# dewpoint - Measured at the end of the 1-hour period.\n# rain - Different from the forecast conventions. The rain from large scale weather systems of the hour in millimeters.\n# snowfall - Different from the forecast conventions. Snowfall over the hour in centimeters.\n# surface_pressure - The air pressure at surface in hectopascals.\n# cloudcover_[low/mid/high/total] - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n# windspeed_10m - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n# winddirection_10m - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n# shortwave_radiation - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n# direct_solar_radiation\n# diffuse_radiation - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n# [latitude/longitude] - The coordinates of the weather station.\n# data_block_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:57:49.256478Z","iopub.execute_input":"2025-09-05T10:57:49.256776Z","iopub.status.idle":"2025-09-05T10:57:49.263701Z","shell.execute_reply.started":"2025-09-05T10:57:49.256754Z","shell.execute_reply":"2025-09-05T10:57:49.262706Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#modelos a correr: NN por ser forecasting y algun xgb o lgb\n#son 4 dias a predecir el comportamiento de los prosumers\n#predecir con rolling de a 1 dia\n\n\n#analizar y preparar\n# como siempre primero ver los faltantes y los NaN y en cada caso decidir que hacer\n# atento con valores expresados para el dia actual o la hora actual / la hora anterior / el dia de ma√±ana , ver de no malinterpretar\n# ver la correlacion de precios sea gas y/o electricidad para ver como se comparta con business y no business\n# lo mismo con el clima para business y no business\n# ver si aplica el supuesto de que los business si operan bajo esos valores (costos de electricidad/gas) y los no business no es tan relevante\n# clima historical vs forecast , adaptar columnas que usan distintas proporciones , medir la certeza del forecast\n# analizar por fuera como funcionan los paneles y como miden lo que miden ( para mejor entendimiento del df)\n\n#columnas a agregar\n#Datetime dias lu-1/ma-2/mi-3/ju-4/vi-5/sa-6/do-7 \n#Date time horas y capaz minutos\n#horario laboral ejemplo 7 a 17hs (analizarlo) sea en el dataset y por fuera\n#horario business , para los que son business ver un pseudo horario de apertura y cierre como afecta y si se puede\n#Dia 6hs a 18hs / noche de 18hs a 6hs ver si lo adapto con 4 columnas por las estaciones\n#hora de dormir ejemplo 22hs a 6hs\n#dias festivos y vacaciones , columnas binarias en ambos\n#estaciones del a√±o 4 columnas binarias\n#periodogram para ver los lags y sea mensuales/quincenales/diarios/hora\n#ver si es relevante agregar alguna columna con la poblacion mensual y usar un ratio (info a buscar afuera)\n\n#sugerencias de features\n\n##Energ√©ticas:\n\n# Ratio production/consumption por prediction_unit_id (hist√≥rico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n##Temporales:\n\n# Lag de consumo/producci√≥n del mismo prediction_unit_id (24h, 48h, 168h)\n# Media m√≥vil de target por prediction_unit_id\n# Cambios d√≠a a d√≠a (delta vs d√≠a anterior)\n\n##Weather engineering:\n\n# √çndice de confort t√©rmico (combinando temp + humidity)\n# Potencial solar real (direct + diffuse radiation ajustado por cloudcover)\n# Diferencia forecast vs historical weather (para medir accuracy del forecast)\n\n##Segmentaci√≥n:\n\n# Clustering de prediction_units por comportamiento similar\n# Ratios county-level (agregaciones por county vs individual)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:57:49.265692Z","iopub.execute_input":"2025-09-05T10:57:49.265987Z","iopub.status.idle":"2025-09-05T10:57:49.291242Z","shell.execute_reply.started":"2025-09-05T10:57:49.265965Z","shell.execute_reply":"2025-09-05T10:57:49.290113Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# hacer scatterplot con \n# train3.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) el alpha para ver con mas claridad las zonas mas poblada\n# podria agregar colores para diferencia temperatura o exposicion a rayos solares o nubes y poder encarar distintos el dataset\n# sino funciona probar %matplotlib inline\n# train3.hist(bins=50, figsize=(20,15)) ver tmb los histograma de los datos cuando este todo junto# cambiar los bins a valores mas chicos\n# plt.show() agregar si hace falta\n# corr_matrix = train3.corr()\n# corr_matrix[\"target\"].sort_values(ascending=False) ver si aplica pero creo que no , recordad probar en columnas numericas si hay otra relacion\n# ejemplo crear columna con raiz/potencia/log y ver como apartan esos valores en correlacion\n# o usar scatter_matrix() de pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:57:49.292896Z","iopub.execute_input":"2025-09-05T10:57:49.293327Z","iopub.status.idle":"2025-09-05T10:57:49.313903Z","shell.execute_reply.started":"2025-09-05T10:57:49.293296Z","shell.execute_reply":"2025-09-05T10:57:49.312780Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train                 = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\nclient                = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\nelectricity_prices    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\nforecast_weather      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\ngas_prices            = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\nhistorical_weather    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\nweather_station       = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv')\n\ntest                  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv')\nclient_t              = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv')\nelectricity_prices_t  = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv')\nforecast_weather_t    = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv')\ngas_prices_t          = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv')\nrevealed_targets      = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:57:49.315552Z","iopub.execute_input":"2025-09-05T10:57:49.316286Z","iopub.status.idle":"2025-09-05T10:58:11.299473Z","shell.execute_reply.started":"2025-09-05T10:57:49.316249Z","shell.execute_reply":"2025-09-05T10:58:11.298435Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# pruebo pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:58:11.300439Z","iopub.execute_input":"2025-09-05T10:58:11.300801Z","iopub.status.idle":"2025-09-05T10:58:11.306076Z","shell.execute_reply.started":"2025-09-05T10:58:11.300770Z","shell.execute_reply":"2025-09-05T10:58:11.305015Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport unicodedata\nfrom sklearn.neighbors import NearestNeighbors\nimport gc\nfrom collections import defaultdict\nfrom pathlib import Path\n\nclass DataPreprocessingPipeline:\n    \"\"\"Pipeline modular para preprocessing de datos de train y test\"\"\"\n    \n    def __init__(self):\n        self.county_id_to_name_map = {\n            0: \"HARJUMAA\", 1: \"HIIUMAA\", 2: \"IDA-VIRUMAA\", 3: \"J√ÑRVAMAA\",\n            4: \"J√ïGEVAMAA\", 5: \"L√Ñ√ÑNE-VIRUMAA\", 6: \"L√Ñ√ÑNEMAA\", 7: \"P√ÑRNUMAA\",\n            8: \"P√ïLVAMAA\", 9: \"RAPLAMAA\", 10: \"SAAREMAA\", 11: \"TARTUMAA\",\n            12: \"UNKNOWN\", 13: \"VALGAMAA\", 14: \"VILJANDIMAA\", 15: \"V√ïRUMAA\"\n        }\n        self.county_name_to_id_map = {v: k for k, v in self.county_id_to_name_map.items()}\n        \n        # Diccionario de sin√≥nimos para normalizaci√≥n\n        self.synonyms_to_canonical = {\n            \"HARJU\": \"HARJUMAA\", \"HIIU\": \"HIIUMAA\", \"IDA-VIRU\": \"IDA-VIRUMAA\",\n            \"JARVA\": \"J√ÑRVAMAA\", \"J√ÑRVA\": \"J√ÑRVAMAA\", \"JOGEVA\": \"J√ïGEVAMAA\", \n            \"J√ïGEVA\": \"J√ïGEVAMAA\", \"LAANE-VIRU\": \"L√Ñ√ÑNE-VIRUMAA\", \n            \"L√Ñ√ÑNE-VIRU\": \"L√Ñ√ÑNE-VIRUMAA\", \"LAANE\": \"L√Ñ√ÑNEMAA\", \"L√Ñ√ÑNE\": \"L√Ñ√ÑNEMAA\",\n            \"PARNU\": \"P√ÑRNUMAA\", \"P√ÑRNU\": \"P√ÑRNUMAA\", \"POLVA\": \"P√ïLVAMAA\", \n            \"P√ïLVA\": \"P√ïLVAMAA\", \"RAPLA\": \"RAPLAMAA\", \"SAARE\": \"SAAREMAA\",\n            \"TARTU\": \"TARTUMAA\", \"VALGA\": \"VALGAMAA\", \"VILJANDI\": \"VILJANDIMAA\",\n            \"VORU\": \"V√ïRUMAA\", \"V√ïRU\": \"V√ïRUMAA\",\n            # versiones ya can√≥nicas\n            **{name: name for name in self.county_id_to_name_map.values()}\n        }\n    \n    def strip_accents(self, s: str) -> str:\n        \"\"\"Eliminar acentos de string\"\"\"\n        return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n    \n    def normalize_county_name(self, x):\n        \"\"\"Normalizar nombre de county\"\"\"\n        if pd.isna(x):\n            return np.nan\n        s = str(x).upper().strip()\n        # eliminar sufijos frecuentes\n        for suf in [\" COUNTY\", \" MAAKOND\"]:\n            if s.endswith(suf):\n                s = s[: -len(suf)]\n        s = s.replace(\"  \", \" \").replace(\"‚Äì\", \"-\")\n        s_noacc = self.strip_accents(s)\n        s_noacc = s_noacc.replace(\"  \", \" \")\n        \n        token = s_noacc.split()[0]\n        if \"-\" in s_noacc:\n            token = s_noacc\n        \n        # intentos de match\n        if s_noacc in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s_noacc]\n        if token in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[token]\n        if s in self.synonyms_to_canonical:\n            return self.synonyms_to_canonical[s]\n        \n        return np.nan\n    \n    def process_weather_stations(self, weather_station):\n        \"\"\"Procesar estaciones meteorol√≥gicas para asignar counties\"\"\"\n        weather_station = weather_station.copy()\n        \n        # Normalizar nombres de county\n        weather_station['county_name_norm'] = weather_station['county_name'].apply(self.normalize_county_name)\n        \n        # Si tengo 'county' num√©rico pero falta nombre, lo inferimos del ID\n        mask = weather_station['county_name_norm'].isna() & weather_station['county'].notna()\n        weather_station.loc[mask, 'county_name_norm'] = (\n            weather_station.loc[mask, 'county'].astype(int).map(self.county_id_to_name_map)\n        )\n        \n        # Completar faltantes por k-NN\n        fcols = ['latitude', 'longitude']\n        known = weather_station.dropna(subset=['county_name_norm']).copy()\n        unknown = weather_station[weather_station['county_name_norm'].isna()].copy()\n        \n        if not unknown.empty and not known.empty:\n            known_rad = np.radians(known[fcols].values)\n            unknown_rad = np.radians(unknown[fcols].values)\n            \n            nbrs = NearestNeighbors(n_neighbors=3, metric='haversine')\n            nbrs.fit(known_rad)\n            distances, idxs = nbrs.kneighbors(unknown_rad)\n            \n            nn_names = known['county_name_norm'].values\n            filled = []\n            for neigh_idx in idxs:\n                cands = nn_names[neigh_idx]\n                vals, counts = np.unique(cands, return_counts=True)\n                filled.append(vals[np.argmax(counts)])\n            \n            weather_station.loc[unknown.index, 'county_name_norm'] = filled\n        \n        # Sincronizar columnas finales\n        weather_station['county_name'] = weather_station['county_name_norm']\n        weather_station['county'] = weather_station['county_name'].map(self.county_name_to_id_map).astype('Int64')\n        \n        # Cualquier remanente a UNKNOWN (12)\n        weather_station.loc[weather_station['county_name'].isna(), 'county'] = 12\n        weather_station['county'] = weather_station['county'].astype('Int64')\n        \n        weather_station.drop(columns=['county_name_norm'], inplace=True)\n        \n        return weather_station\n    \n    def optimize_dtypes(self, df, is_train=True):\n        \"\"\"Optimizar tipos de datos para reducir memoria\"\"\"\n        df = df.copy()\n        \n        # Tipos comunes\n        if 'county' in df.columns:\n            df['county'] = df['county'].astype('uint8')\n        if 'is_business' in df.columns:\n            df['is_business'] = df['is_business'].astype('uint8')\n        if 'product_type' in df.columns:\n            df['product_type'] = df['product_type'].astype('uint8')\n        if 'is_consumption' in df.columns:\n            df['is_consumption'] = df['is_consumption'].astype('uint8')\n        if 'hour' in df.columns:\n            df['hour'] = df['hour'].astype('uint8')\n        \n        # IDs\n        if 'data_block_id' in df.columns:\n            df['data_block_id'] = df['data_block_id'].astype('uint16')\n        if 'row_id' in df.columns:\n            df['row_id'] = df['row_id'].astype('uint32')\n        if 'prediction_unit_id' in df.columns:\n            df['prediction_unit_id'] = df['prediction_unit_id'].astype('uint8')\n        \n        # Coordenadas y variables meteorol√≥gicas - float32\n        float_cols = ['latitude', 'longitude', 'target', 'lowest_price_per_mwh', \n                     'highest_price_per_mwh', 'euros_per_mwh', 'eic_count', \n                     'installed_capacity']\n        \n        # Variables meteorol√≥gicas\n        weather_cols = ['ftemperature', 'fdewpoint', 'fcloudcover_high', 'fcloudcover_low', \n                       'fcloudcover_mid', 'fcloudcover_total', 'f10_metre_u_wind_component', \n                       'f10_metre_v_wind_component', 'fdirect_solar_radiation', \n                       'fsurface_solar_radiation_downwards', 'fsnowfall', 'ftotal_precipitation',\n                       'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure',\n                       'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n                       'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', \n                       'direct_solar_radiation', 'diffuse_radiation']\n        \n        # Aplicar float32 a columnas que existen\n        for col in float_cols + weather_cols:\n            if col in df.columns:\n                df[col] = df[col].astype('float32')\n        \n        # Weather forecast hour\n        if 'weather_forecast_hour' in df.columns:\n            df['weather_forecast_hour'] = df['weather_forecast_hour'].astype('uint8')\n        \n        return df\n\n\nclass TrainPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline espec√≠fico para datos de entrenamiento\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.temp_dir = Path(\"temp_chunks\")\n        self.temp_dir.mkdir(exist_ok=True)\n    \n    def part1_prepare_base_merges(self, train, gas_prices, electricity_prices, client):\n        \"\"\"Parte 1: Preparar datos base y hacer primeros merges\"\"\"\n        print(\"=== PARTE 1: Preparaci√≥n y merges base ===\")\n        \n        # Preparar copias\n        train1 = train.dropna().copy()\n        gas_prices1 = gas_prices.copy()\n        electricity_prices1 = electricity_prices.copy()\n        client1 = client.copy()\n        \n        # Procesar train\n        train1['datetime'] = pd.to_datetime(train1['datetime'])\n        train1['hour'] = train1['datetime'].dt.hour\n        train1['forecast_date'] = train1['datetime']\n        \n        # Merge 1: Gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        gas_prices1 = gas_prices1.rename(columns={'origin_date': 'gas_origin_date'})\n        \n        # Expandir gas a nivel horario\n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices1_hourly = pd.DataFrame(gas_hourly)\n        train2 = pd.merge(train1, gas_prices1_hourly, on='forecast_date', how='left')\n        \n        # Forward fill gas prices\n        train2['lowest_price_per_mwh'] = train2['lowest_price_per_mwh'].fillna(method='ffill')\n        train2['highest_price_per_mwh'] = train2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge 2: Electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        elec_columns = ['forecast_date', 'euros_per_mwh']\n        train3 = pd.merge(train2, electricity_prices1[elec_columns], on='forecast_date', how='left')\n        train3['euros_per_mwh'] = train3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge 3: Client data\n        print(\"Procesando client data...\")\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        train4 = train3.merge(client1.drop('data_block_id', axis=1),\n                             left_on=['product_type', 'county', 'is_business', train3['datetime'].dt.date],\n                             right_on=['product_type', 'county', 'is_business', 'date'],\n                             how='left')\n        \n        train4 = train4.drop('date', axis=1)\n        train4 = train4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        print(\"Completando datos de clientes...\")\n        train4 = train4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        train4['eic_count'] = train4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        train4['installed_capacity'] = train4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        train4['eic_count'] = train4['eic_count'].fillna(train4.groupby('county')['eic_count'].transform('mean'))\n        train4['installed_capacity'] = train4['installed_capacity'].fillna(train4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Optimizar tipos\n        train4 = self.optimize_dtypes(train4, is_train=True)\n        \n        print(f\"Parte 1 completada. Dataset: {train4.shape[0]:,} filas, {train4.shape[1]} columnas\")\n        return train4\n    \n    def part2_prepare_weather_merge(self, train4, forecast_weather, historical_weather, weather_station):\n        \"\"\"Parte 2: Preparar datos meteorol√≥gicos y hacer merge\"\"\"\n        print(\"=== PARTE 2: Preparaci√≥n datos meteorol√≥gicos ===\")\n        \n        # Procesar weather stations\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        forecast_weather1 = forecast_weather.dropna().copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Merge con historical weather\n        print(\"Mergeando forecast con historical weather...\")\n        historical_weather1 = historical_weather.copy()\n        historical_weather1['datetime'] = pd.to_datetime(historical_weather1['datetime'])\n        \n        forecast_weather2 = forecast_weather2.rename(columns={'data_block_id': 'f_data_block_id'})\n        \n        merged_weather = pd.merge(\n            forecast_weather2,\n            historical_weather1,\n            left_on=['latitude', 'longitude', 'forecast_datetime'],\n            right_on=['latitude', 'longitude', 'datetime'],\n            how='inner'\n        )\n        \n        # Limpiar y optimizar\n        merged_weather = merged_weather.drop(['f_data_block_id', 'forecast_datetime'], axis=1)\n        merged_weather = self.optimize_dtypes(merged_weather, is_train=True)\n        \n        print(f\"Weather data preparado: {merged_weather.shape[0]:,} filas, {merged_weather.shape[1]} columnas\")\n        return train4, merged_weather\n    \n    def part3_final_merge_and_cleanup(self, train4, merged_weather, chunk_size=1_000_000):\n        \"\"\"Parte 3: Merge final y limpieza\"\"\"\n        print(\"=== PARTE 3: Merge final y limpieza ===\")\n        \n        # Definir per√≠odos para procesar por chunks\n        periods = [\n            ('2021-09-01', '2021-10-01'), ('2021-10-01', '2021-11-01'), ('2021-11-01', '2021-12-01'), \n            ('2021-12-01', '2022-01-01'), ('2022-01-01', '2022-02-01'), ('2022-02-01', '2022-03-01'), \n            ('2022-03-01', '2022-04-01'), ('2022-04-01', '2022-05-01'), ('2022-05-01', '2022-06-01'), \n            ('2022-06-01', '2022-07-01'), ('2022-07-01', '2022-08-01'), ('2022-08-01', '2022-09-01'),\n            ('2022-09-01', '2022-10-01'), ('2022-10-01', '2022-11-01'), ('2022-11-01', '2022-12-01'), \n            ('2022-12-01', '2023-01-01'), ('2023-01-01', '2023-02-01'), ('2023-02-01', '2023-03-01'), \n            ('2023-03-01', '2023-04-01'), ('2023-04-01', '2023-05-01'), ('2023-05-01', '2023-05-31')\n        ]\n        \n        # Procesar por chunks temporales\n        chunk_files = []\n        for i, (start_date, end_date) in enumerate(periods):\n            print(f\"Procesando per√≠odo {i+1}/{len(periods)}: {start_date} a {end_date}\")\n            \n            # Filtrar por per√≠odo\n            mask_train = (train4['datetime'] >= start_date) & (train4['datetime'] < end_date)\n            train4_chunk = train4[mask_train].copy()\n            \n            mask_weather = (merged_weather['datetime'] >= start_date) & (merged_weather['datetime'] < end_date)\n            weather_chunk = merged_weather[mask_weather].copy()\n            \n            if len(train4_chunk) > 0 and len(weather_chunk) > 0:\n                # Merge chunk\n                chunk_result = pd.merge(train4_chunk, weather_chunk, \n                               on=['county', 'datetime'], \n                               how='left')\n                \n                # Guardar chunk temporal\n                chunk_file = self.temp_dir / f'train5_chunk_{i+1}.parquet'\n                chunk_result.to_parquet(chunk_file, index=False)\n                chunk_files.append(chunk_file)\n                \n                print(f\"Chunk {i+1} guardado: {chunk_result.shape}\")\n                \n                # Liberar memoria\n                del train4_chunk, weather_chunk, chunk_result\n                gc.collect()\n        \n        # Concatenar chunks\n        print(\"Concatenando chunks...\")\n        final_chunks = []\n        for chunk_file in chunk_files:\n            chunk_data = pd.read_parquet(chunk_file)\n            final_chunks.append(chunk_data)\n        \n        train5 = pd.concat(final_chunks, ignore_index=True)\n        \n        # Limpieza final\n        print(\"Aplicando limpieza final...\")\n        train5 = self._cleanup_merged_data(train5, chunk_size)\n        \n        # Limpiar archivos temporales\n        for chunk_file in chunk_files:\n            chunk_file.unlink()\n        \n        return train5\n    \n    def _cleanup_merged_data(self, train5, chunk_size=1_000_000):\n        \"\"\"Limpiar datos despu√©s del merge\"\"\"\n        # Identificar columnas a eliminar y renombrar\n        sample = train5.head(1000)\n        cols_to_drop = [col for col in sample.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in sample.columns if col.endswith('_x')}\n        \n        print(f\"Eliminando {len(cols_to_drop)} columnas, renombrando {len(rename_dict)}\")\n        \n        # Procesar por chunks\n        chunks_processed = []\n        total_rows = len(train5)\n        n_chunks = (total_rows // chunk_size) + 1\n        \n        for i in range(n_chunks):\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, total_rows)\n            \n            print(f\"Procesando chunk limpieza {i+1}/{n_chunks}: filas {start_idx:,} a {end_idx:,}\")\n            \n            chunk = train5.iloc[start_idx:end_idx].copy()\n            \n            # Filtrar county != 12\n            chunk = chunk[chunk['county'] != 12]\n            \n            if len(chunk) > 0:\n                # Eliminar columnas _y y renombrar _x\n                chunk = chunk.drop(columns=cols_to_drop)\n                chunk.rename(columns=rename_dict, inplace=True)\n                chunks_processed.append(chunk)\n            \n            del chunk\n            gc.collect()\n        \n        # Concatenar final\n        train5_clean = pd.concat(chunks_processed, ignore_index=True)\n        del chunks_processed\n        gc.collect()\n        \n        # Eliminar datos de clima faltantes\n        weather_key_cols = ['latitude', 'longitude', 'ftemperature', 'temperature']\n        initial_rows = len(train5_clean)\n        train5_clean = train5_clean.dropna(subset=weather_key_cols)\n        print(f\"Filas eliminadas por clima faltante: {initial_rows - len(train5_clean):,}\")\n        \n        # Reordenar columnas\n        train5_clean = self._reorder_columns(train5_clean)\n        \n        return train5_clean\n    \n    def _reorder_columns(self, df):\n        \"\"\"Reordenar columnas en el orden especificado\"\"\"\n        main_cols = [\n            'row_id', 'target', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = main_cols.copy()\n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n    \n    def run_full_pipeline(self, train, gas_prices, electricity_prices, client, \n                         forecast_weather, historical_weather, weather_station):\n        \"\"\"Ejecutar pipeline completo de entrenamiento\"\"\"\n        print(\"Iniciando pipeline completo de entrenamiento...\")\n        \n        # Parte 1: Merges base\n        train4 = self.part1_prepare_base_merges(train, gas_prices, electricity_prices, client)\n        \n        # Parte 2: Preparar datos meteorol√≥gicos\n        train4, merged_weather = self.part2_prepare_weather_merge(\n            train4, forecast_weather, historical_weather, weather_station\n        )\n        \n        # Parte 3: Merge final y limpieza\n        train5 = self.part3_final_merge_and_cleanup(train4, merged_weather)\n        \n        print(f\"Pipeline completado! Dataset final: {train5.shape[0]:,} filas, {train5.shape[1]} columnas\")\n        return train5\n\n\nclass TestPipeline(DataPreprocessingPipeline):\n    \"\"\"Pipeline simplificado para datos de test\"\"\"\n    \n    def run_test_pipeline(self, test, gas_prices_t, electricity_prices_t, client_t, \n                         forecast_weather_t, weather_station):\n        \"\"\"Pipeline completo para test set (m√°s simple, sin historical weather)\"\"\"\n        print(\"=== PIPELINE TEST ===\")\n        \n        # Preparar datos base\n        test1 = test.copy()\n        test1['datetime'] = pd.to_datetime(test1['prediction_datetime'])\n        test1['hour'] = test1['datetime'].dt.hour\n        test1['forecast_date'] = test1['datetime']\n        \n        # Merge gas prices\n        print(\"Procesando gas prices...\")\n        gas_prices1 = gas_prices_t.copy()\n        gas_prices1['origin_date'] = pd.to_datetime(gas_prices1['origin_date'])\n        gas_prices1['forecast_date'] = pd.to_datetime(gas_prices1['forecast_date'])\n        \n        gas_hourly = []\n        for _, row in gas_prices1.iterrows():\n            for hour in range(24):\n                new_row = {\n                    'forecast_date': pd.to_datetime(row['forecast_date']) + pd.Timedelta(hours=hour),\n                    'lowest_price_per_mwh': row['lowest_price_per_mwh'],\n                    'highest_price_per_mwh': row['highest_price_per_mwh']\n                }\n                gas_hourly.append(new_row)\n        \n        gas_prices_hourly = pd.DataFrame(gas_hourly)\n        test2 = pd.merge(test1, gas_prices_hourly, on='forecast_date', how='left')\n        test2['lowest_price_per_mwh'] = test2['lowest_price_per_mwh'].fillna(method='ffill')\n        test2['highest_price_per_mwh'] = test2['highest_price_per_mwh'].fillna(method='ffill')\n        \n        # Merge electricity prices\n        print(\"Procesando electricity prices...\")\n        electricity_prices1 = electricity_prices_t.copy()\n        electricity_prices1['forecast_date'] = pd.to_datetime(electricity_prices1['forecast_date'])\n        electricity_prices1 = electricity_prices1.drop(columns=['origin_date'])\n        \n        test3 = pd.merge(test2, electricity_prices1[['forecast_date', 'euros_per_mwh']], \n                        on='forecast_date', how='left')\n        test3['euros_per_mwh'] = test3['euros_per_mwh'].fillna(method='ffill')\n        \n        # Merge client data\n        print(\"Procesando client data...\")\n        client1 = client_t.copy()\n        client1['date'] = pd.to_datetime(client1['date']).dt.date\n        \n        test4 = test3.merge(client1.drop('data_block_id', axis=1),\n                           left_on=['product_type', 'county', 'is_business', test3['datetime'].dt.date],\n                           right_on=['product_type', 'county', 'is_business', 'date'],\n                           how='left')\n        \n        test4 = test4.drop('date', axis=1)\n        test4 = test4.rename(columns={'data_block_id_x': 'data_block_id'})\n        \n        # Forward fill client data\n        test4 = test4.sort_values(['county', 'is_business', 'product_type', 'datetime'])\n        test4['eic_count'] = test4.groupby(['county', 'is_business', 'product_type'])['eic_count'].fillna(method='ffill')\n        test4['installed_capacity'] = test4.groupby(['county', 'is_business', 'product_type'])['installed_capacity'].fillna(method='ffill')\n        \n        # Rellenar restantes con promedio por county\n        test4['eic_count'] = test4['eic_count'].fillna(test4.groupby('county')['eic_count'].transform('mean'))\n        test4['installed_capacity'] = test4['installed_capacity'].fillna(test4.groupby('county')['installed_capacity'].transform('mean'))\n        \n        # Procesar forecast weather\n        print(\"Procesando forecast weather...\")\n        weather_station_processed = self.process_weather_stations(weather_station)\n        \n        forecast_weather1 = forecast_weather_t.copy()\n        \n        # Merge forecast con weather stations\n        forecast_weather1 = forecast_weather1.merge(\n            weather_station_processed[['latitude', 'longitude', 'county', 'county_name']], \n            on=['latitude', 'longitude'], \n            how='left'\n        )\n        \n        # Completar counties faltantes con k-NN\n        missing_county = forecast_weather1['county'].isna()\n        if missing_county.sum() > 0:\n            print(f\"Completando {missing_county.sum()} counties faltantes con k-NN...\")\n            known = weather_station_processed[['latitude', 'longitude', 'county', 'county_name']].copy()\n            missing_coords = forecast_weather1.loc[missing_county, ['latitude', 'longitude']].copy()\n            \n            nbrs = NearestNeighbors(n_neighbors=1, metric='euclidean')\n            nbrs.fit(known[['latitude', 'longitude']].values)\n            distances, indices = nbrs.kneighbors(missing_coords[['latitude', 'longitude']].values)\n            \n            nearest_counties = known.iloc[indices.flatten()]\n            forecast_weather1.loc[missing_county, 'county'] = nearest_counties['county'].values\n            forecast_weather1.loc[missing_county, 'county_name'] = nearest_counties['county_name'].values\n        \n        # Transformar forecast weather\n        forecast_weather2 = forecast_weather1.copy()\n        forecast_weather2 = forecast_weather2.drop(['county_name', 'origin_datetime'], axis=1)\n        forecast_weather2['forecast_datetime'] = pd.to_datetime(forecast_weather2['forecast_datetime'])\n        \n        # Renombrar con prefijo 'f'\n        rename_dict = {}\n        for col in forecast_weather2.columns:\n            if col not in ['county', 'hours_ahead', 'latitude', 'longitude', 'data_block_id', \n                          'forecast_datetime']:\n                rename_dict[col] = 'f' + col\n        \n        forecast_weather2 = forecast_weather2.rename(columns=rename_dict)\n        forecast_weather2 = forecast_weather2.rename(columns={\"hours_ahead\": \"weather_forecast_hour\"})\n        \n        # Para test, no tenemos historical weather, as√≠ que creamos columnas dummy o usamos solo forecast\n        # Opci√≥n 1: Solo usar forecast weather (renombrar las f columns)\n        # Opci√≥n 2: Crear columnas hist√≥ricas como NaN y llenar despu√©s\n        \n        # Vamos con opci√≥n 1: usar forecast como hist√≥rico tambi√©n (aproximaci√≥n)\n        historical_cols_mapping = {\n            'ftemperature': 'temperature',\n            'fdewpoint': 'dewpoint', \n            'fcloudcover_high': 'cloudcover_high',\n            'fcloudcover_low': 'cloudcover_low',\n            'fcloudcover_mid': 'cloudcover_mid', \n            'fcloudcover_total': 'cloudcover_total',\n            'fdirect_solar_radiation': 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards': 'shortwave_radiation',\n            'fsnowfall': 'snowfall',\n            'ftotal_precipitation': 'rain'\n        }\n        \n        # Crear versiones hist√≥ricas basadas en forecast\n        for fcol, hcol in historical_cols_mapping.items():\n            if fcol in forecast_weather2.columns:\n                forecast_weather2[hcol] = forecast_weather2[fcol]\n        \n        # Agregar columnas que solo existen en historical\n        forecast_weather2['surface_pressure'] = 1013.25  # valor t√≠pico\n        forecast_weather2['windspeed_10m'] = np.sqrt(\n            forecast_weather2['f10_metre_u_wind_component']**2 + \n            forecast_weather2['f10_metre_v_wind_component']**2\n        ) if 'f10_metre_u_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['winddirection_10m'] = np.arctan2(\n            forecast_weather2['f10_metre_v_wind_component'], \n            forecast_weather2['f10_metre_u_wind_component']\n        ) * 180 / np.pi if 'f10_metre_v_wind_component' in forecast_weather2.columns else 0\n        \n        forecast_weather2['diffuse_radiation'] = forecast_weather2.get('fsurface_solar_radiation_downwards', 0) - forecast_weather2.get('fdirect_solar_radiation', 0)\n        forecast_weather2['diffuse_radiation'] = forecast_weather2['diffuse_radiation'].clip(lower=0)\n        \n        # Limpiar y optimizar\n        forecast_weather2 = forecast_weather2.drop(columns=['data_block_id'], errors='ignore')\n        forecast_weather2 = self.optimize_dtypes(forecast_weather2, is_train=False)\n        \n        # Merge final con weather\n        print(\"Merge final con datos meteorol√≥gicos...\")\n        test5 = pd.merge(test4, forecast_weather2, \n                        left_on=['datetime', 'county'], \n                        right_on=['forecast_datetime', 'county'], \n                        how='left')\n\n\n\n        # Limpiar columnas duplicadas y renombrar\n        cols_to_drop = [col for col in test5.columns if col.endswith('_y')]\n        rename_dict = {col: col[:-2] for col in test5.columns if col.endswith('_x')}\n        \n        test5 = test5.drop(columns=cols_to_drop + ['forecast_datetime'], errors='ignore')\n        test5.rename(columns=rename_dict, inplace=True)\n        \n        # Optimizar tipos finales\n        test5 = self.optimize_dtypes(test5, is_train=False)\n        \n        # Reordenar columnas igual que train\n        test5 = self._reorder_columns_test(test5)\n        \n        print(f\"Pipeline test completado! Dataset: {test5.shape[0]:,} filas, {test5.shape[1]} columnas\")\n        return test5\n    \n    def _reorder_columns_test(self, df):\n        \"\"\"Reordenar columnas para test (mismo orden que train)\"\"\"\n        main_cols = [\n            'row_id', 'datetime', 'forecast_date', 'hour', 'data_block_id', \n            'prediction_unit_id', 'is_business', 'product_type', 'county', 'latitude', \n            'longitude', 'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh', \n            'eic_count', 'installed_capacity'\n        ]\n        \n        weather_pairs = [\n            'ftemperature', 'temperature', 'fdewpoint', 'dewpoint',\n            'fcloudcover_high', 'cloudcover_high', 'fcloudcover_low', 'cloudcover_low', \n            'fcloudcover_mid', 'cloudcover_mid', 'fcloudcover_total', 'cloudcover_total',\n            'f10_metre_u_wind_component', 'windspeed_10m', 'f10_metre_v_wind_component', \n            'winddirection_10m', 'fdirect_solar_radiation', 'direct_solar_radiation',\n            'fsurface_solar_radiation_downwards', 'shortwave_radiation', 'diffuse_radiation',\n            'fsnowfall', 'snowfall', 'ftotal_precipitation', 'rain', 'surface_pressure', \n            'weather_forecast_hour'\n        ]\n        \n        final_order = []\n        # Solo agregar columnas que existen\n        for col in main_cols:\n            if col in df.columns:\n                final_order.append(col)\n        \n        for col in weather_pairs:\n            if col in df.columns:\n                final_order.append(col)\n        \n        remaining_cols = [col for col in df.columns if col not in final_order]\n        final_order.extend(remaining_cols)\n        \n        return df[final_order]\n\n\n# Funciones de utilidad para usar el pipeline\ndef run_train_pipeline(train, gas_prices, electricity_prices, client, \n                      forecast_weather, historical_weather, weather_station):\n    \"\"\"\n    Funci√≥n principal para ejecutar pipeline de entrenamiento\n    \n    Returns:\n        pd.DataFrame: Dataset procesado train5\n    \"\"\"\n    pipeline = TrainPipeline()\n    return pipeline.run_full_pipeline(\n        train, gas_prices, electricity_prices, client, \n        forecast_weather, historical_weather, weather_station\n    )\n\ndef run_test_pipeline(test, gas_prices_t, electricity_prices_t, client_t, \n                     forecast_weather_t, weather_station):\n    \"\"\"\n    Funci√≥n principal para ejecutar pipeline de test\n    \n    Returns:\n        pd.DataFrame: Dataset procesado test5\n    \"\"\"\n    pipeline = TestPipeline()\n    return pipeline.run_test_pipeline(\n        test, gas_prices_t, electricity_prices_t, client_t, \n        forecast_weather_t, weather_station\n    )\n\n# Ejemplo de uso:\n\"\"\"\n# Para train:\ntrain5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)\n\n# Para test:\ntest5 = run_test_pipeline(\n    test, gas_prices_t, electricity_prices_t, client_t, \n    forecast_weather_t, weather_station\n)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:58:11.307419Z","iopub.execute_input":"2025-09-05T10:58:11.307779Z","iopub.status.idle":"2025-09-05T10:58:11.419867Z","shell.execute_reply.started":"2025-09-05T10:58:11.307754Z","shell.execute_reply":"2025-09-05T10:58:11.418848Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n# Para train:\\ntrain5 = run_train_pipeline(\\n    train, gas_prices, electricity_prices, client, \\n    forecast_weather, historical_weather, weather_station\\n)\\n\\n# Para test:\\ntest5 = run_test_pipeline(\\n    test, gas_prices_t, electricity_prices_t, client_t, \\n    forecast_weather_t, weather_station\\n)\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train5 = run_train_pipeline(\n    train, gas_prices, electricity_prices, client, \n    forecast_weather, historical_weather, weather_station\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:58:11.421162Z","iopub.execute_input":"2025-09-05T10:58:11.421632Z","iopub.status.idle":"2025-09-05T11:00:03.953904Z","shell.execute_reply.started":"2025-09-05T10:58:11.421597Z","shell.execute_reply":"2025-09-05T11:00:03.952415Z"}},"outputs":[{"name":"stdout","text":"Iniciando pipeline completo de entrenamiento...\n=== PARTE 1: Preparaci√≥n y merges base ===\nProcesando gas prices...\nProcesando electricity prices...\nProcesando client data...\nCompletando datos de clientes...\nParte 1 completada. Dataset: 2,017,824 filas, 16 columnas\n=== PARTE 2: Preparaci√≥n datos meteorol√≥gicos ===\nProcesando forecast weather...\nCompletando 2140318 counties faltantes con k-NN...\nMergeando forecast con historical weather...\nWeather data preparado: 3,418,242 filas, 32 columnas\n=== PARTE 3: Merge final y limpieza ===\nProcesando per√≠odo 1/21: 2021-09-01 a 2021-10-01\nChunk 1 guardado: (1423080, 46)\nProcesando per√≠odo 2/21: 2021-10-01 a 2021-11-01\nChunk 2 guardado: (1532066, 46)\nProcesando per√≠odo 3/21: 2021-11-01 a 2021-12-01\nChunk 3 guardado: (1484640, 46)\nProcesando per√≠odo 4/21: 2021-12-01 a 2022-01-01\nChunk 4 guardado: (1543056, 46)\nProcesando per√≠odo 5/21: 2022-01-01 a 2022-02-01\nChunk 5 guardado: (1590672, 46)\nProcesando per√≠odo 6/21: 2022-02-01 a 2022-03-01\nChunk 6 guardado: (1441632, 46)\nProcesando per√≠odo 7/21: 2022-03-01 a 2022-04-01\nChunk 7 guardado: (1597678, 46)\nProcesando per√≠odo 8/21: 2022-04-01 a 2022-05-01\nChunk 8 guardado: (1630080, 46)\nProcesando per√≠odo 9/21: 2022-05-01 a 2022-06-01\nChunk 9 guardado: (1644240, 46)\nProcesando per√≠odo 10/21: 2022-06-01 a 2022-07-01\nChunk 10 guardado: (1573920, 46)\nProcesando per√≠odo 11/21: 2022-07-01 a 2022-08-01\nChunk 11 guardado: (1581744, 46)\nProcesando per√≠odo 12/21: 2022-08-01 a 2022-09-01\nChunk 12 guardado: (1581716, 46)\nProcesando per√≠odo 13/21: 2022-09-01 a 2022-10-01\nChunk 13 guardado: (1573920, 46)\nProcesando per√≠odo 14/21: 2022-10-01 a 2022-11-01\nChunk 14 guardado: (1686610, 46)\nProcesando per√≠odo 15/21: 2022-11-01 a 2022-12-01\nChunk 15 guardado: (1643084, 46)\nProcesando per√≠odo 16/21: 2022-12-01 a 2023-01-01\nChunk 16 guardado: (1669008, 46)\nProcesando per√≠odo 17/21: 2023-01-01 a 2023-02-01\nChunk 17 guardado: (1654608, 46)\nProcesando per√≠odo 18/21: 2023-02-01 a 2023-03-01\nChunk 18 guardado: (1494048, 46)\nProcesando per√≠odo 19/21: 2023-03-01 a 2023-04-01\nChunk 19 guardado: (1604926, 46)\nProcesando per√≠odo 20/21: 2023-04-01 a 2023-05-01\nChunk 20 guardado: (1514016, 46)\nProcesando per√≠odo 21/21: 2023-05-01 a 2023-05-31\nChunk 21 guardado: (1530872, 46)\nConcatenando chunks...\nAplicando limpieza final...\nEliminando 1 columnas, renombrando 1\nProcesando chunk limpieza 1/33: filas 0 a 1,000,000\nProcesando chunk limpieza 2/33: filas 1,000,000 a 2,000,000\nProcesando chunk limpieza 3/33: filas 2,000,000 a 3,000,000\nProcesando chunk limpieza 4/33: filas 3,000,000 a 4,000,000\nProcesando chunk limpieza 5/33: filas 4,000,000 a 5,000,000\nProcesando chunk limpieza 6/33: filas 5,000,000 a 6,000,000\nProcesando chunk limpieza 7/33: filas 6,000,000 a 7,000,000\nProcesando chunk limpieza 8/33: filas 7,000,000 a 8,000,000\nProcesando chunk limpieza 9/33: filas 8,000,000 a 9,000,000\nProcesando chunk limpieza 10/33: filas 9,000,000 a 10,000,000\nProcesando chunk limpieza 11/33: filas 10,000,000 a 11,000,000\nProcesando chunk limpieza 12/33: filas 11,000,000 a 12,000,000\nProcesando chunk limpieza 13/33: filas 12,000,000 a 13,000,000\nProcesando chunk limpieza 14/33: filas 13,000,000 a 14,000,000\nProcesando chunk limpieza 15/33: filas 14,000,000 a 15,000,000\nProcesando chunk limpieza 16/33: filas 15,000,000 a 16,000,000\nProcesando chunk limpieza 17/33: filas 16,000,000 a 17,000,000\nProcesando chunk limpieza 18/33: filas 17,000,000 a 18,000,000\nProcesando chunk limpieza 19/33: filas 18,000,000 a 19,000,000\nProcesando chunk limpieza 20/33: filas 19,000,000 a 20,000,000\nProcesando chunk limpieza 21/33: filas 20,000,000 a 21,000,000\nProcesando chunk limpieza 22/33: filas 21,000,000 a 22,000,000\nProcesando chunk limpieza 23/33: filas 22,000,000 a 23,000,000\nProcesando chunk limpieza 24/33: filas 23,000,000 a 24,000,000\nProcesando chunk limpieza 25/33: filas 24,000,000 a 25,000,000\nProcesando chunk limpieza 26/33: filas 25,000,000 a 26,000,000\nProcesando chunk limpieza 27/33: filas 26,000,000 a 27,000,000\nProcesando chunk limpieza 28/33: filas 27,000,000 a 28,000,000\nProcesando chunk limpieza 29/33: filas 28,000,000 a 29,000,000\nProcesando chunk limpieza 30/33: filas 29,000,000 a 30,000,000\nProcesando chunk limpieza 31/33: filas 30,000,000 a 31,000,000\nProcesando chunk limpieza 32/33: filas 31,000,000 a 32,000,000\nProcesando chunk limpieza 33/33: filas 32,000,000 a 32,995,616\nFilas eliminadas por clima faltante: 2,024\nPipeline completado! Dataset final: 32,963,024 filas, 45 columnas\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train5.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:03.958984Z","iopub.execute_input":"2025-09-05T11:00:03.959800Z","iopub.status.idle":"2025-09-05T11:00:04.019912Z","shell.execute_reply.started":"2025-09-05T11:00:03.959760Z","shell.execute_reply":"2025-09-05T11:00:04.018806Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"           row_id      target            datetime       forecast_date  hour  \\\n32965017  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965018  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965019  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965020  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n32965021  2013541  444.356995 2023-05-30 10:00:00 2023-05-30 10:00:00    10   \n\n          data_block_id  prediction_unit_id  is_business  product_type  \\\n32965017            636                  60            1             3   \n32965018            636                  60            1             3   \n32965019            636                  60            1             3   \n32965020            636                  60            1             3   \n32965021            636                  60            1             3   \n\n          county   latitude  longitude  lowest_price_per_mwh  \\\n32965017      15  57.599998  28.200001                  29.0   \n32965018      15  57.900002  26.700001                  29.0   \n32965019      15  57.900002  27.200001                  29.0   \n32965020      15  57.900002  27.700001                  29.0   \n32965021      15  57.900002  28.200001                  29.0   \n\n          highest_price_per_mwh  euros_per_mwh  eic_count  installed_capacity  \\\n32965017                   34.0      82.370003       55.0         2188.199951   \n32965018                   34.0      82.370003       55.0         2188.199951   \n32965019                   34.0      82.370003       55.0         2188.199951   \n32965020                   34.0      82.370003       55.0         2188.199951   \n32965021                   34.0      82.370003       55.0         2188.199951   \n\n          ftemperature  temperature  fdewpoint  dewpoint  fcloudcover_high  \\\n32965017     12.136377         13.8   2.330225       3.9               0.0   \n32965018     14.534815         13.9   2.355859       3.3               0.0   \n32965019     13.503565         13.9   3.533350       2.9               0.0   \n32965020     13.150782         13.6   2.600976       3.2               0.0   \n32965021     12.648584         13.6   1.409814       3.6               0.0   \n\n          cloudcover_high  fcloudcover_low  cloudcover_low  fcloudcover_mid  \\\n32965017              0.0         0.000000             7.0         0.966461   \n32965018              0.0         0.270050            15.0         0.544281   \n32965019              0.0         0.024902            18.0         0.181091   \n32965020              0.0         0.000153            18.0         0.866364   \n32965021              0.0         0.000000             9.0         0.997070   \n\n          cloudcover_mid  fcloudcover_total  cloudcover_total  \\\n32965017            71.0           0.966461              49.0   \n32965018            73.0           0.652069              57.0   \n32965019            66.0           0.184845              56.0   \n32965020            66.0           0.866455              56.0   \n32965021            78.0           0.997070              55.0   \n\n          f10_metre_u_wind_component  windspeed_10m  \\\n32965017                   -1.009770       2.055556   \n32965018                    2.205562       2.916667   \n32965019                    0.538569       3.194444   \n32965020                   -0.223149       3.527778   \n32965021                   -0.198003       3.194444   \n\n          f10_metre_v_wind_component  winddirection_10m  \\\n32965017                   -0.927298              346.0   \n32965018                   -1.008352              297.0   \n32965019                   -1.362844              309.0   \n32965020                   -1.600393              317.0   \n32965021                   -2.190237              319.0   \n\n          fdirect_solar_radiation  direct_solar_radiation  \\\n32965017                 1.858164                   155.0   \n32965018               832.791504                   249.0   \n32965019               486.835938                   257.0   \n32965020                26.035942                   240.0   \n32965021                63.440388                   186.0   \n\n          fsurface_solar_radiation_downwards  shortwave_radiation  \\\n32965017                          191.511948                410.0   \n32965018                          685.467529                490.0   \n32965019                          500.649719                498.0   \n32965020                          268.223053                482.0   \n32965021                          304.685272                446.0   \n\n          diffuse_radiation  fsnowfall  snowfall  ftotal_precipitation  rain  \\\n32965017              255.0        0.0       0.0              0.000000   0.1   \n32965018              241.0        0.0       0.0              0.000056   0.1   \n32965019              241.0        0.0       0.0              0.000000   0.0   \n32965020              242.0        0.0       0.0              0.000000   0.0   \n32965021              260.0        0.0       0.0              0.000000   0.0   \n\n          surface_pressure  weather_forecast_hour  is_consumption  \n32965017       1012.799988                    8.0               1  \n32965018       1005.200012                    8.0               1  \n32965019       1007.200012                    8.0               1  \n32965020       1011.099976                    8.0               1  \n32965021       1013.000000                    8.0               1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>32965017</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.599998</td>\n      <td>28.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>12.136377</td>\n      <td>13.8</td>\n      <td>2.330225</td>\n      <td>3.9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>7.0</td>\n      <td>0.966461</td>\n      <td>71.0</td>\n      <td>0.966461</td>\n      <td>49.0</td>\n      <td>-1.009770</td>\n      <td>2.055556</td>\n      <td>-0.927298</td>\n      <td>346.0</td>\n      <td>1.858164</td>\n      <td>155.0</td>\n      <td>191.511948</td>\n      <td>410.0</td>\n      <td>255.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>1012.799988</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965018</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>26.700001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>14.534815</td>\n      <td>13.9</td>\n      <td>2.355859</td>\n      <td>3.3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.270050</td>\n      <td>15.0</td>\n      <td>0.544281</td>\n      <td>73.0</td>\n      <td>0.652069</td>\n      <td>57.0</td>\n      <td>2.205562</td>\n      <td>2.916667</td>\n      <td>-1.008352</td>\n      <td>297.0</td>\n      <td>832.791504</td>\n      <td>249.0</td>\n      <td>685.467529</td>\n      <td>490.0</td>\n      <td>241.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000056</td>\n      <td>0.1</td>\n      <td>1005.200012</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965019</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>27.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>13.503565</td>\n      <td>13.9</td>\n      <td>3.533350</td>\n      <td>2.9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.024902</td>\n      <td>18.0</td>\n      <td>0.181091</td>\n      <td>66.0</td>\n      <td>0.184845</td>\n      <td>56.0</td>\n      <td>0.538569</td>\n      <td>3.194444</td>\n      <td>-1.362844</td>\n      <td>309.0</td>\n      <td>486.835938</td>\n      <td>257.0</td>\n      <td>500.649719</td>\n      <td>498.0</td>\n      <td>241.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1007.200012</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965020</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>27.700001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>13.150782</td>\n      <td>13.6</td>\n      <td>2.600976</td>\n      <td>3.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000153</td>\n      <td>18.0</td>\n      <td>0.866364</td>\n      <td>66.0</td>\n      <td>0.866455</td>\n      <td>56.0</td>\n      <td>-0.223149</td>\n      <td>3.527778</td>\n      <td>-1.600393</td>\n      <td>317.0</td>\n      <td>26.035942</td>\n      <td>240.0</td>\n      <td>268.223053</td>\n      <td>482.0</td>\n      <td>242.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1011.099976</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32965021</th>\n      <td>2013541</td>\n      <td>444.356995</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>10</td>\n      <td>636</td>\n      <td>60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>57.900002</td>\n      <td>28.200001</td>\n      <td>29.0</td>\n      <td>34.0</td>\n      <td>82.370003</td>\n      <td>55.0</td>\n      <td>2188.199951</td>\n      <td>12.648584</td>\n      <td>13.6</td>\n      <td>1.409814</td>\n      <td>3.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.997070</td>\n      <td>78.0</td>\n      <td>0.997070</td>\n      <td>55.0</td>\n      <td>-0.198003</td>\n      <td>3.194444</td>\n      <td>-2.190237</td>\n      <td>319.0</td>\n      <td>63.440388</td>\n      <td>186.0</td>\n      <td>304.685272</td>\n      <td>446.0</td>\n      <td>260.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1013.000000</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Features temporales\ntrain5['day_of_week'] = train5['datetime'].dt.dayofweek\ntrain5['month'] = train5['datetime'].dt.month\n# Crear is_daylight (1 si hay radiaci√≥n solar, 0 si es noche)\ntrain5['is_daylight'] = (train5['direct_solar_radiation'] > 0).astype(int)\n# Crear weekend (1 para s√°bado/domingo, 0 para lunes-viernes)\ntrain5['weekend'] = (train5['day_of_week'] >= 5).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:04.020941Z","iopub.execute_input":"2025-09-05T11:00:04.021663Z","iopub.status.idle":"2025-09-05T11:00:06.505995Z","shell.execute_reply.started":"2025-09-05T11:00:04.021627Z","shell.execute_reply":"2025-09-05T11:00:06.504817Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Mae puesto n1 kaggle 52.3090\n#1er primer modelo xgb sobre train5 haciendo split 80/20 de train y test \n#Mae = 107.9896\n#sin lag sin feature, sin nada , solo haciendo el merge\n\n\n#agrego day of week y month para todos los siguientes\n\n#2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# estas variables de lags las quito\n\n#agrego is_daylight binaria\n\n#4to modelo lgb 14.2% y sobre esto split 80/20 de train y test  \n#de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses 14.2% aprox\n# se que no deberia ser comparable al usar de base distinto , pero parte de aca ahora\n#Mae = 86.6747\n\n#agrego weekend binaria\n\n#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n\n#para todos los demas modelos ya tienen incluido\n#day of week\n#month\n#is_daylight\n#weekend\n\n#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo la saco por ahora a is_active_hours\n\n#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que  la saco a is_sleeping_hours\n\n#empiezo a probar con variables\n##Energ√©ticas:\n\n# Ratio production/consumption por prediction_unit_id (hist√≥rico)\n# Capacidad instalada per capita (installed_capacity / eic_count)\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n\n#9no modelo lgb mantengo las 3 anteriores y prueba una nueva Ratio production/cn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.506937Z","iopub.execute_input":"2025-09-05T11:00:06.507338Z","iopub.status.idle":"2025-09-05T11:00:06.513435Z","shell.execute_reply.started":"2025-09-05T11:00:06.507310Z","shell.execute_reply":"2025-09-05T11:00:06.512383Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n# # Sample 10% manteniendo orden temporal\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# sample_df = train5.sample(frac=0.1, random_state=42).sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"Sample 10%: {len(sample_df):,}\")\n\n# # Convertir datetime\n# sample_df['datetime'] = pd.to_datetime(sample_df['datetime'])\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# # LightGBM (m√°s eficiente en memoria)\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE: {mae:.4f}\")\n# print(f\"Features usadas: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# Dataset original: 32,963,024\n# Sample 10%: 3,296,302\n# MAE: 89.3026\n# Features usadas: 43\n# Train: 2,637,041 | Test: 659,261","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.514654Z","iopub.execute_input":"2025-09-05T11:00:06.515077Z","iopub.status.idle":"2025-09-05T11:00:06.538725Z","shell.execute_reply.started":"2025-09-05T11:00:06.515046Z","shell.execute_reply":"2025-09-05T11:00:06.537560Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # Feature importance\n# importance = model.feature_importances_\n# feature_names = X_train.columns\n\n# # Crear DataFrame y ordenar\n# importance_df = pd.DataFrame({\n#     'feature': feature_names,\n#     'importance': importance\n# }).sort_values('importance', ascending=False)\n\n# # Ver top 15\n# print(\"Top 15 features m√°s importantes:\")\n# print(importance_df.head(15))\n\n# # Plot opcional (si quieres visualizar)\n# plt.figure(figsize=(10, 8))\n# plt.barh(importance_df.head(15)['feature'], importance_df.head(15)['importance'])\n# plt.title('Top 15 Feature Importance')\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.540054Z","iopub.execute_input":"2025-09-05T11:00:06.540475Z","iopub.status.idle":"2025-09-05T11:00:06.560067Z","shell.execute_reply.started":"2025-09-05T11:00:06.540441Z","shell.execute_reply":"2025-09-05T11:00:06.558789Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# importance_df['importance_pct'] = (importance_df['importance'] / importance_df['importance'].sum()) * 100\n# print(importance_df.head(15)[['feature', 'importance', 'importance_pct']])\n\n#                               feature  importance  importance_pct\n# 40                      is_consumption         469       15.633333\n# 12                  installed_capacity         436       14.533333\n# 11                           eic_count         292        9.733333\n# 2                   prediction_unit_id         228        7.600000\n# 31  fsurface_solar_radiation_downwards         227        7.566667\n# 0                                 hour         180        6.000000\n# 1                        data_block_id         164        5.466667\n# 41                         day_of_week         161        5.366667\n# 32                 shortwave_radiation         111        3.700000\n# 3                          is_business         105        3.500000\n# 10                       euros_per_mwh          83        2.766667\n# 14                         temperature          73        2.433333\n# 42                               month          72        2.400000\n# 30              direct_solar_radiation          60        2.000000\n# 29             fdirect_solar_radiation          48        1.600000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.561215Z","iopub.execute_input":"2025-09-05T11:00:06.561578Z","iopub.status.idle":"2025-09-05T11:00:06.582026Z","shell.execute_reply.started":"2025-09-05T11:00:06.561546Z","shell.execute_reply":"2025-09-05T11:00:06.580961Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n# Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# Sample 10%\n\n\n\n\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# sample_df = train5.sample(frac=0.1, random_state=42).sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Sample 10%: {len(sample_df):,}\")\n\n# # Convertir datetime\n# sample_df['datetime'] = pd.to_datetime(sample_df['datetime'])\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Variables clim√°ticas importantes para lags\n# climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                 'temperature', 'direct_solar_radiation']\n\n# # Crear lags por prediction_unit_id\n# print(\"üîÑ Creando lags clim√°ticos...\")\n# lag_dfs = []\n\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id].copy()\n    \n#     # Lags de 1h, 4h, 12h para cada variable clim√°tica\n#     for var in climate_vars:\n#         if var in unit_data.columns:\n#             unit_data[f'{var}_lag1h'] = unit_data[var].shift(1)\n#             unit_data[f'{var}_lag4h'] = unit_data[var].shift(4)\n#             unit_data[f'{var}_lag12h'] = unit_data[var].shift(12)\n    \n#     lag_dfs.append(unit_data)\n\n# sample_with_lags = pd.concat(lag_dfs, ignore_index=True)\n# sample_with_lags = sample_with_lags.sort_values(['prediction_unit_id', 'datetime'])\n\n# print(f\"Datos con lags: {len(sample_with_lags):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_with_lags.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal\n# split_idx = int(len(sample_with_lags) * 0.8)\n# train_data = sample_with_lags.iloc[:split_idx]\n# test_data = sample_with_lags.iloc[split_idx:]\n\n# # Dropear NaN (por los lags)\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train final: {len(X_train):,}\")\n# print(f\"Test final: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con lags: {mae:.4f}\")\n# print(f\"Mejora vs 89.3: {89.3 - mae:.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.583260Z","iopub.execute_input":"2025-09-05T11:00:06.583623Z","iopub.status.idle":"2025-09-05T11:00:06.601472Z","shell.execute_reply.started":"2025-09-05T11:00:06.583600Z","shell.execute_reply":"2025-09-05T11:00:06.600238Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.602895Z","iopub.execute_input":"2025-09-05T11:00:06.603202Z","iopub.status.idle":"2025-09-05T11:00:06.624431Z","shell.execute_reply.started":"2025-09-05T11:00:06.603177Z","shell.execute_reply":"2025-09-05T11:00:06.623024Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#4to modelo lgb de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses por memoria \n#Mae = 86.6747\n\n\n# Preparar datos y tomar √∫ltimos 3 meses\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# train5['datetime'] = pd.to_datetime(train5['datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# # Features temporales\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Crear is_daylight (1 si hay radiaci√≥n solar, 0 si es noche)\n# sample_df['is_daylight'] = (sample_df['direct_solar_radiation'] > 0).astype(int)\n\n# print(f\"Daylight distribution:\")\n# print(sample_df['is_daylight'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_daylight (3 meses): {mae:.4f}\")\n# print(f\"Baseline original era: 89.3\")\n\n# # #Dataset original: 32,963,024\n# # √öltimos 3 meses: 4,674,024\n# # Porcentaje: 14.2%\n# # Daylight distribution:\n# # is_daylight\n# # 1    2592660\n# # 0    2081364\n# # Name: count, dtype: int64\n# # Features totales: 44\n# # Train: 3,739,219 | Test: 934,805\n# # MAE con is_daylight (3 meses): 86.6747\n# # Baseline original era: 89.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.625605Z","iopub.execute_input":"2025-09-05T11:00:06.625879Z","iopub.status.idle":"2025-09-05T11:00:06.644239Z","shell.execute_reply.started":"2025-09-05T11:00:06.625858Z","shell.execute_reply":"2025-09-05T11:00:06.643070Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n# Preparar datos y tomar √∫ltimos 3 meses\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n# train5['datetime'] = pd.to_datetime(train5['datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# # Features temporales\n# sample_df['day_of_week'] = sample_df['datetime'].dt.dayofweek\n# sample_df['month'] = sample_df['datetime'].dt.month\n\n# # Crear is_daylight (1 si hay radiaci√≥n solar, 0 si es noche)\n# sample_df['is_daylight'] = (sample_df['direct_solar_radiation'] > 0).astype(int)\n\n# # Crear weekend (1 para s√°bado/domingo, 0 para lunes-viernes)\n# sample_df['weekend'] = (sample_df['day_of_week'] >= 5).astype(int)\n\n# print(f\"Daylight distribution:\")\n# print(sample_df['is_daylight'].value_counts())\n# print(f\"Weekend distribution:\")\n# print(sample_df['weekend'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_daylight + weekend (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior era: 86.67\")\n\n# # Dataset original: 32,963,024\n# # √öltimos 3 meses: 4,674,024\n# # Porcentaje: 14.2%\n# # Daylight distribution:\n# # is_daylight\n# # 1    2592660\n# # 0    2081364\n# # Name: count, dtype: int64\n# # Weekend distribution:\n# # weekend\n# # 0    3341592\n# # 1    1332432\n# # Name: count, dtype: int64\n# # Features totales: 45\n# # Train: 3,739,219 | Test: 934,805\n# # MAE con is_daylight + weekend (3 meses): 86.6747\n# # MAE anterior era: 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.645405Z","iopub.execute_input":"2025-09-05T11:00:06.645758Z","iopub.status.idle":"2025-09-05T11:00:06.668362Z","shell.execute_reply.started":"2025-09-05T11:00:06.645733Z","shell.execute_reply":"2025-09-05T11:00:06.667370Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo\n\n# # Usar train5 con las features ya agregadas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Agregar is_active_hours (horario laboral/escolar lunes-viernes 8-17h)\n# train5['is_active_hours'] = (\n#     (train5['day_of_week'] < 5) &  # Lunes-viernes\n#     (train5['hour'] >= 8) & \n#     (train5['hour'] <= 17)\n# ).astype(int)\n\n# # √öltimos 3 meses para evitar problemas de memoria\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_active_hours (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior (is_daylight): 86.67\")\n\n# MAE con is_active_hours (3 meses): 90.4776\n# MAE anterior (is_daylight): 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.669594Z","iopub.execute_input":"2025-09-05T11:00:06.669853Z","iopub.status.idle":"2025-09-05T11:00:06.693056Z","shell.execute_reply.started":"2025-09-05T11:00:06.669834Z","shell.execute_reply":"2025-09-05T11:00:06.691638Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que no la saco\n\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # Agregar is_sleeping_hours (22h-6h)\n# train5['is_sleeping_hours'] = (\n#     (train5['hour'] >= 22) | (train5['hour'] <= 6)\n# ).astype(int)\n\n# # √öltimos 3 meses para evitar problemas de memoria\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(f\"Dataset original: {len(train5):,}\")\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Porcentaje: {len(sample_df)/len(train5)*100:.1f}%\")\n\n# print(f\"Sleeping hours distribution:\")\n# print(sample_df['is_sleeping_hours'].value_counts())\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con is_active_hours (3 meses): {mae:.4f}\")\n# print(f\"MAE anterior (is_daylight): 86.67\")\n\n# Dataset original: 32,963,024\n# √öltimos 3 meses: 4,674,024\n# Porcentaje: 14.2%\n# Sleeping hours distribution:\n# is_sleeping_hours\n# 0    2923632\n# 1    1750392\n# Name: count, dtype: int64\n# Features totales: 46\n# Train: 3,739,219 | Test: 934,805\n# MAE con is_active_hours (3 meses): 98.3152\n# MAE anterior (is_daylight): 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.694385Z","iopub.execute_input":"2025-09-05T11:00:06.694797Z","iopub.status.idle":"2025-09-05T11:00:06.716864Z","shell.execute_reply.started":"2025-09-05T11:00:06.694767Z","shell.execute_reply":"2025-09-05T11:00:06.715622Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n# Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# # Features energ√©ticas \n# print(\"üîÑ Calculando features energ√©ticas...\")\n\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     # Usar installed_capacity como proxy de capacidad productiva\n#     capacity = unit_data['installed_capacity'].iloc[0]\n    \n#     # Ratio de tipos: cu√°ntas observaciones son production vs consumption\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     production_ratio = production_obs / total_obs if total_obs > 0 else 0\n#     consumption_ratio = consumption_obs / total_obs if total_obs > 0 else 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_ratio,\n#         'consumption_obs_ratio': consumption_ratio\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n\n# # Merge con datos principales\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"√öltimos 3 meses: {len(sample_df):,}\")\n# print(f\"Nuevas features: capacity_per_obs, production_obs_ratio, consumption_obs_ratio\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n\n# print(f\"MAE con features energ√©ticas: {mae:.4f}\")\n# print(f\"MAE anterior: 86.67\")\n\n# # BORRAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data, energy_df, energy_stats\n# import gc\n# gc.collect()\n\n# √öltimos 3 meses: 4,674,024\n# Nuevas features: capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n# Features totales: 48\n# Train: 3,739,219 | Test: 934,805\n# MAE con features energ√©ticas: 76.2703\n# MAE anterior: 86.67","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.718103Z","iopub.execute_input":"2025-09-05T11:00:06.718406Z","iopub.status.idle":"2025-09-05T11:00:06.741824Z","shell.execute_reply.started":"2025-09-05T11:00:06.718374Z","shell.execute_reply":"2025-09-05T11:00:06.740602Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#v9 no optimizado para poder usarlo #\n\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 4 MESES para calcular ratio, luego reducir a 3\n# max_date = train5['datetime'].max()\n# cutoff_4m = max_date - pd.DateOffset(months=4)\n# cutoff_3m = max_date - pd.DateOffset(months=3)\n\n# sample_4m = train5[train5['datetime'] >= cutoff_4m].copy()\n# print(\"üîÑ Calculando ratio production/consumption hist√≥rico (30d)...\")\n\n# # M√âTODO SIMPLE Y DIRECTO - sin reindexing problem√°tico\n# sample_4m['prod_cons_ratio_30d'] = 0.0\n\n# for unit_id in sample_4m['prediction_unit_id'].unique():\n#     unit_data = sample_4m[sample_4m['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     # Para cada fila, calcular ratio con ventana de 30 d√≠as hacia atr√°s (SIMPLE)\n#     for i in range(len(unit_data)):\n#         current_time = unit_data.iloc[i]['datetime']\n#         start_time = current_time - pd.Timedelta(days=30)\n        \n#         # Datos en ventana de 30 d√≠as\n#         window_data = unit_data[\n#             (unit_data['datetime'] >= start_time) & \n#             (unit_data['datetime'] < current_time)\n#         ]\n        \n#         if len(window_data) > 0:\n#             # Promedios simples\n#             prod_mean = window_data[window_data['is_consumption'] == 0]['target'].mean()\n#             cons_mean = window_data[window_data['is_consumption'] == 1]['target'].mean()\n            \n#             # Manejar NaN y divisi√≥n por cero\n#             if pd.isna(prod_mean): prod_mean = 0\n#             if pd.isna(cons_mean): cons_mean = 1\n            \n#             ratio = prod_mean / cons_mean if cons_mean != 0 else 0\n#             sample_4m.iloc[i, sample_4m.columns.get_loc('prod_cons_ratio_30d')] = ratio\n\n# # Reducir a 3 meses finales\n# sample_df = sample_4m[sample_4m['datetime'] >= cutoff_3m].copy()\n# print(f\"Datos finales (3 meses): {len(sample_df):,}\")\n\n# # Features energ√©ticas\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# # Split y entrenamiento\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# split_idx = int(len(sample_df) * 0.8)\n# X_train = sample_df.iloc[:split_idx][feature_cols].dropna()\n# y_train = sample_df.iloc[:split_idx].loc[X_train.index, 'target']\n# X_test = sample_df.iloc[split_idx:][feature_cols].dropna()\n# y_test = sample_df.iloc[split_idx:].loc[X_test.index, 'target']\n\n# print(f\"Features: {len(feature_cols)} | Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n#                          random_state=42, n_jobs=-1, verbose=-1)\n# model.fit(X_train, y_train)\n\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con ratio 30d: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 76.27\")\n\n# print(f\"üìà Ratio stats: min={sample_df['prod_cons_ratio_30d'].min():.3f}, max={sample_df['prod_cons_ratio_30d'].max():.3f}, NaNs={sample_df['prod_cons_ratio_30d'].isna().sum()}\")\n\n# # Limpiar memoria\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_4m, energy_df, energy_stats\n# import gc\n# gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.742980Z","iopub.execute_input":"2025-09-05T11:00:06.743337Z","iopub.status.idle":"2025-09-05T11:00:06.762185Z","shell.execute_reply.started":"2025-09-05T11:00:06.743311Z","shell.execute_reply":"2025-09-05T11:00:06.760581Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# #10mo modelo lgb mantengo las 3 anteriores y pruebo  Capacidad instalada per capita (installed_capacity / eic_count)\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita...\")\n\n# # Features energ√©ticas existentes + NUEVA: capacity per capita\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0  # NUEVA FEATURE\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con capacity per capita: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 76.27\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats, feature_importance\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.763108Z","iopub.execute_input":"2025-09-05T11:00:06.763396Z","iopub.status.idle":"2025-09-05T11:00:06.780765Z","shell.execute_reply.started":"2025-09-05T11:00:06.763374Z","shell.execute_reply":"2025-09-05T11:00:06.779594Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train5.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.782082Z","iopub.execute_input":"2025-09-05T11:00:06.782374Z","iopub.status.idle":"2025-09-05T11:00:06.841282Z","shell.execute_reply.started":"2025-09-05T11:00:06.782353Z","shell.execute_reply":"2025-09-05T11:00:06.839953Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n6      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n7      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n8      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n9      366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n10     366   0.496 2021-09-01 03:00:00 2021-09-01 03:00:00     3   \n\n    data_block_id  prediction_unit_id  is_business  product_type  county  \\\n6               0                   0            0             1       0   \n7               0                   0            0             1       0   \n8               0                   0            0             1       0   \n9               0                   0            0             1       0   \n10              0                   0            0             1       0   \n\n     latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n6   59.099998  24.200001                 45.23                  46.32   \n7   59.099998  25.200001                 45.23                  46.32   \n8   59.400002  22.700001                 45.23                  46.32   \n9   59.400002  23.200001                 45.23                  46.32   \n10  59.400002  23.700001                 45.23                  46.32   \n\n    euros_per_mwh  eic_count  installed_capacity  ftemperature  temperature  \\\n6       86.879997      108.0          952.890015     12.681543         12.4   \n7       86.879997      108.0          952.890015     12.868921         12.3   \n8       86.879997      108.0          952.890015     15.041773         15.2   \n9       86.879997      108.0          952.890015     14.632105         14.9   \n10      86.879997      108.0          952.890015     14.480005         12.8   \n\n    fdewpoint  dewpoint  fcloudcover_high  cloudcover_high  fcloudcover_low  \\\n6    9.783228       9.7          0.023590             11.0         0.002380   \n7    9.498316       9.6          0.431854              6.0         0.211182   \n8   11.860376      11.8          0.134674             18.0         0.202515   \n9   11.773584      11.5          0.255188             19.0         0.036774   \n10  11.581568      10.4          0.338074             12.0         0.037109   \n\n    cloudcover_low  fcloudcover_mid  cloudcover_mid  fcloudcover_total  \\\n6             10.0         0.001251             0.0           0.026398   \n7             23.0         0.006790             1.0           0.548508   \n8              7.0         0.003906             0.0           0.308930   \n9              6.0         0.026245             0.0           0.286194   \n10             4.0         0.016510             1.0           0.368134   \n\n    cloudcover_total  f10_metre_u_wind_component  windspeed_10m  \\\n6               12.0                    1.840991       4.222222   \n7               23.0                    1.505298       4.027778   \n8               12.0                    3.185351       9.055555   \n9               11.0                    3.474780       8.361111   \n10               8.0                    3.211841       5.416667   \n\n    f10_metre_v_wind_component  winddirection_10m  fdirect_solar_radiation  \\\n6                    -3.857846              338.0                      0.0   \n7                    -3.590024              337.0                      0.0   \n8                    -8.173276              338.0                      0.0   \n9                    -8.008969              335.0                      0.0   \n10                   -7.426206              337.0                      0.0   \n\n    direct_solar_radiation  fsurface_solar_radiation_downwards  \\\n6                      0.0                                 0.0   \n7                      0.0                                 0.0   \n8                      0.0                                 0.0   \n9                      0.0                                 0.0   \n10                     0.0                                 0.0   \n\n    shortwave_radiation  diffuse_radiation  fsnowfall  snowfall  \\\n6                   0.0                0.0        0.0       0.0   \n7                   0.0                0.0        0.0       0.0   \n8                   0.0                0.0        0.0       0.0   \n9                   0.0                0.0        0.0       0.0   \n10                  0.0                0.0        0.0       0.0   \n\n    ftotal_precipitation  rain  surface_pressure  weather_forecast_hour  \\\n6                    0.0   0.0       1009.200012                    1.0   \n7                    0.0   0.0       1004.200012                    1.0   \n8                    0.0   0.0       1015.000000                    1.0   \n9                    0.0   0.0       1014.500000                    1.0   \n10                   0.0   0.0       1014.000000                    1.0   \n\n    is_consumption  day_of_week  month  is_daylight  weekend  \n6                0            2      9            0        0  \n7                0            2      9            0        0  \n8                0            2      9            0        0  \n9                0            2      9            0        0  \n10               0            2      9            0        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.681543</td>\n      <td>12.4</td>\n      <td>9.783228</td>\n      <td>9.7</td>\n      <td>0.023590</td>\n      <td>11.0</td>\n      <td>0.002380</td>\n      <td>10.0</td>\n      <td>0.001251</td>\n      <td>0.0</td>\n      <td>0.026398</td>\n      <td>12.0</td>\n      <td>1.840991</td>\n      <td>4.222222</td>\n      <td>-3.857846</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1009.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>12.868921</td>\n      <td>12.3</td>\n      <td>9.498316</td>\n      <td>9.6</td>\n      <td>0.431854</td>\n      <td>6.0</td>\n      <td>0.211182</td>\n      <td>23.0</td>\n      <td>0.006790</td>\n      <td>1.0</td>\n      <td>0.548508</td>\n      <td>23.0</td>\n      <td>1.505298</td>\n      <td>4.027778</td>\n      <td>-3.590024</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1004.200012</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>15.041773</td>\n      <td>15.2</td>\n      <td>11.860376</td>\n      <td>11.8</td>\n      <td>0.134674</td>\n      <td>18.0</td>\n      <td>0.202515</td>\n      <td>7.0</td>\n      <td>0.003906</td>\n      <td>0.0</td>\n      <td>0.308930</td>\n      <td>12.0</td>\n      <td>3.185351</td>\n      <td>9.055555</td>\n      <td>-8.173276</td>\n      <td>338.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1015.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.632105</td>\n      <td>14.9</td>\n      <td>11.773584</td>\n      <td>11.5</td>\n      <td>0.255188</td>\n      <td>19.0</td>\n      <td>0.036774</td>\n      <td>6.0</td>\n      <td>0.026245</td>\n      <td>0.0</td>\n      <td>0.286194</td>\n      <td>11.0</td>\n      <td>3.474780</td>\n      <td>8.361111</td>\n      <td>-8.008969</td>\n      <td>335.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.500000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>366</td>\n      <td>0.496</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>2021-09-01 03:00:00</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>86.879997</td>\n      <td>108.0</td>\n      <td>952.890015</td>\n      <td>14.480005</td>\n      <td>12.8</td>\n      <td>11.581568</td>\n      <td>10.4</td>\n      <td>0.338074</td>\n      <td>12.0</td>\n      <td>0.037109</td>\n      <td>4.0</td>\n      <td>0.016510</td>\n      <td>1.0</td>\n      <td>0.368134</td>\n      <td>8.0</td>\n      <td>3.211841</td>\n      <td>5.416667</td>\n      <td>-7.426206</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1014.000000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# train5[\"fsurface_solar_radiation_downwards\"].describe()\n\n# count    3.296302e+07\n# mean     1.103279e+02\n# std      1.714488e+02\n# min     -3.258333e-01\n# 25%      0.000000e+00\n# 50%      5.706424e-01\n# 75%      1.429561e+02\n# max      8.487144e+02\n# Name: fsurface_solar_radiation_downwards, dtype: float64\n\ntrain5[\"fsurface_solar_radiation_downwards\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:06.842433Z","iopub.execute_input":"2025-09-05T11:00:06.842774Z","iopub.status.idle":"2025-09-05T11:00:08.053949Z","shell.execute_reply.started":"2025-09-05T11:00:06.842752Z","shell.execute_reply":"2025-09-05T11:00:08.052617Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"fsurface_solar_radiation_downwards\n0.000000      15297128\n0.284444         39564\n0.142222         35932\n0.568889         28742\n0.071111         21062\n                ...   \n169.200272           2\n320.213043           2\n48.876667            2\n42.334446            2\n268.698120           2\nName: count, Length: 1291576, dtype: int64"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# #11vo modelo lgb 4 feature electrica y pruebo panel_efficiency\n# # ‚úÖ MAE con panel efficiency: 85.0309\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita + panel efficiency...\")\n\n# # Features energ√©ticas existentes + NUEVAS: capacity per capita + panel efficiency\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     # Calcular eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n#     production_data = unit_data[unit_data['is_consumption'] == 0]\n#     if len(production_data) > 0 and 'fsurface_solar_radiation_downwards' in production_data.columns:\n#         solar_rad = production_data['fsurface_solar_radiation_downwards']\n#         production = production_data['target']\n#         # Filtrar valores v√°lidos (solar > 0 para evitar divisi√≥n por 0)\n#         valid_mask = (solar_rad > 0) & (production >= 0)\n#         if valid_mask.sum() > 0:\n#             panel_efficiency = (production[valid_mask] / solar_rad[valid_mask]).mean()\n#         else:\n#             panel_efficiency = 0\n#     else:\n#         panel_efficiency = 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0,\n#         'panel_efficiency': panel_efficiency  # NUEVA FEATURE\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con panel efficiency: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"üìà Panel efficiency stats:\")\n# print(f\"   - Min: {sample_df['panel_efficiency'].min():.4f}\")\n# print(f\"   - Max: {sample_df['panel_efficiency'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['panel_efficiency'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['panel_efficiency'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:08.055162Z","iopub.execute_input":"2025-09-05T11:00:08.055482Z","iopub.status.idle":"2025-09-05T11:00:08.063061Z","shell.execute_reply.started":"2025-09-05T11:00:08.055458Z","shell.execute_reply":"2025-09-05T11:00:08.061805Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# #12vo modelo lgb , lo anterior pero ahora efficiency panel es log\n# # ‚úÖ MAE con panel efficiency LOG: 85.3200\n# # üìä MAE anterior: 72.3572\n\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # √öltimos 3 meses\n# max_date = train5['datetime'].max()\n# cutoff_date = max_date - pd.DateOffset(months=3)\n# sample_df = train5[train5['datetime'] >= cutoff_date].copy()\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita + panel efficiency LOG...\")\n\n# # Features energ√©ticas existentes + NUEVAS: capacity per capita + panel efficiency LOG\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     # Calcular eficiencia de paneles LOG (production vs solar_radiation cuando is_consumption=False)\n#     production_data = unit_data[unit_data['is_consumption'] == 0]\n#     if len(production_data) > 0 and 'fsurface_solar_radiation_downwards' in production_data.columns:\n#         solar_rad = production_data['fsurface_solar_radiation_downwards']\n#         production = production_data['target']\n#         # Filtrar valores v√°lidos (solar > 0 para evitar divisi√≥n por 0)\n#         valid_mask = (solar_rad > 0) & (production >= 0)\n#         if valid_mask.sum() > 0:\n#             efficiency_ratio = (production[valid_mask] / solar_rad[valid_mask]).mean()\n#             # Aplicar log pero manejando valores <= 1\n#             if efficiency_ratio > 1:\n#                 panel_efficiency_log = np.log(efficiency_ratio)\n#             elif efficiency_ratio > 0:\n#                 panel_efficiency_log = -np.log(1/efficiency_ratio)  # log inverso para valores < 1\n#             else:\n#                 panel_efficiency_log = 0\n#         else:\n#             panel_efficiency_log = 0\n#     else:\n#         panel_efficiency_log = 0\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0,\n#         'panel_efficiency_log': panel_efficiency_log  # NUEVA FEATURE CON LOG\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con panel efficiency LOG: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"üìà Capacity per capita stats:\")\n# print(f\"   - Min: {sample_df['capacity_per_capita'].min():.4f}\")\n# print(f\"   - Max: {sample_df['capacity_per_capita'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['capacity_per_capita'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['capacity_per_capita'].isna().sum()}\")\n\n# print(f\"üìà Panel efficiency LOG stats:\")\n# print(f\"   - Min: {sample_df['panel_efficiency_log'].min():.4f}\")\n# print(f\"   - Max: {sample_df['panel_efficiency_log'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['panel_efficiency_log'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['panel_efficiency_log'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:08.067857Z","iopub.execute_input":"2025-09-05T11:00:08.068198Z","iopub.status.idle":"2025-09-05T11:00:08.093323Z","shell.execute_reply.started":"2025-09-05T11:00:08.068175Z","shell.execute_reply":"2025-09-05T11:00:08.091776Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# #13vo modelo probando volatilidad de precio de electricidad 7 dias\n# # ‚úÖ MAE con electricity price volatility: 73.2246\n# # üìä MAE anterior: 72.3572\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 120 d√≠as para calcular volatilidad, luego reducir a 90\n# max_date = train5['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular volatilidad\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular volatilidad\n# sample_120d = train5[train5['datetime'] >= cutoff_120d].copy()\n\n# print(\"üîÑ Calculando volatilidad de precios (7d rolling) con 120 d√≠as...\")\n\n# # Calcular volatilidad de electricity prices (rolling std 7 d√≠as)\n# sample_120d['electricity_price_volatility_7d'] = 0.0\n\n# for unit_id in sample_120d['prediction_unit_id'].unique():\n#     unit_data = sample_120d[sample_120d['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     if 'euros_per_mwh' in unit_data.columns and len(unit_data) > 0:\n#         # Rolling std de 7 d√≠as para electricity prices\n#         unit_data = unit_data.set_index('datetime')\n#         price_vol = unit_data['euros_per_mwh'].rolling('7D', min_periods=1).std()\n        \n#         # Manejar NaN y resetear index\n#         price_vol = price_vol.fillna(0)\n#         unit_data = unit_data.reset_index()\n        \n#         # Asignar volatilidad al dataframe principal\n#         mask = sample_120d['prediction_unit_id'] == unit_id\n#         sample_120d.loc[mask, 'electricity_price_volatility_7d'] = price_vol.values\n\n# # AHORA reducir a √∫ltimos 90 d√≠as (con volatilidad ya calculada)\n# sample_df = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n# print(f\"Datos finales (90 d√≠as con volatilidad): {len(sample_df):,}\")\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita...\")\n\n# # Features energ√©ticas existentes + capacity per capita (SIN panel_efficiency)\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas + volatilidad: {len(sample_df):,}\")\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con electricity price volatility: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 72.3572\")\n\n# # Verificar nuevas features\n# print(f\"üìà Electricity price volatility 7d stats:\")\n# print(f\"   - Min: {sample_df['electricity_price_volatility_7d'].min():.4f}\")\n# print(f\"   - Max: {sample_df['electricity_price_volatility_7d'].max():.4f}\")\n# print(f\"   - Mean: {sample_df['electricity_price_volatility_7d'].mean():.4f}\")\n# print(f\"   - NaN count: {sample_df['electricity_price_volatility_7d'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_120d, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:08.094875Z","iopub.execute_input":"2025-09-05T11:00:08.095278Z","iopub.status.idle":"2025-09-05T11:00:08.124865Z","shell.execute_reply.started":"2025-09-05T11:00:08.095246Z","shell.execute_reply":"2025-09-05T11:00:08.123183Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# #14vo modelo ademas de electricity_volatility pruebo con gas\n# # ‚úÖ MAE con electricity + gas price volatility: 70.6517\n# # üìä MAE anterior: 73.2246\n\n# # Usar train5 con features b√°sicas\n# train5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# # USAR 120 d√≠as para calcular volatilidad, luego reducir a 90\n# max_date = train5['datetime'].max()\n# cutoff_120d = max_date - pd.DateOffset(days=120)  # 120 d√≠as para calcular volatilidad\n# cutoff_90d = max_date - pd.DateOffset(days=90)    # 90 d√≠as finales\n\n# # Datos de 120 d√≠as para calcular volatilidad\n# sample_120d = train5[train5['datetime'] >= cutoff_120d].copy()\n\n# print(\"üîÑ Calculando volatilidad de electricity y gas prices (7d rolling) con 120 d√≠as...\")\n\n# # Calcular volatilidad de electricity y gas prices (rolling std 7 d√≠as)\n# sample_120d['electricity_price_volatility_7d'] = 0.0\n# sample_120d['gas_price_volatility_7d'] = 0.0\n\n# for unit_id in sample_120d['prediction_unit_id'].unique():\n#     unit_data = sample_120d[sample_120d['prediction_unit_id'] == unit_id].copy()\n#     unit_data = unit_data.sort_values('datetime')\n    \n#     if len(unit_data) > 0:\n#         unit_data_indexed = unit_data.set_index('datetime')\n        \n#         # Volatilidad electricity prices\n#         if 'euros_per_mwh' in unit_data.columns:\n#             elec_vol = unit_data_indexed['euros_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n#         else:\n#             elec_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n#         # Volatilidad gas prices\n#         if 'lowest_price_per_mwh' in unit_data.columns:\n#             gas_vol = unit_data_indexed['lowest_price_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n#         else:\n#             gas_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n#         # Asignar volatilidades al dataframe principal usando iloc\n#         mask = sample_120d['prediction_unit_id'] == unit_id\n#         indices = sample_120d[mask].index\n        \n#         sample_120d.loc[indices, 'electricity_price_volatility_7d'] = elec_vol.values\n#         sample_120d.loc[indices, 'gas_price_volatility_7d'] = gas_vol.values\n\n# # AHORA reducir a √∫ltimos 90 d√≠as (conservando TODAS las columnas)\n# sample_df = sample_120d[sample_120d['datetime'] >= cutoff_90d].copy()\n# print(f\"Datos finales (90 d√≠as con volatilidades): {len(sample_df):,}\")\n\n# print(\"üîÑ Calculando features energ√©ticas + capacity per capita...\")\n\n# # Features energ√©ticas existentes + capacity per capita\n# energy_stats = []\n# for unit_id in sample_df['prediction_unit_id'].unique():\n#     unit_data = sample_df[sample_df['prediction_unit_id'] == unit_id]\n    \n#     capacity = unit_data['installed_capacity'].iloc[0]\n#     eic_count = unit_data['eic_count'].iloc[0]\n#     total_obs = len(unit_data)\n#     production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n#     consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n#     energy_stats.append({\n#         'prediction_unit_id': unit_id,\n#         'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n#         'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n#         'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n#         'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n#     })\n\n# energy_df = pd.DataFrame(energy_stats)\n# sample_df = sample_df.merge(energy_df, on='prediction_unit_id', how='left')\n\n# print(f\"Datos con features energ√©ticas + volatilidades: {len(sample_df):,}\")\n\n\n# # Features (todas menos datetime/forecast_date/target/row_id)\n# feature_cols = [col for col in sample_df.columns \n#                 if col not in ['target', 'datetime', 'forecast_date', 'row_id']]\n\n# # Split temporal 80/20\n# split_idx = int(len(sample_df) * 0.8)\n# train_data = sample_df.iloc[:split_idx]\n# test_data = sample_df.iloc[split_idx:]\n\n# X_train = train_data[feature_cols].dropna()\n# y_train = train_data.loc[X_train.index, 'target']\n# X_test = test_data[feature_cols].dropna()\n# y_test = test_data.loc[X_test.index, 'target']\n\n# print(f\"Features totales: {len(feature_cols)}\")\n# print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n\n# # LightGBM\n# model = lgb.LGBMRegressor(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     random_state=42,\n#     n_jobs=-1,\n#     verbose=-1\n# )\n\n# model.fit(X_train, y_train)\n\n# # Resultados\n# pred = model.predict(X_test)\n# mae = mean_absolute_error(y_test, pred)\n# print(f\"‚úÖ MAE con electricity + gas price volatility: {mae:.4f}\")\n# print(f\"üìä MAE anterior: 73.2246\")\n\n# # Verificar nuevas features\n\n# if 'gas_price_volatility_7d' in sample_df.columns:\n#     print(f\"üìà Gas price volatility 7d stats:\")\n#     print(f\"   - Min: {sample_df['gas_price_volatility_7d'].min():.4f}\")\n#     print(f\"   - Max: {sample_df['gas_price_volatility_7d'].max():.4f}\")\n#     print(f\"   - Mean: {sample_df['gas_price_volatility_7d'].mean():.4f}\")\n#     print(f\"   - NaN count: {sample_df['gas_price_volatility_7d'].isna().sum()}\")\n\n# # LIMPIAR MEMORIA\n# del model, X_train, X_test, y_train, y_test, pred, sample_df, sample_120d, train_data, test_data\n# del energy_df, energy_stats\n# import gc\n# gc.collect()\n# print(\"üßπ Memoria liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:08.126703Z","iopub.execute_input":"2025-09-05T11:00:08.127080Z","iopub.status.idle":"2025-09-05T11:00:08.155474Z","shell.execute_reply.started":"2025-09-05T11:00:08.127050Z","shell.execute_reply":"2025-09-05T11:00:08.154282Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# CREAR TRAIN6 con 150 d√≠as y todas las features consolidadas\nprint(\"üîÑ Creando train6 con 150 d√≠as...\")\n\n# Usar train5 como base\ntrain5 = train5.sort_values(['prediction_unit_id', 'datetime'])\n\n# √öltimos 150 d√≠as para train6\nmax_date = train5['datetime'].max()\ncutoff_150d = max_date - pd.DateOffset(days=150)\ntrain6 = train5[train5['datetime'] >= cutoff_150d].copy()\n\nprint(f\"üìä train6 creado: {len(train6):,} filas ({len(train6['prediction_unit_id'].unique())} units)\")\n\nprint(\"üîÑ Calculando features energ√©ticas...\")\n\n# Features energ√©ticas\nenergy_stats = []\nfor unit_id in train6['prediction_unit_id'].unique():\n    unit_data = train6[train6['prediction_unit_id'] == unit_id]\n    \n    capacity = unit_data['installed_capacity'].iloc[0]\n    eic_count = unit_data['eic_count'].iloc[0]\n    total_obs = len(unit_data)\n    production_obs = len(unit_data[unit_data['is_consumption'] == 0])\n    consumption_obs = len(unit_data[unit_data['is_consumption'] == 1])\n    \n    energy_stats.append({\n        'prediction_unit_id': unit_id,\n        'capacity_per_obs': capacity / total_obs if total_obs > 0 else 0,\n        'production_obs_ratio': production_obs / total_obs if total_obs > 0 else 0,\n        'consumption_obs_ratio': consumption_obs / total_obs if total_obs > 0 else 0,\n        'capacity_per_capita': capacity / eic_count if eic_count > 0 else 0\n    })\n\nenergy_df = pd.DataFrame(energy_stats)\ntrain6 = train6.merge(energy_df, on='prediction_unit_id', how='left')\n\nprint(\"üîÑ Calculando volatilidades de precios (7d rolling)...\")\n\n# Volatilidades de precios (7d rolling)\ntrain6['electricity_price_volatility_7d'] = 0.0\ntrain6['gas_price_volatility_7d'] = 0.0\n\nfor unit_id in train6['prediction_unit_id'].unique():\n    unit_data = train6[train6['prediction_unit_id'] == unit_id].copy()\n    unit_data = unit_data.sort_values('datetime')\n    \n    if len(unit_data) > 0:\n        unit_data_indexed = unit_data.set_index('datetime')\n        \n        # Volatilidad electricity prices\n        if 'euros_per_mwh' in unit_data.columns:\n            elec_vol = unit_data_indexed['euros_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n        else:\n            elec_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n        # Volatilidad gas prices\n        if 'lowest_price_per_mwh' in unit_data.columns:\n            gas_vol = unit_data_indexed['lowest_price_per_mwh'].rolling('7D', min_periods=1).std().fillna(0)\n        else:\n            gas_vol = pd.Series(0, index=unit_data_indexed.index)\n        \n        # Asignar volatilidades\n        mask = train6['prediction_unit_id'] == unit_id\n        indices = train6[mask].index\n        \n        train6.loc[indices, 'electricity_price_volatility_7d'] = elec_vol.values\n        train6.loc[indices, 'gas_price_volatility_7d'] = gas_vol.values\n\nprint(\"‚úÖ train6 creado con todas las features:\")\nprint(f\"   üìä Datos: {len(train6):,} filas\")\nprint(f\"   üìä Per√≠odo: {train6['datetime'].min()} a {train6['datetime'].max()}\")\nprint(f\"   üîß Features energ√©ticas: 4 (capacity_per_obs, production_obs_ratio, consumption_obs_ratio, capacity_per_capita)\")\nprint(f\"   üìà Volatilidades: 2 (electricity_price_volatility_7d, gas_price_volatility_7d)\")\n\n# Verificar features creadas\nfeature_check = [\n    'capacity_per_obs', 'production_obs_ratio', 'consumption_obs_ratio', \n    'capacity_per_capita', 'electricity_price_volatility_7d', 'gas_price_volatility_7d'\n]\n\nprint(\"\\nüîç Verificaci√≥n de features:\")\nfor feature in feature_check:\n    if feature in train6.columns:\n        print(f\"   ‚úÖ {feature}: OK\")\n    else:\n        print(f\"   ‚ùå {feature}: FALTA\")\n\n# Estad√≠sticas b√°sicas de las nuevas features\nprint(f\"\\nüìà Estad√≠sticas r√°pidas:\")\nfor feature in feature_check:\n    if feature in train6.columns:\n        print(f\"   {feature}: min={train6[feature].min():.3f}, max={train6[feature].max():.3f}, mean={train6[feature].mean():.3f}\")\n\nprint(f\"\\n‚ú® train6 listo para usar con {len(train6.columns)} columnas totales\")\n\n# Limpiar memoria temporal\ndel energy_df, energy_stats\nimport gc\ngc.collect()\nprint(\"üßπ Memoria temporal liberada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:08.156808Z","iopub.execute_input":"2025-09-05T11:00:08.157144Z","iopub.status.idle":"2025-09-05T11:00:27.250669Z","shell.execute_reply.started":"2025-09-05T11:00:08.157120Z","shell.execute_reply":"2025-09-05T11:00:27.249700Z"}},"outputs":[{"name":"stdout","text":"üîÑ Creando train6 con 150 d√≠as...\nüìä train6 creado: 7,821,024 filas (68 units)\nüîÑ Calculando features energ√©ticas...\nüîÑ Calculando volatilidades de precios (7d rolling)...\n‚úÖ train6 creado con todas las features:\n   üìä Datos: 7,821,024 filas\n   üìä Per√≠odo: 2022-12-31 10:00:00 a 2023-05-30 10:00:00\n   üîß Features energ√©ticas: 4 (capacity_per_obs, production_obs_ratio, consumption_obs_ratio, capacity_per_capita)\n   üìà Volatilidades: 2 (electricity_price_volatility_7d, gas_price_volatility_7d)\n\nüîç Verificaci√≥n de features:\n   ‚úÖ capacity_per_obs: OK\n   ‚úÖ production_obs_ratio: OK\n   ‚úÖ consumption_obs_ratio: OK\n   ‚úÖ capacity_per_capita: OK\n   ‚úÖ electricity_price_volatility_7d: OK\n   ‚úÖ gas_price_volatility_7d: OK\n\nüìà Estad√≠sticas r√°pidas:\n   capacity_per_obs: min=0.000, max=0.091, mean=0.014\n   production_obs_ratio: min=0.500, max=0.500, mean=0.500\n   consumption_obs_ratio: min=0.500, max=0.500, mean=0.500\n   capacity_per_capita: min=0.438, max=213.750, mean=26.458\n   electricity_price_volatility_7d: min=0.000, max=66.608, mean=39.864\n   gas_price_volatility_7d: min=0.000, max=12.594, mean=2.619\n\n‚ú® train6 listo para usar con 55 columnas totales\nüßπ Memoria temporal liberada\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"train6.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:27.251655Z","iopub.execute_input":"2025-09-05T11:00:27.251997Z","iopub.status.idle":"2025-09-05T11:00:27.300136Z","shell.execute_reply.started":"2025-09-05T11:00:27.251966Z","shell.execute_reply":"2025-09-05T11:00:27.298967Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"    row_id  target            datetime       forecast_date  hour  \\\n0  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n1  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n2  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n3  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n4  1535708   1.379 2022-12-31 10:00:00 2022-12-31 10:00:00    10   \n\n   data_block_id  prediction_unit_id  is_business  product_type  county  \\\n0            486                   0            0             1       0   \n1            486                   0            0             1       0   \n2            486                   0            0             1       0   \n3            486                   0            0             1       0   \n4            486                   0            0             1       0   \n\n    latitude  longitude  lowest_price_per_mwh  highest_price_per_mwh  \\\n0  59.099998  24.200001                  74.0              86.199997   \n1  59.099998  25.200001                  74.0              86.199997   \n2  59.400002  22.700001                  74.0              86.199997   \n3  59.400002  23.200001                  74.0              86.199997   \n4  59.400002  23.700001                  74.0              86.199997   \n\n   euros_per_mwh  eic_count  installed_capacity  ftemperature  temperature  \\\n0           3.29      417.0         4001.080078      2.458154          2.5   \n1           3.29      417.0         4001.080078      1.464502          2.2   \n2           3.29      417.0         4001.080078      5.330591          4.7   \n3           3.29      417.0         4001.080078      4.218042          3.8   \n4           3.29      417.0         4001.080078      3.586084          3.0   \n\n   fdewpoint  dewpoint  fcloudcover_high  cloudcover_high  fcloudcover_low  \\\n0   1.860742       1.8          0.989075             95.0              1.0   \n1   0.857690       1.0          0.897797             98.0              1.0   \n2   3.928491       2.8          0.064026             69.0              1.0   \n3   2.536279       2.0          0.132874             94.0              1.0   \n4   2.155054       2.1          0.618713             95.0              1.0   \n\n   cloudcover_low  fcloudcover_mid  cloudcover_mid  fcloudcover_total  \\\n0           100.0         1.000000            97.0           0.999996   \n1            40.0         1.000000            96.0           0.999996   \n2           100.0         0.054596            55.0           0.999996   \n3           100.0         0.371002            82.0           0.999996   \n4           100.0         1.000000            98.0           0.999996   \n\n   cloudcover_total  f10_metre_u_wind_component  windspeed_10m  \\\n0             100.0                    0.320052       8.777778   \n1             100.0                    0.415511       7.666667   \n2             100.0                    4.160628      14.194445   \n3             100.0                    1.823714      14.722222   \n4             100.0                    0.730208      10.750000   \n\n   f10_metre_v_wind_component  winddirection_10m  fdirect_solar_radiation  \\\n0                    8.630277              187.0                -0.008889   \n1                    7.309232              187.0                 0.008889   \n2                   12.238919              195.0                 0.008889   \n3                   13.567777              190.0                 0.000000   \n4                   12.648099              185.0                 0.000000   \n\n   direct_solar_radiation  fsurface_solar_radiation_downwards  \\\n0                     0.0                            5.534878   \n1                     0.0                           15.983768   \n2                     0.0                            5.841545   \n3                     0.0                            5.081545   \n4                     0.0                            3.925990   \n\n   shortwave_radiation  diffuse_radiation  fsnowfall  snowfall  \\\n0                  1.0                1.0   0.000023       0.0   \n1                  3.0                3.0   0.000063       0.0   \n2                  0.0                0.0   0.000000       0.0   \n3                  0.0                0.0   0.000000       0.0   \n4                  1.0                1.0   0.000000       0.0   \n\n   ftotal_precipitation  rain  surface_pressure  weather_forecast_hour  \\\n0              0.000508   0.7        995.500000                   33.0   \n1              0.000489   0.2        993.000000                   33.0   \n2              0.000222   0.5        995.700012                   33.0   \n3              0.000197   0.7        996.700012                   33.0   \n4              0.000252   0.8        998.900024                   33.0   \n\n   is_consumption  day_of_week  month  is_daylight  weekend  capacity_per_obs  \\\n0               0            5     12            0        1          0.016344   \n1               0            5     12            0        1          0.016344   \n2               0            5     12            0        1          0.016344   \n3               0            5     12            0        1          0.016344   \n4               0            5     12            0        1          0.016344   \n\n   production_obs_ratio  consumption_obs_ratio  capacity_per_capita  \\\n0                   0.5                    0.5             9.594916   \n1                   0.5                    0.5             9.594916   \n2                   0.5                    0.5             9.594916   \n3                   0.5                    0.5             9.594916   \n4                   0.5                    0.5             9.594916   \n\n   electricity_price_volatility_7d  gas_price_volatility_7d  \n0                              0.0                      0.0  \n1                              0.0                      0.0  \n2                              0.0                      0.0  \n3                              0.0                      0.0  \n4                              0.0                      0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n      <th>capacity_per_obs</th>\n      <th>production_obs_ratio</th>\n      <th>consumption_obs_ratio</th>\n      <th>capacity_per_capita</th>\n      <th>electricity_price_volatility_7d</th>\n      <th>gas_price_volatility_7d</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>24.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>2.458154</td>\n      <td>2.5</td>\n      <td>1.860742</td>\n      <td>1.8</td>\n      <td>0.989075</td>\n      <td>95.0</td>\n      <td>1.0</td>\n      <td>100.0</td>\n      <td>1.000000</td>\n      <td>97.0</td>\n      <td>0.999996</td>\n      <td>100.0</td>\n      <td>0.320052</td>\n      <td>8.777778</td>\n      <td>8.630277</td>\n      <td>187.0</td>\n      <td>-0.008889</td>\n      <td>0.0</td>\n      <td>5.534878</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.000023</td>\n      <td>0.0</td>\n      <td>0.000508</td>\n      <td>0.7</td>\n      <td>995.500000</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.099998</td>\n      <td>25.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>1.464502</td>\n      <td>2.2</td>\n      <td>0.857690</td>\n      <td>1.0</td>\n      <td>0.897797</td>\n      <td>98.0</td>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>1.000000</td>\n      <td>96.0</td>\n      <td>0.999996</td>\n      <td>100.0</td>\n      <td>0.415511</td>\n      <td>7.666667</td>\n      <td>7.309232</td>\n      <td>187.0</td>\n      <td>0.008889</td>\n      <td>0.0</td>\n      <td>15.983768</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.000063</td>\n      <td>0.0</td>\n      <td>0.000489</td>\n      <td>0.2</td>\n      <td>993.000000</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>22.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>5.330591</td>\n      <td>4.7</td>\n      <td>3.928491</td>\n      <td>2.8</td>\n      <td>0.064026</td>\n      <td>69.0</td>\n      <td>1.0</td>\n      <td>100.0</td>\n      <td>0.054596</td>\n      <td>55.0</td>\n      <td>0.999996</td>\n      <td>100.0</td>\n      <td>4.160628</td>\n      <td>14.194445</td>\n      <td>12.238919</td>\n      <td>195.0</td>\n      <td>0.008889</td>\n      <td>0.0</td>\n      <td>5.841545</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000222</td>\n      <td>0.5</td>\n      <td>995.700012</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.200001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>4.218042</td>\n      <td>3.8</td>\n      <td>2.536279</td>\n      <td>2.0</td>\n      <td>0.132874</td>\n      <td>94.0</td>\n      <td>1.0</td>\n      <td>100.0</td>\n      <td>0.371002</td>\n      <td>82.0</td>\n      <td>0.999996</td>\n      <td>100.0</td>\n      <td>1.823714</td>\n      <td>14.722222</td>\n      <td>13.567777</td>\n      <td>190.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>5.081545</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000197</td>\n      <td>0.7</td>\n      <td>996.700012</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1535708</td>\n      <td>1.379</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>10</td>\n      <td>486</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>59.400002</td>\n      <td>23.700001</td>\n      <td>74.0</td>\n      <td>86.199997</td>\n      <td>3.29</td>\n      <td>417.0</td>\n      <td>4001.080078</td>\n      <td>3.586084</td>\n      <td>3.0</td>\n      <td>2.155054</td>\n      <td>2.1</td>\n      <td>0.618713</td>\n      <td>95.0</td>\n      <td>1.0</td>\n      <td>100.0</td>\n      <td>1.000000</td>\n      <td>98.0</td>\n      <td>0.999996</td>\n      <td>100.0</td>\n      <td>0.730208</td>\n      <td>10.750000</td>\n      <td>12.648099</td>\n      <td>185.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>3.925990</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000252</td>\n      <td>0.8</td>\n      <td>998.900024</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.016344</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>9.594916</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"train6.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:04:58.627081Z","iopub.execute_input":"2025-09-05T11:04:58.628618Z","iopub.status.idle":"2025-09-05T11:05:12.379472Z","shell.execute_reply.started":"2025-09-05T11:04:58.628527Z","shell.execute_reply":"2025-09-05T11:05:12.378080Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"             row_id        target                       datetime  \\\ncount  7.821024e+06  7.821024e+06                        7821024   \nmean   1.772665e+06  4.421838e+02  2023-03-15 16:23:24.042642176   \nmin    1.535708e+06  0.000000e+00            2022-12-31 10:00:00   \n25%    1.653012e+06  8.790000e-01            2023-02-06 00:00:00   \n50%    1.771463e+06  5.382800e+01            2023-03-15 03:00:00   \n75%    1.892214e+06  3.180770e+02            2023-04-22 11:00:00   \nmax    2.013541e+06  1.535311e+04            2023-05-30 10:00:00   \nstd    1.380096e+05  1.286305e+03                            NaN   \n\n                       forecast_date          hour  data_block_id  \\\ncount                        7821024  7.821024e+06   7.821024e+06   \nmean   2023-03-15 16:23:24.042642176  1.150334e+01   5.602036e+02   \nmin              2022-12-31 10:00:00  0.000000e+00   4.860000e+02   \n25%              2023-02-06 00:00:00  6.000000e+00   5.230000e+02   \n50%              2023-03-15 03:00:00  1.200000e+01   5.600000e+02   \n75%              2023-04-22 11:00:00  1.800000e+01   5.980000e+02   \nmax              2023-05-30 10:00:00  2.300000e+01   6.360000e+02   \nstd                              NaN  6.920686e+00   4.343607e+01   \n\n       prediction_unit_id   is_business  product_type        county  \\\ncount        7.821024e+06  7.821024e+06  7.821024e+06  7.821024e+06   \nmean         3.105827e+01  5.473324e-01  1.814040e+00  6.327553e+00   \nmin          0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n25%          1.000000e+01  0.000000e+00  1.000000e+00  2.000000e+00   \n50%          3.000000e+01  1.000000e+00  2.000000e+00  7.000000e+00   \n75%          4.500000e+01  1.000000e+00  3.000000e+00  1.000000e+01   \nmax          6.800000e+01  1.000000e+00  3.000000e+00  1.500000e+01   \nstd          2.061792e+01  4.977546e-01  1.080609e+00  4.901031e+00   \n\n           latitude     longitude  lowest_price_per_mwh  \\\ncount  7.821024e+06  7.821024e+06          7.821024e+06   \nmean   5.867327e+01  2.495431e+01          4.954704e+01   \nmin    5.760000e+01  2.170000e+01          2.810000e+01   \n25%    5.790000e+01  2.320000e+01          4.400000e+01   \n50%    5.850000e+01  2.520000e+01          4.843000e+01   \n75%    5.940000e+01  2.670000e+01          5.500000e+01   \nmax    5.970000e+01  2.820000e+01          7.600000e+01   \nstd    3.592712e+00  2.292524e+00          1.018205e+01   \n\n       highest_price_per_mwh  euros_per_mwh     eic_count  installed_capacity  \\\ncount           7.821024e+06   7.821024e+06  7.821024e+06        7.821024e+06   \nmean            5.734236e+01   8.635880e+01  1.357397e+02        2.408381e+03   \nmin             3.400000e+01  -1.006000e+01  5.000000e+00        5.500000e+00   \n25%             4.900000e+01   4.734000e+01  1.700000e+01        4.320000e+02   \n50%             5.450000e+01   8.560000e+01  4.800000e+01        1.110500e+03   \n75%             6.450000e+01   1.161200e+02  1.120000e+02        2.419890e+03   \nmax             8.999000e+01   2.637400e+02  1.517000e+03        1.931431e+04   \nstd             1.262821e+01   4.733058e+01  2.420391e+02        3.790431e+03   \n\n       ftemperature   temperature     fdewpoint      dewpoint  \\\ncount  7.821024e+06  7.821024e+06  7.821024e+06  7.821024e+06   \nmean   2.664004e+00  2.746641e+00 -7.600056e-01 -1.006187e+00   \nmin   -2.287803e+01 -2.170000e+01 -2.488384e+01 -2.430000e+01   \n25%   -8.665527e-01 -1.100000e+00 -3.808814e+00 -4.000000e+00   \n50%    1.928980e+00  1.800000e+00 -3.448242e-01 -7.000000e-01   \n75%    5.602960e+00  6.200000e+00  2.300928e+00  2.200000e+00   \nmax    2.489663e+01  2.520000e+01  1.618179e+01  1.470000e+01   \nstd    6.029604e+00  6.360111e+00  4.914762e+00  4.968519e+00   \n\n       fcloudcover_high  cloudcover_high  fcloudcover_low  cloudcover_low  \\\ncount      7.821024e+06     7.821024e+06     7.821024e+06    7.821024e+06   \nmean       4.281366e-01     3.921708e+01     4.543341e-01    4.567398e+01   \nmin        0.000000e+00     0.000000e+00     0.000000e+00    0.000000e+00   \n25%        0.000000e+00     0.000000e+00     0.000000e+00    0.000000e+00   \n50%        1.504745e-01     1.400000e+01     2.293091e-01    3.600000e+01   \n75%        9.961548e-01     9.300000e+01     9.999962e-01    9.500000e+01   \nmax        1.000008e+00     1.000000e+02     1.000008e+00    1.000000e+02   \nstd        4.484851e-01     4.204316e+01     4.588894e-01    4.286545e+01   \n\n       fcloudcover_mid  cloudcover_mid  fcloudcover_total  cloudcover_total  \\\ncount     7.821024e+06    7.821024e+06       7.821024e+06      7.821024e+06   \nmean      3.916900e-01    3.636143e+01       6.994910e-01      6.018308e+01   \nmin       0.000000e+00    0.000000e+00       0.000000e+00      0.000000e+00   \n25%       0.000000e+00    0.000000e+00       2.624524e-01      1.900000e+01   \n50%       1.144714e-01    1.500000e+01       9.982118e-01      7.500000e+01   \n75%       9.783729e-01    8.300000e+01       1.000000e+00      1.000000e+02   \nmax       1.000008e+00    1.000000e+02       1.000008e+00      1.000000e+02   \nstd       4.313985e-01    4.029211e+01       4.070652e-01      3.990512e+01   \n\n       f10_metre_u_wind_component  windspeed_10m  f10_metre_v_wind_component  \\\ncount                7.821024e+06   7.821024e+06                7.821024e+06   \nmean                 9.957677e-01   4.819931e+00                6.545376e-01   \nmin                 -1.546236e+01   0.000000e+00               -1.732074e+01   \n25%                 -1.680173e+00   2.833333e+00               -2.242003e+00   \n50%                  1.138676e+00   4.472222e+00                5.167341e-01   \n75%                  3.562516e+00   6.305555e+00                3.507244e+00   \nmax                  2.203584e+01   2.175000e+01                1.810218e+01   \nstd                  4.018321e+00   2.548536e+00                4.305371e+00   \n\n       winddirection_10m  fdirect_solar_radiation  direct_solar_radiation  \\\ncount       7.821024e+06             7.821024e+06            7.821024e+06   \nmean        1.965366e+02             1.562141e+02            7.016632e+01   \nmin         0.000000e+00            -7.111111e-01            0.000000e+00   \n25%         1.260000e+02             0.000000e+00            0.000000e+00   \n50%         2.120000e+02             0.000000e+00            0.000000e+00   \n75%         2.720000e+02             1.999176e+02            4.700000e+01   \nmax         3.600000e+02             9.544222e+02            7.040000e+02   \nstd         9.685699e+01             2.703410e+02            1.446891e+02   \n\n       fsurface_solar_radiation_downwards  shortwave_radiation  \\\ncount                        7.821024e+06         7.821024e+06   \nmean                         1.159485e+02         1.136265e+02   \nmin                         -3.191667e-01         0.000000e+00   \n25%                          0.000000e+00         0.000000e+00   \n50%                          1.162778e+00         1.000000e+00   \n75%                          1.533425e+02         1.520000e+02   \nmax                          8.372131e+02         8.190000e+02   \nstd                          1.946380e+02         1.916752e+02   \n\n       diffuse_radiation     fsnowfall      snowfall  ftotal_precipitation  \\\ncount       7.821024e+06  7.821024e+06  7.821024e+06          7.821024e+06   \nmean        4.346019e+01  3.443305e-05  2.236098e-02          6.828613e-05   \nmin         0.000000e+00 -3.814697e-06  0.000000e+00         -1.525879e-05   \n25%         0.000000e+00  0.000000e+00  0.000000e+00          0.000000e+00   \n50%         1.000000e+00  0.000000e+00  0.000000e+00          0.000000e+00   \n75%         8.000000e+01  1.639128e-06  0.000000e+00          2.240390e-05   \nmax         3.790000e+02  4.538059e-03  1.960000e+00          7.633198e-03   \nstd         6.320733e+01  1.433325e-04  8.994741e-02          2.153762e-04   \n\n               rain  surface_pressure  weather_forecast_hour  is_consumption  \\\ncount  7.821024e+06      7.821024e+06           7.821024e+06       7821024.0   \nmean   3.183777e-02      1.009692e+03           2.450075e+01             0.5   \nmin    0.000000e+00      9.604000e+02           1.000000e+00             0.0   \n25%    0.000000e+00      9.995000e+02           1.300000e+01             0.0   \n50%    0.000000e+00      1.011200e+03           2.450000e+01             0.5   \n75%    0.000000e+00      1.020500e+03           3.700000e+01             1.0   \nmax    4.600000e+00      1.041500e+03           4.800000e+01             1.0   \nstd    1.317892e-01      1.912592e+01           1.385316e+01             0.5   \n\n        day_of_week         month   is_daylight       weekend  \\\ncount  7.821024e+06  7.821024e+06  7.821024e+06  7.821024e+06   \nmean   3.000927e+00  3.006637e+00  4.235384e-01  2.903405e-01   \nmin    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n25%    1.000000e+00  2.000000e+00  0.000000e+00  0.000000e+00   \n50%    3.000000e+00  3.000000e+00  0.000000e+00  0.000000e+00   \n75%    5.000000e+00  4.000000e+00  1.000000e+00  1.000000e+00   \nmax    6.000000e+00  1.200000e+01  1.000000e+00  1.000000e+00   \nstd    2.016589e+00  1.528535e+00  4.941191e-01  4.539195e-01   \n\n       capacity_per_obs  production_obs_ratio  consumption_obs_ratio  \\\ncount      7.821024e+06             7821024.0              7821024.0   \nmean       1.420335e-02                   0.5                    0.5   \nmin        5.548158e-05                   0.5                    0.5   \n25%        2.630401e-03                   0.5                    0.5   \n50%        7.252315e-03                   0.5                    0.5   \n75%        1.441093e-02                   0.5                    0.5   \nmax        9.125695e-02                   0.5                    0.5   \nstd        1.953069e-02                   0.0                    0.0   \n\n       capacity_per_capita  electricity_price_volatility_7d  \\\ncount         7.821024e+06                     7.821024e+06   \nmean          2.645750e+01                     3.986384e+01   \nmin           4.375000e-01                     0.000000e+00   \n25%           1.021500e+01                     3.521267e+01   \n50%           1.598947e+01                     3.991086e+01   \n75%           3.511406e+01                     4.368835e+01   \nmax           2.137500e+02                     6.660753e+01   \nstd           2.563400e+01                     8.743305e+00   \n\n       gas_price_volatility_7d  \ncount             7.821024e+06  \nmean              2.618916e+00  \nmin               0.000000e+00  \n25%               1.224004e+00  \n50%               2.165919e+00  \n75%               2.793054e+00  \nmax               1.259355e+01  \nstd               2.118944e+00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n      <th>datetime</th>\n      <th>forecast_date</th>\n      <th>hour</th>\n      <th>data_block_id</th>\n      <th>prediction_unit_id</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>euros_per_mwh</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>ftemperature</th>\n      <th>temperature</th>\n      <th>fdewpoint</th>\n      <th>dewpoint</th>\n      <th>fcloudcover_high</th>\n      <th>cloudcover_high</th>\n      <th>fcloudcover_low</th>\n      <th>cloudcover_low</th>\n      <th>fcloudcover_mid</th>\n      <th>cloudcover_mid</th>\n      <th>fcloudcover_total</th>\n      <th>cloudcover_total</th>\n      <th>f10_metre_u_wind_component</th>\n      <th>windspeed_10m</th>\n      <th>f10_metre_v_wind_component</th>\n      <th>winddirection_10m</th>\n      <th>fdirect_solar_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>fsurface_solar_radiation_downwards</th>\n      <th>shortwave_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>fsnowfall</th>\n      <th>snowfall</th>\n      <th>ftotal_precipitation</th>\n      <th>rain</th>\n      <th>surface_pressure</th>\n      <th>weather_forecast_hour</th>\n      <th>is_consumption</th>\n      <th>day_of_week</th>\n      <th>month</th>\n      <th>is_daylight</th>\n      <th>weekend</th>\n      <th>capacity_per_obs</th>\n      <th>production_obs_ratio</th>\n      <th>consumption_obs_ratio</th>\n      <th>capacity_per_capita</th>\n      <th>electricity_price_volatility_7d</th>\n      <th>gas_price_volatility_7d</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7821024</td>\n      <td>7821024</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7821024.0</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7821024.0</td>\n      <td>7821024.0</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n      <td>7.821024e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.772665e+06</td>\n      <td>4.421838e+02</td>\n      <td>2023-03-15 16:23:24.042642176</td>\n      <td>2023-03-15 16:23:24.042642176</td>\n      <td>1.150334e+01</td>\n      <td>5.602036e+02</td>\n      <td>3.105827e+01</td>\n      <td>5.473324e-01</td>\n      <td>1.814040e+00</td>\n      <td>6.327553e+00</td>\n      <td>5.867327e+01</td>\n      <td>2.495431e+01</td>\n      <td>4.954704e+01</td>\n      <td>5.734236e+01</td>\n      <td>8.635880e+01</td>\n      <td>1.357397e+02</td>\n      <td>2.408381e+03</td>\n      <td>2.664004e+00</td>\n      <td>2.746641e+00</td>\n      <td>-7.600056e-01</td>\n      <td>-1.006187e+00</td>\n      <td>4.281366e-01</td>\n      <td>3.921708e+01</td>\n      <td>4.543341e-01</td>\n      <td>4.567398e+01</td>\n      <td>3.916900e-01</td>\n      <td>3.636143e+01</td>\n      <td>6.994910e-01</td>\n      <td>6.018308e+01</td>\n      <td>9.957677e-01</td>\n      <td>4.819931e+00</td>\n      <td>6.545376e-01</td>\n      <td>1.965366e+02</td>\n      <td>1.562141e+02</td>\n      <td>7.016632e+01</td>\n      <td>1.159485e+02</td>\n      <td>1.136265e+02</td>\n      <td>4.346019e+01</td>\n      <td>3.443305e-05</td>\n      <td>2.236098e-02</td>\n      <td>6.828613e-05</td>\n      <td>3.183777e-02</td>\n      <td>1.009692e+03</td>\n      <td>2.450075e+01</td>\n      <td>0.5</td>\n      <td>3.000927e+00</td>\n      <td>3.006637e+00</td>\n      <td>4.235384e-01</td>\n      <td>2.903405e-01</td>\n      <td>1.420335e-02</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>2.645750e+01</td>\n      <td>3.986384e+01</td>\n      <td>2.618916e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.535708e+06</td>\n      <td>0.000000e+00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>2022-12-31 10:00:00</td>\n      <td>0.000000e+00</td>\n      <td>4.860000e+02</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>5.760000e+01</td>\n      <td>2.170000e+01</td>\n      <td>2.810000e+01</td>\n      <td>3.400000e+01</td>\n      <td>-1.006000e+01</td>\n      <td>5.000000e+00</td>\n      <td>5.500000e+00</td>\n      <td>-2.287803e+01</td>\n      <td>-2.170000e+01</td>\n      <td>-2.488384e+01</td>\n      <td>-2.430000e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>-1.546236e+01</td>\n      <td>0.000000e+00</td>\n      <td>-1.732074e+01</td>\n      <td>0.000000e+00</td>\n      <td>-7.111111e-01</td>\n      <td>0.000000e+00</td>\n      <td>-3.191667e-01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>-3.814697e-06</td>\n      <td>0.000000e+00</td>\n      <td>-1.525879e-05</td>\n      <td>0.000000e+00</td>\n      <td>9.604000e+02</td>\n      <td>1.000000e+00</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>5.548158e-05</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>4.375000e-01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.653012e+06</td>\n      <td>8.790000e-01</td>\n      <td>2023-02-06 00:00:00</td>\n      <td>2023-02-06 00:00:00</td>\n      <td>6.000000e+00</td>\n      <td>5.230000e+02</td>\n      <td>1.000000e+01</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>2.000000e+00</td>\n      <td>5.790000e+01</td>\n      <td>2.320000e+01</td>\n      <td>4.400000e+01</td>\n      <td>4.900000e+01</td>\n      <td>4.734000e+01</td>\n      <td>1.700000e+01</td>\n      <td>4.320000e+02</td>\n      <td>-8.665527e-01</td>\n      <td>-1.100000e+00</td>\n      <td>-3.808814e+00</td>\n      <td>-4.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>2.624524e-01</td>\n      <td>1.900000e+01</td>\n      <td>-1.680173e+00</td>\n      <td>2.833333e+00</td>\n      <td>-2.242003e+00</td>\n      <td>1.260000e+02</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>9.995000e+02</td>\n      <td>1.300000e+01</td>\n      <td>0.0</td>\n      <td>1.000000e+00</td>\n      <td>2.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>2.630401e-03</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>1.021500e+01</td>\n      <td>3.521267e+01</td>\n      <td>1.224004e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.771463e+06</td>\n      <td>5.382800e+01</td>\n      <td>2023-03-15 03:00:00</td>\n      <td>2023-03-15 03:00:00</td>\n      <td>1.200000e+01</td>\n      <td>5.600000e+02</td>\n      <td>3.000000e+01</td>\n      <td>1.000000e+00</td>\n      <td>2.000000e+00</td>\n      <td>7.000000e+00</td>\n      <td>5.850000e+01</td>\n      <td>2.520000e+01</td>\n      <td>4.843000e+01</td>\n      <td>5.450000e+01</td>\n      <td>8.560000e+01</td>\n      <td>4.800000e+01</td>\n      <td>1.110500e+03</td>\n      <td>1.928980e+00</td>\n      <td>1.800000e+00</td>\n      <td>-3.448242e-01</td>\n      <td>-7.000000e-01</td>\n      <td>1.504745e-01</td>\n      <td>1.400000e+01</td>\n      <td>2.293091e-01</td>\n      <td>3.600000e+01</td>\n      <td>1.144714e-01</td>\n      <td>1.500000e+01</td>\n      <td>9.982118e-01</td>\n      <td>7.500000e+01</td>\n      <td>1.138676e+00</td>\n      <td>4.472222e+00</td>\n      <td>5.167341e-01</td>\n      <td>2.120000e+02</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.162778e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.011200e+03</td>\n      <td>2.450000e+01</td>\n      <td>0.5</td>\n      <td>3.000000e+00</td>\n      <td>3.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>7.252315e-03</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>1.598947e+01</td>\n      <td>3.991086e+01</td>\n      <td>2.165919e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.892214e+06</td>\n      <td>3.180770e+02</td>\n      <td>2023-04-22 11:00:00</td>\n      <td>2023-04-22 11:00:00</td>\n      <td>1.800000e+01</td>\n      <td>5.980000e+02</td>\n      <td>4.500000e+01</td>\n      <td>1.000000e+00</td>\n      <td>3.000000e+00</td>\n      <td>1.000000e+01</td>\n      <td>5.940000e+01</td>\n      <td>2.670000e+01</td>\n      <td>5.500000e+01</td>\n      <td>6.450000e+01</td>\n      <td>1.161200e+02</td>\n      <td>1.120000e+02</td>\n      <td>2.419890e+03</td>\n      <td>5.602960e+00</td>\n      <td>6.200000e+00</td>\n      <td>2.300928e+00</td>\n      <td>2.200000e+00</td>\n      <td>9.961548e-01</td>\n      <td>9.300000e+01</td>\n      <td>9.999962e-01</td>\n      <td>9.500000e+01</td>\n      <td>9.783729e-01</td>\n      <td>8.300000e+01</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+02</td>\n      <td>3.562516e+00</td>\n      <td>6.305555e+00</td>\n      <td>3.507244e+00</td>\n      <td>2.720000e+02</td>\n      <td>1.999176e+02</td>\n      <td>4.700000e+01</td>\n      <td>1.533425e+02</td>\n      <td>1.520000e+02</td>\n      <td>8.000000e+01</td>\n      <td>1.639128e-06</td>\n      <td>0.000000e+00</td>\n      <td>2.240390e-05</td>\n      <td>0.000000e+00</td>\n      <td>1.020500e+03</td>\n      <td>3.700000e+01</td>\n      <td>1.0</td>\n      <td>5.000000e+00</td>\n      <td>4.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.441093e-02</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>3.511406e+01</td>\n      <td>4.368835e+01</td>\n      <td>2.793054e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.013541e+06</td>\n      <td>1.535311e+04</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2023-05-30 10:00:00</td>\n      <td>2.300000e+01</td>\n      <td>6.360000e+02</td>\n      <td>6.800000e+01</td>\n      <td>1.000000e+00</td>\n      <td>3.000000e+00</td>\n      <td>1.500000e+01</td>\n      <td>5.970000e+01</td>\n      <td>2.820000e+01</td>\n      <td>7.600000e+01</td>\n      <td>8.999000e+01</td>\n      <td>2.637400e+02</td>\n      <td>1.517000e+03</td>\n      <td>1.931431e+04</td>\n      <td>2.489663e+01</td>\n      <td>2.520000e+01</td>\n      <td>1.618179e+01</td>\n      <td>1.470000e+01</td>\n      <td>1.000008e+00</td>\n      <td>1.000000e+02</td>\n      <td>1.000008e+00</td>\n      <td>1.000000e+02</td>\n      <td>1.000008e+00</td>\n      <td>1.000000e+02</td>\n      <td>1.000008e+00</td>\n      <td>1.000000e+02</td>\n      <td>2.203584e+01</td>\n      <td>2.175000e+01</td>\n      <td>1.810218e+01</td>\n      <td>3.600000e+02</td>\n      <td>9.544222e+02</td>\n      <td>7.040000e+02</td>\n      <td>8.372131e+02</td>\n      <td>8.190000e+02</td>\n      <td>3.790000e+02</td>\n      <td>4.538059e-03</td>\n      <td>1.960000e+00</td>\n      <td>7.633198e-03</td>\n      <td>4.600000e+00</td>\n      <td>1.041500e+03</td>\n      <td>4.800000e+01</td>\n      <td>1.0</td>\n      <td>6.000000e+00</td>\n      <td>1.200000e+01</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>9.125695e-02</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>2.137500e+02</td>\n      <td>6.660753e+01</td>\n      <td>1.259355e+01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.380096e+05</td>\n      <td>1.286305e+03</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.920686e+00</td>\n      <td>4.343607e+01</td>\n      <td>2.061792e+01</td>\n      <td>4.977546e-01</td>\n      <td>1.080609e+00</td>\n      <td>4.901031e+00</td>\n      <td>3.592712e+00</td>\n      <td>2.292524e+00</td>\n      <td>1.018205e+01</td>\n      <td>1.262821e+01</td>\n      <td>4.733058e+01</td>\n      <td>2.420391e+02</td>\n      <td>3.790431e+03</td>\n      <td>6.029604e+00</td>\n      <td>6.360111e+00</td>\n      <td>4.914762e+00</td>\n      <td>4.968519e+00</td>\n      <td>4.484851e-01</td>\n      <td>4.204316e+01</td>\n      <td>4.588894e-01</td>\n      <td>4.286545e+01</td>\n      <td>4.313985e-01</td>\n      <td>4.029211e+01</td>\n      <td>4.070652e-01</td>\n      <td>3.990512e+01</td>\n      <td>4.018321e+00</td>\n      <td>2.548536e+00</td>\n      <td>4.305371e+00</td>\n      <td>9.685699e+01</td>\n      <td>2.703410e+02</td>\n      <td>1.446891e+02</td>\n      <td>1.946380e+02</td>\n      <td>1.916752e+02</td>\n      <td>6.320733e+01</td>\n      <td>1.433325e-04</td>\n      <td>8.994741e-02</td>\n      <td>2.153762e-04</td>\n      <td>1.317892e-01</td>\n      <td>1.912592e+01</td>\n      <td>1.385316e+01</td>\n      <td>0.5</td>\n      <td>2.016589e+00</td>\n      <td>1.528535e+00</td>\n      <td>4.941191e-01</td>\n      <td>4.539195e-01</td>\n      <td>1.953069e-02</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.563400e+01</td>\n      <td>8.743305e+00</td>\n      <td>2.118944e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"#Mae puesto n1 kaggle 52.3090\n#1er primer modelo xgb sobre train5 haciendo split 80/20 de train y test \n#Mae = 107.9896\n#sin lag sin feature, sin nada , solo haciendo el merge\n\n\n#agrego day of week y month para todos los siguientes\n\n#2do modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 89.3\n\n#3er modelo lgb 10% y sobre esto split 80/20 de train y test \n#Mae = 103.1249\n#climate_vars = ['fsurface_solar_radiation_downwards', 'shortwave_radiation', \n#                'temperature', 'direct_solar_radiation']\n# agrego lags de 1h/4h/12h en las variables de arriba\n# empeoro porque agregue los lags al azar no hice un analisis para agregarlo en los marcos de tiempo relevante\n# estas variables de lags las quito\n\n#agrego is_daylight binaria\n\n#4to modelo lgb 14.2% y sobre esto split 80/20 de train y test  \n#de base solo agrego la variable is_daylight , pero ahora uso info de los ultimos 3 meses 14.2% aprox\n# se que no deberia ser comparable al usar de base distinto , pero parte de aca ahora\n#Mae = 86.6747\n\n#agrego weekend binaria\n\n#5to modelo lgb igual que el anterior , solo agregue binaria Weekend y no vi cambios\n#Mae = 86.6747\n\n#para todos los demas modelos ya tienen incluido\n#day of week\n#month\n#is_daylight\n#weekend\n\n#6to modelo lgb , agregue variable binaria is_active_hours , horario de 8 a 17 hs binaria\n#Mae = 90.4776 , empeoro el modelo la saco por ahora a is_active_hours\n\n#7mo modelo lgb, agregue variable binaria is_sleeping_hours 22h-6h\n#Mae = 98.3152 empeora el modelo , asi que  la saco a is_sleeping_hours\n\n#empiezo a probar con variables\n##Energ√©ticas:\n\n# Ratio production/consumption por prediction_unit_id (hist√≥rico) # no se pudo probar\n# Capacidad instalada per capita (installed_capacity / eic_count) # si mejora el modelo\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)\n# Volatilidad de precios (rolling std de electricity/gas prices)\n# Spread gas-electricidad (diferencia de precios, importante para switching)\n\n#8vo modelo lgb agregue 3 variables energeticas capacity_per_obs, production_obs_ratio, consumption_obs_ratio\n#Mae = 76.2703 mejor bastante el modelo\n\n#9no modelo lgb  mantengo las 3 anteriores y prueba una nueva Ratio production/cn\n# no optimizado no pudo correr\n\n#10mo modelo lgb mantengo las 3 anteriores y pruebo  Capacidad instalada per capita (installed_capacity / eic_count)\n#Mae = 72.3572 mejora el modelo \n\n#para los demas modelos sigo usando las siguientes variables\n#capacity_per_obs\n#production_obs_ratio\n#consumption_obs_ratio\n#capacity_per_capita\n\n#11vo modelo lgb con esto #Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False)#\n#Mae anterior = 72.3572\n#Mae con eficiencia de paneles = 85.0309 empeora , pruebo con log de esto  y sino no la uso por ahora\n\n\n#12vo modelo lgb , lo anterior pero ahora efficiency panel es log , lo saco\n# ‚úÖ MAE con panel efficiency LOG: 85.3200\n# üìä MAE anterior: 72.3572\n\n\n# #13vo modelo probando volatilidad de precio de electricidad 7 dias\n# # ‚úÖ MAE con electricity price volatility: 73.2246 si , falta probar mas versiones y gas\n# # üìä MAE anterior: 72.3572\n\n#14vo modelo ademas de electricity_volatility pruebo con gas\n# ‚úÖ MAE con electricity + gas price volatility: 70.6517 se queda\n# üìä MAE anterior: 73.2246\n\n#creo train6 de 150 dias para agregar\n#capacity_per_obs\n#production_obs_ratio\n#consumption_obs_ratio\n#capacity_per_capita\n#elect price volatility\n#gas price volatility\n\n#15vo probar gas y/o electricity de 30 dias , agrega mucho ruido no sirve\n\n#16vo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:27.301278Z","iopub.execute_input":"2025-09-05T11:00:27.301616Z","iopub.status.idle":"2025-09-05T11:00:27.308716Z","shell.execute_reply.started":"2025-09-05T11:00:27.301593Z","shell.execute_reply":"2025-09-05T11:00:27.307592Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Ratio production/consumption por prediction_unit_id (hist√≥rico) #no se pudo probar\n# Capacidad instalada per capita (installed_capacity / eic_count) # si mejora el modelo\n# Eficiencia de paneles (production vs solar_radiation cuando is_consumption=False) # empeora el modelo\n# Volatilidad de precios (rolling std de electricity/gas prices) # mejor poco 1 cosa falta probar mas\n# Volatilidad individual + combinada\n# electricity_price_volatility_7d = rolling(7).std()\n# gas_price_volatility_7d = rolling(7).std()\n# electricity_vol_30d = rolling std de electricity prices\n# gas_vol_30d = rolling std de gas prices  \n# combined_price_vol = std de (electricity + gas weighted)\n# price_volatility = rolling_std / rolling_mean\n# Spread gas-electricidad (diferencia de precios)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:00:27.310271Z","iopub.execute_input":"2025-09-05T11:00:27.311069Z","iopub.status.idle":"2025-09-05T11:00:27.335809Z","shell.execute_reply.started":"2025-09-05T11:00:27.311029Z","shell.execute_reply":"2025-09-05T11:00:27.334336Z"}},"outputs":[],"execution_count":34}]}